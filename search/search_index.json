{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Home Section 1 Point A Point B What do I do when I get stuck on a problem? It is very normal to get stuck on issues in software development, and you will continue to get stuck on things throughout your career (such is the nature of software engineering!). Luckily, you have many tools and steps you can use in these situations. Listed below are 5 cards that you can follow when you find yourself so: - Card 1: Stack Overflow Posts / Google / Wiki. If you have trouble building something, if you're encountering problems for the first time or if you're even just stuck on general c# queries. Paste that error code into the search bar of stack overflow / google / the wiki and see if you can find something. Sometimes its crazy to see how many people have encountered the same problem you have and there's detailed answers to some of the more common problems. Unfortunately, we no longer have stack overflow at WiseTech, but all the old posts can be found here . - Card 2: Ask your fellow E&L mentees. There is a good chance they may have come across the issue in the past, or know of some helpful debugging techniques that you might be able to try. Teaching others is one of the best ways to learn so this is also a good chance for them to consolidate what they know if they have come across something similar before. - Card 3: If you can't find an answer to your question, it's a good idea to post a question in your team's general help channel. Someone should get back to you soon, and your question could help someone else later too. Or who knows, you might run into the same problem later and your own post might help you when you've forgotten the solution. In BorderWise, we have project specific channels (BW gang, WW squad, UMP crew) so that you can post in the channel relevant to the project you have the issue on. Message your mentor to add you to the channel if you are not already part of one. - Card 4: Ask your mentor! We are here to help you at any time. - Card 5: Ask someone else on your team, chances are they'll be able to help you too. And if they can't they'll refer you to someone else who can help you. A guide to asking questions well When asking questions, it is important to communicate clearly in order to save you and the person you are asking the question of time. This can save you a lot of back and forth. The following structure is one I like to following when asking questions. PR link WI link Summary of what the WI is / general question context My question Steps I have tried so far to try solve the problem What I've understood / gathered from my debugging or investigation Adding these elements to your question means that the person you are asking has all the information at their fingertips they need to answer your question, and that they can tailor their response in a way that matches your understanding and fills in any knowledge gaps you might have in the area. Also see https://nohello.net/en/. _Credits to Neeraj Mirashi for the cards.","title":"Home"},{"location":"#home","text":"","title":"Home"},{"location":"#section-1","text":"Point A Point B","title":"Section 1"},{"location":"#what-do-i-do-when-i-get-stuck-on-a-problem","text":"It is very normal to get stuck on issues in software development, and you will continue to get stuck on things throughout your career (such is the nature of software engineering!). Luckily, you have many tools and steps you can use in these situations. Listed below are 5 cards that you can follow when you find yourself so: - Card 1: Stack Overflow Posts / Google / Wiki. If you have trouble building something, if you're encountering problems for the first time or if you're even just stuck on general c# queries. Paste that error code into the search bar of stack overflow / google / the wiki and see if you can find something. Sometimes its crazy to see how many people have encountered the same problem you have and there's detailed answers to some of the more common problems. Unfortunately, we no longer have stack overflow at WiseTech, but all the old posts can be found here . - Card 2: Ask your fellow E&L mentees. There is a good chance they may have come across the issue in the past, or know of some helpful debugging techniques that you might be able to try. Teaching others is one of the best ways to learn so this is also a good chance for them to consolidate what they know if they have come across something similar before. - Card 3: If you can't find an answer to your question, it's a good idea to post a question in your team's general help channel. Someone should get back to you soon, and your question could help someone else later too. Or who knows, you might run into the same problem later and your own post might help you when you've forgotten the solution. In BorderWise, we have project specific channels (BW gang, WW squad, UMP crew) so that you can post in the channel relevant to the project you have the issue on. Message your mentor to add you to the channel if you are not already part of one. - Card 4: Ask your mentor! We are here to help you at any time. - Card 5: Ask someone else on your team, chances are they'll be able to help you too. And if they can't they'll refer you to someone else who can help you.","title":"What do I do when I get stuck on a problem?"},{"location":"#a-guide-to-asking-questions-well","text":"When asking questions, it is important to communicate clearly in order to save you and the person you are asking the question of time. This can save you a lot of back and forth. The following structure is one I like to following when asking questions. PR link WI link Summary of what the WI is / general question context My question Steps I have tried so far to try solve the problem What I've understood / gathered from my debugging or investigation Adding these elements to your question means that the person you are asking has all the information at their fingertips they need to answer your question, and that they can tailor their response in a way that matches your understanding and fills in any knowledge gaps you might have in the area. Also see https://nohello.net/en/. _Credits to Neeraj Mirashi for the cards.","title":"A guide to asking questions well"},{"location":"Earn-%26-Learn-Resources/","text":"Earn & Learn Resources General Guides What do I do when I get stuck on a problem? Frontend WSL / Linux / Ubuntu / Bash Linux Journey: The Shell [Interactive] Learn the Command Line [Interactive] Typescript Beginner's Typescript Vue Vue Docs Vue JS 3 Tutorial for Beginners #1 Introduction Vue.js Basics (Part 1 & 2) The Vue Router The Composition API (Part 1 & 2) UI Component Library: Vuetify Global state management: Pinia Working with forms: Vee-Validate React React Docs Codecademy: Learn React UI Component Library: MUI Global state management: Zustand Working with forms: React Hook Form AngularJS AngularJS Docs Zod Learn Zod in 30 Minutes Vue/React Query Tanstack Query Docs Testing Testing Library Querying elements in tests: Queries Regular Expressions (masks/patterns) regex101 Backend .NET Logging Serilog: Writing Log Events Database SQL Entity Framework MongoDB","title":"Earn &amp; Learn Resources"},{"location":"Earn-%26-Learn-Resources/#earn-learn-resources","text":"","title":"Earn &amp; Learn Resources"},{"location":"Earn-%26-Learn-Resources/#general-guides","text":"What do I do when I get stuck on a problem?","title":"General Guides"},{"location":"Earn-%26-Learn-Resources/#frontend","text":"","title":"Frontend"},{"location":"Earn-%26-Learn-Resources/#wsl-linux-ubuntu-bash","text":"Linux Journey: The Shell [Interactive] Learn the Command Line [Interactive]","title":"WSL / Linux / Ubuntu / Bash"},{"location":"Earn-%26-Learn-Resources/#typescript","text":"Beginner's Typescript","title":"Typescript"},{"location":"Earn-%26-Learn-Resources/#vue","text":"Vue Docs Vue JS 3 Tutorial for Beginners #1 Introduction Vue.js Basics (Part 1 & 2) The Vue Router The Composition API (Part 1 & 2) UI Component Library: Vuetify Global state management: Pinia Working with forms: Vee-Validate","title":"Vue"},{"location":"Earn-%26-Learn-Resources/#react","text":"React Docs Codecademy: Learn React UI Component Library: MUI Global state management: Zustand Working with forms: React Hook Form","title":"React"},{"location":"Earn-%26-Learn-Resources/#angularjs","text":"AngularJS Docs","title":"AngularJS"},{"location":"Earn-%26-Learn-Resources/#zod","text":"Learn Zod in 30 Minutes","title":"Zod"},{"location":"Earn-%26-Learn-Resources/#vuereact-query","text":"Tanstack Query Docs","title":"Vue/React Query"},{"location":"Earn-%26-Learn-Resources/#testing","text":"Testing Library Querying elements in tests: Queries","title":"Testing"},{"location":"Earn-%26-Learn-Resources/#regular-expressions-maskspatterns","text":"regex101","title":"Regular Expressions (masks/patterns)"},{"location":"Earn-%26-Learn-Resources/#backend","text":"","title":"Backend"},{"location":"Earn-%26-Learn-Resources/#net","text":"","title":".NET"},{"location":"Earn-%26-Learn-Resources/#logging","text":"Serilog: Writing Log Events","title":"Logging"},{"location":"Earn-%26-Learn-Resources/#database","text":"","title":"Database"},{"location":"Earn-%26-Learn-Resources/#sql","text":"","title":"SQL"},{"location":"Earn-%26-Learn-Resources/#entity-framework","text":"","title":"Entity Framework"},{"location":"Earn-%26-Learn-Resources/#mongodb","text":"","title":"MongoDB"},{"location":"Guides/","text":"Various guides that are more advanced or just aren't usually needed for new starters, hidden in here not to confuse new starters","title":"Guides"},{"location":"Monitoring/","text":"HA Proxy URLs (Only accessible through JumpHost) External: http://10.2.64.12:2031/?scope=borderwise Internal: http://10.2.83.22:8181/?scope=border","title":"Monitoring"},{"location":"Monitoring/#ha-proxy-urls-only-accessible-through-jumphost","text":"External: http://10.2.64.12:2031/?scope=borderwise Internal: http://10.2.83.22:8181/?scope=border","title":"HA Proxy URLs (Only accessible through JumpHost)"},{"location":"New-Starters/","text":"Start here, in the following order: 1. Intro to BorderWise 2. Environment Setup 3. Next steps 4. Creating a PR","title":"New Starters"},{"location":"Specs/","text":"Various documentation on how stuff works","title":"Specs"},{"location":"Testing/","text":"BWW Vue testing strategy (WIP) We should aim for the majority of tests to be black-box tests using @testing-library/vue as it encourages writing tests from the user perspective. \"I click here, I see this\" instead of \"I'm calling function, I observe internal state change\". Whitebox tests should be used when we're either testing some complex and critical logic that would require many-many tests to cover all cases that we care about. If doing black-box testing requires too much setup and isn't easily achievable - perhaps we're trying to test something that we shouldn't be testing to begin with. testing-library encourages us to write tests from the user perspective (click stuff, see stuff), which is blackbox because we don't care what happens inside of methods/state/etc. test-utils allow us to actually call certain methods, see what they return, check internal state of components, etc. - hence whitebox. So if you're trying to test internal state of component or data returned by a specific method - we can't test it with blackbox testing-library approach. And we only should be testing these things for very critical/important stuff, otherwise, every little refactoring will break tests which isn't great. Refer to the flow chart below: \"Yes\" path is always down, \"No\" path is always to the right. Stuff in the clouds is comments/examples for the path next to the cloud. https://docs.google.com/spreadsheets/d/1UytHl_1cJq48jiEU64cOl91qnyzI1yZHu_qyG-zSPcE/edit?usp=sharing","title":"Testing"},{"location":"Testing/#bww-vue-testing-strategy-wip","text":"We should aim for the majority of tests to be black-box tests using @testing-library/vue as it encourages writing tests from the user perspective. \"I click here, I see this\" instead of \"I'm calling function, I observe internal state change\". Whitebox tests should be used when we're either testing some complex and critical logic that would require many-many tests to cover all cases that we care about. If doing black-box testing requires too much setup and isn't easily achievable - perhaps we're trying to test something that we shouldn't be testing to begin with. testing-library encourages us to write tests from the user perspective (click stuff, see stuff), which is blackbox because we don't care what happens inside of methods/state/etc. test-utils allow us to actually call certain methods, see what they return, check internal state of components, etc. - hence whitebox. So if you're trying to test internal state of component or data returned by a specific method - we can't test it with blackbox testing-library approach. And we only should be testing these things for very critical/important stuff, otherwise, every little refactoring will break tests which isn't great. Refer to the flow chart below: \"Yes\" path is always down, \"No\" path is always to the right. Stuff in the clouds is comments/examples for the path next to the cloud. https://docs.google.com/spreadsheets/d/1UytHl_1cJq48jiEU64cOl91qnyzI1yZHu_qyG-zSPcE/edit?usp=sharing","title":"BWW Vue testing strategy (WIP)"},{"location":"about/","text":"","title":"About"},{"location":"Earn-%26-Learn-Resources/What-do-I-do-when-I-get-stuck-on-a-problem%253F/","text":"What do I do when I get stuck on a problem? It is very normal to get stuck on issues in software development, and you will continue to get stuck on things throughout your career (such is the nature of software engineering!). Luckily, you have many tools and steps you can use in these situations. Listed below are 5 cards that you can follow when you find yourself so: - Card 1: Stack Overflow Posts / Google / Wiki. If you have trouble building something, if you're encountering problems for the first time or if you're even just stuck on general c# queries. Paste that error code into the search bar of stack overflow / google / the wiki and see if you can find something. Sometimes its crazy to see how many people have encountered the same problem you have and there's detailed answers to some of the more common problems. Unfortunately, we no longer have stack overflow at WiseTech, but all the old posts can be found here . - Card 2: Ask your fellow E&L mentees. There is a good chance they may have come across the issue in the past, or know of some helpful debugging techniques that you might be able to try. Teaching others is one of the best ways to learn so this is also a good chance for them to consolidate what they know if they have come across something similar before. - Card 3: If you can't find an answer to your question, it's a good idea to post a question in your team's general help channel. Someone should get back to you soon, and your question could help someone else later too. Or who knows, you might run into the same problem later and your own post might help you when you've forgotten the solution. In BorderWise, we have project specific channels (BW gang, WW squad, UMP crew) so that you can post in the channel relevant to the project you have the issue on. Message your mentor to add you to the channel if you are not already part of one. - Card 4: Ask your mentor! We are here to help you at any time. - Card 5: Ask someone else on your team, chances are they'll be able to help you too. And if they can't they'll refer you to someone else who can help you. A guide to asking questions well When asking questions, it is important to communicate clearly in order to save you and the person you are asking the question of time. This can save you a lot of back and forth. The following structure is one I like to following when asking questions. PR link WI link Summary of what the WI is / general question context My question Steps I have tried so far to try solve the problem What I've understood / gathered from my debugging or investigation Adding these elements to your question means that the person you are asking has all the information at their fingertips they need to answer your question, and that they can tailor their response in a way that matches your understanding and fills in any knowledge gaps you might have in the area. Also see https://nohello.net/en/. Credits to Neeraj Mirashi for the cards.","title":"What do I do when I get stuck on a problem?"},{"location":"Earn-%26-Learn-Resources/What-do-I-do-when-I-get-stuck-on-a-problem%253F/#what-do-i-do-when-i-get-stuck-on-a-problem","text":"It is very normal to get stuck on issues in software development, and you will continue to get stuck on things throughout your career (such is the nature of software engineering!). Luckily, you have many tools and steps you can use in these situations. Listed below are 5 cards that you can follow when you find yourself so: - Card 1: Stack Overflow Posts / Google / Wiki. If you have trouble building something, if you're encountering problems for the first time or if you're even just stuck on general c# queries. Paste that error code into the search bar of stack overflow / google / the wiki and see if you can find something. Sometimes its crazy to see how many people have encountered the same problem you have and there's detailed answers to some of the more common problems. Unfortunately, we no longer have stack overflow at WiseTech, but all the old posts can be found here . - Card 2: Ask your fellow E&L mentees. There is a good chance they may have come across the issue in the past, or know of some helpful debugging techniques that you might be able to try. Teaching others is one of the best ways to learn so this is also a good chance for them to consolidate what they know if they have come across something similar before. - Card 3: If you can't find an answer to your question, it's a good idea to post a question in your team's general help channel. Someone should get back to you soon, and your question could help someone else later too. Or who knows, you might run into the same problem later and your own post might help you when you've forgotten the solution. In BorderWise, we have project specific channels (BW gang, WW squad, UMP crew) so that you can post in the channel relevant to the project you have the issue on. Message your mentor to add you to the channel if you are not already part of one. - Card 4: Ask your mentor! We are here to help you at any time. - Card 5: Ask someone else on your team, chances are they'll be able to help you too. And if they can't they'll refer you to someone else who can help you.","title":"What do I do when I get stuck on a problem?"},{"location":"Earn-%26-Learn-Resources/What-do-I-do-when-I-get-stuck-on-a-problem%253F/#a-guide-to-asking-questions-well","text":"When asking questions, it is important to communicate clearly in order to save you and the person you are asking the question of time. This can save you a lot of back and forth. The following structure is one I like to following when asking questions. PR link WI link Summary of what the WI is / general question context My question Steps I have tried so far to try solve the problem What I've understood / gathered from my debugging or investigation Adding these elements to your question means that the person you are asking has all the information at their fingertips they need to answer your question, and that they can tailor their response in a way that matches your understanding and fills in any knowledge gaps you might have in the area. Also see https://nohello.net/en/. Credits to Neeraj Mirashi for the cards.","title":"A guide to asking questions well"},{"location":"Guides/AngularJS-to-Vue-props-reactivity/","text":"Recently we have finished something I've never seen anywhere else before: deep object/array reactivity from AngularJS into Vue. It will allow us to pass complex/nested objects/arrays as props into components You can use it like this: const data = { one: { two: 3 } }; const reactiveData = reactive(data); // in template: <ve-some-component ng-prop-some_prop=\"$ctrl.reactiveData\" ... /> // DO NOT do this: ng-prop-some_prop=\"$ctrl.reactiveData.one\" - will not work, only pass the \"root\" object that was defined using reactive() reactiveData.one.two++; // this deep change will be detected and `ve-some-component` re-rendered // DO NOT do this: reactiveData = {} // NOT reactive anymore // DO this instead: reactiveData = reactive({}) // In Vue you need to use `AngularJSReactiveProperty` type to define props: props: { something: { type: Object as PropType<AngularJSReactiveProperty<{ one: { two: number } }>> } } // To access reactive prop value, you need to use `.value` similar to how reactivity works in Vue: setup(props) { console.log(props.something.value) // ... // There's internal `props.something._version` - this it to trigger re-render, don't worry about it } // In Vue you can also use watch() to detect changes: watch( () => props.myObject, (new, old) => { // value change detected console.log(props.myObject.value); // NOTE: this is always true // DO NOT rely on old value new.value === old.value } ); Learn more in README There are also some plans to make it better and helps devs catch bugs/mistakes. Feel free to start using it in your WIs, and ask questions or share feedback if something doesn't work as it's a very early stage.","title":"AngularJS to Vue props reactivity"},{"location":"Guides/Autologin-token-manual-generation/","text":"Find user email and org code Edit and run this code (replace originalString ) (in https://dotnetfiddle.net/ for example): (based on https://devops.wisetechglobal.com/wtg/BorderWise/_git/UMP?path=%2FBackend%2FUmp%2FHelpers%2FCryptoServiceProviders%2FCw1CryptoServiceProvider.cs&_a=contents&version=GBmaster) using System; using System.Security.Cryptography; using System.Text; public class SimpleEncryptor { private TripleDESCryptoServiceProvider CryptoServiceProvider; public SimpleEncryptor() { CryptoServiceProvider = new TripleDESCryptoServiceProvider(); // Disable warning for weak cryptographic algorithm // Set the key CryptoServiceProvider.Key = MD5.HashData(Encoding.ASCII.GetBytes(\"w;lerhpoihpOIYO&)(*&LKJ%$#%$TDFGLKJ;lwkjer;tlkwh\")); CryptoServiceProvider.IV = new byte[8] { 240, 3, 45, 29, 0, 76, 173, 59 }; } public string Encode(string data) { var buffer = Encoding.ASCII.GetBytes(data); var encryptor = CryptoServiceProvider.CreateEncryptor(); var encryptedBuffer = encryptor.TransformFinalBlock(buffer, 0, buffer.Length); return Convert.ToBase64String(encryptedBuffer); } static void Main(string[] args) { var simpleEncryptor = new SimpleEncryptor(); string originalString = \"UserEmail=carol@atoshipping.com&OrgCode=ACROCEMEL2\"; string encryptedString = simpleEncryptor.Encode(originalString); Console.WriteLine(originalString); Console.WriteLine(encryptedString); } } Urlencode the encrypted string and make request: curl -X 'GET' 'https://ump.borderwise.com/api/v1/auth/autoLogin/url?securedQueryString=xVFTBNulvHlQQgZJHe3KtPprlrsli1qGPeQFTD0/ZPiVeWl7YNdJ8aXuGkNSlpo3ki/g9sYd7nY=' -H 'accept: */*' You get URL, use it for logging in: {\"Url\":\"https://app.borderwise.com/?authtoken=uIirYxlnicd4I%2bqoQpcbqnx8ZLoeBl2SU3812C1wzDhmIIMv06ph8McHt69h%2fOF4\",\" It will also work on local for debugging, just make sure to use prod API in .env Happy debugging!","title":"Autologin token manual generation"},{"location":"Guides/BorderWise-Tariff-Classification-User-Guide/","text":"BorderWise Multi-line Tariff Classification BorderWise Multi-line Tariff Classification feature allows users to classify all invoice lines at once and submit the entire data back to CargoWise One. This document provides instructions on how to utilize this feature through the user guide. Step 1: Open WiseCloud Sand and you will see a list of CW1 sand servers, search for the keyword \u201cBatch\u201d by clicking ctrl+F. Open Sand Server \u201cWI00549317BatchMode\u201d. Step 2: Once you access the Sand Server instance, a Username and password prompt will appear. Follow these steps: 1. For the Username, enter \"CW1Support\". 2. To obtain the password, navigate to ediProd and locate the \"Support Token\" option under the Help section in the bottom panel. Copy the password to your clipboard, and then simply paste it directly into the login form for the Sand Server. Step 3: After the CW1 Sand Server instance is opened, browse to Staff browse to Staff and Resources module and create a new Staff Entry with your credentials. Refer to the below screenshot Step 4: Now login with the newly created credentials and browse to \u201cCustoms Declaration\u201d Module, and create a New Customs Declaration. Step 5: On the Customs Declaration screen, navigate to \u201cInv. Headers\u201d tab, and specify Invoice Headers for your declaration. Step 6: Navigate to Inv. Lines tab, and specify the Invoice Lines for your customs Declaration. Step 7: Now when you click on any of the Tariff Code, you should see an ellipsis icon, clicking on which should take you to the BorderWise Application in your default web browser. Step 8: In the BorderWise Application, you will be prompted with the dialog explaining the Multi-Line Tariff Classification feature, you can opt not to see this dialog again before proceeding so that you won\u2019t be prompted with this dialog every time you try to access this feature. Step 9: You should be able to access the Multi-Line Tariff Classification Panel. You can find the explanation of some of the useful features in the Multi-line Tariff Classification Panel Below Dock View: The dock view button on the top right corner allows the user to toggle between Vertical View and Horizontal View. Vertical View Horizontal View Add New Invoice Line: User can add new invoice lines in the tariff classification by clicking on the plus icon highlighted in the screenshot below. User is also allowed to delete these newly added invoice lines. Browse Unclassified Invoice Lines The unclassified tab option in the navbar at the top allows user to only see unclassified invoice lines. After classifying invoice lines in the unclassified tab, user is prompted with a refresh option in the top navbar which will remove classified lines from the Unclassified Tab. Classify Invoice Line In order to classify invoice lines, users can select any of the options below. 1. Directly type a valid Tariff Code in the Tariff input field. 2. Specify the parent heading for the tariff and type \u201cEnter\u201d, which will open the Tariff Table corresponding to that parent heading in the center view, and user can select the appropriate tariff code from the table by clicking on the stat code link. 3. User can type the respective tariff heading in the search box at the top, and select the appropriate tariff code by clicking on the stat code link. BorderWise Single-line Tariff Classification BorderWise also supports single-line tariff classification which allows users to classify individual invoice lines and submit the tariff code back to CargoWise One. Following are the steps to access this feature: Step 1: Open CargoWiseOne, and navigate to Import Classification Lookup / Export Tariff Classification Module. Create a New tariff Classification and a New Classification Lookup Dialog should appear on the screen. Step 2: Click on the ellipsis button adjacent to the Tariff Number field and BorderWise Application will open in your default web browser with the Tariff Table corresponding to the Tariff Number entered in the Classification Lookup form. Step 3: You can select the appropriate Tariff Code by clicking on the stat code link and you will be prompted with the dialog containing the selected Tariff Code. You can opt to not see this dialog again before submitting. Step 4: Click on the \u201dSubmit CW1\u201d button and the selected Tariff Code will be populated in the \u201dTariff Number\u201d field.","title":"BorderWise Multi-line Tariff Classification"},{"location":"Guides/BorderWise-Tariff-Classification-User-Guide/#borderwise-multi-line-tariff-classification","text":"BorderWise Multi-line Tariff Classification feature allows users to classify all invoice lines at once and submit the entire data back to CargoWise One. This document provides instructions on how to utilize this feature through the user guide. Step 1: Open WiseCloud Sand and you will see a list of CW1 sand servers, search for the keyword \u201cBatch\u201d by clicking ctrl+F. Open Sand Server \u201cWI00549317BatchMode\u201d. Step 2: Once you access the Sand Server instance, a Username and password prompt will appear. Follow these steps: 1. For the Username, enter \"CW1Support\". 2. To obtain the password, navigate to ediProd and locate the \"Support Token\" option under the Help section in the bottom panel. Copy the password to your clipboard, and then simply paste it directly into the login form for the Sand Server. Step 3: After the CW1 Sand Server instance is opened, browse to Staff browse to Staff and Resources module and create a new Staff Entry with your credentials. Refer to the below screenshot Step 4: Now login with the newly created credentials and browse to \u201cCustoms Declaration\u201d Module, and create a New Customs Declaration. Step 5: On the Customs Declaration screen, navigate to \u201cInv. Headers\u201d tab, and specify Invoice Headers for your declaration. Step 6: Navigate to Inv. Lines tab, and specify the Invoice Lines for your customs Declaration. Step 7: Now when you click on any of the Tariff Code, you should see an ellipsis icon, clicking on which should take you to the BorderWise Application in your default web browser. Step 8: In the BorderWise Application, you will be prompted with the dialog explaining the Multi-Line Tariff Classification feature, you can opt not to see this dialog again before proceeding so that you won\u2019t be prompted with this dialog every time you try to access this feature. Step 9: You should be able to access the Multi-Line Tariff Classification Panel. You can find the explanation of some of the useful features in the Multi-line Tariff Classification Panel Below","title":"BorderWise Multi-line Tariff Classification"},{"location":"Guides/BorderWise-Tariff-Classification-User-Guide/#dock-view","text":"The dock view button on the top right corner allows the user to toggle between Vertical View and Horizontal View. Vertical View Horizontal View","title":"Dock View:"},{"location":"Guides/BorderWise-Tariff-Classification-User-Guide/#add-new-invoice-line","text":"User can add new invoice lines in the tariff classification by clicking on the plus icon highlighted in the screenshot below. User is also allowed to delete these newly added invoice lines.","title":"Add New Invoice Line:"},{"location":"Guides/BorderWise-Tariff-Classification-User-Guide/#browse-unclassified-invoice-lines","text":"The unclassified tab option in the navbar at the top allows user to only see unclassified invoice lines. After classifying invoice lines in the unclassified tab, user is prompted with a refresh option in the top navbar which will remove classified lines from the Unclassified Tab.","title":"Browse Unclassified Invoice Lines"},{"location":"Guides/BorderWise-Tariff-Classification-User-Guide/#classify-invoice-line","text":"In order to classify invoice lines, users can select any of the options below. 1. Directly type a valid Tariff Code in the Tariff input field. 2. Specify the parent heading for the tariff and type \u201cEnter\u201d, which will open the Tariff Table corresponding to that parent heading in the center view, and user can select the appropriate tariff code from the table by clicking on the stat code link. 3. User can type the respective tariff heading in the search box at the top, and select the appropriate tariff code by clicking on the stat code link.","title":"Classify Invoice Line"},{"location":"Guides/BorderWise-Tariff-Classification-User-Guide/#borderwise-single-line-tariff-classification","text":"BorderWise also supports single-line tariff classification which allows users to classify individual invoice lines and submit the tariff code back to CargoWise One. Following are the steps to access this feature: Step 1: Open CargoWiseOne, and navigate to Import Classification Lookup / Export Tariff Classification Module. Create a New tariff Classification and a New Classification Lookup Dialog should appear on the screen. Step 2: Click on the ellipsis button adjacent to the Tariff Number field and BorderWise Application will open in your default web browser with the Tariff Table corresponding to the Tariff Number entered in the Classification Lookup form. Step 3: You can select the appropriate Tariff Code by clicking on the stat code link and you will be prompted with the dialog containing the selected Tariff Code. You can opt to not see this dialog again before submitting. Step 4: Click on the \u201dSubmit CW1\u201d button and the selected Tariff Code will be populated in the \u201dTariff Number\u201d field.","title":"BorderWise Single-line Tariff Classification"},{"location":"Guides/CSP-Violations/","text":"Getting Started Use csp-reports-analyzer to find new issues, aggregate them, group, etc. Github Copilot helps a lot. Check out Known issues Pro Tip : for checking all the content for 3rd party images of base64 data usage, you can use mongo-html-checker Known issues Note, that this is an exert from Maxim Mazurok's Obsidian, so internal links might not work, but should be good enough Images from wcotradetools.org - fixed by manual team [x] Double check all other content for src=\"h... , etc. Examples: https://www.wcotradetools.org/sites/default/files/nomenclature/inline-images/explanatory-notes/0629_2942_en_2017_fichiers/image001.png https://www.wcotradetools.org/sites/default/files/nomenclature/inline-images/explanatory-notes/0629_2942_en_2017_fichiers/image138.png https://www.wcotradetools.org/s3/global/wco/hsnotes/2022/images/explanatory-notes/0629_2942_en_2017_fichiers/image138.png https://www.wcotradetools.org/sites/default/files/nomenclature/inline-images/explanatory-notes/0629_2925_en_2017_fichiers/image001.gif https://www.wcotradetools.org/sites/default/files/nomenclature/inline-images/explanatory-notes/0739_3905_en_2017_fichiers/image001.gif https://www.wcotradetools.org/sites/default/files/nomenclature/inline-images/explanatory-notes/0629_2924_en_2017_fichiers/image001.gif fonts.gstatic.cn - legit Google fonts for China, allowed Examples: https://fonts.gstatic.cn/s/roboto/v30/KFOkCnqEu92Fr1Mu51xIIzIXKMny.woff2 https://fonts.gstatic.cn/s/roboto/v30/KFOlCnqEu92Fr1MmYUtfChc4AMP6lbBP.woff2 https://fonts.gstatic.cn/s/roboto/v30/KFOlCnqEu92Fr1MmSU5fABc4AMP6lbBP.woff2 etc. collinsdictionary.com - looks like some plugin used for translations/definitions? Examples: https://www.collinsdictionary.com/external/images/info.png?version=5.0.34 https://www.collinsdictionary.com/external/images/info.png?version=5.0.35 https://www.collinsdictionary.com/external/images/info.png?version=5.0.37 https://www.collinsdictionary.com/external/images/lbcefrb/a2.png?version=5.0.35 https://www.collinsdictionary.com/external/images/lbcefrb/b1plus.png?version=5.0.34 https://www.collinsdictionary.com/external/images/lbcefrb/b2.png?version=5.0.35 Already allowed, weird: `sentry.io Examples: https://o827986.ingest.sentry.io/api/5818829/envelope/?sentry_key=dfc9b7af16364687a712017b52545d52&sentry_version=7 hotjar.io Examples: https://metrics.hotjar.io/?v=6 https://static.hotjar.com/c/hotjar-3657851.js?sv=6 borderwise.com Examples: https://app.borderwise.com/api/error-report/frontend-issue https://app.borderwise.com/api/host/version https://app.borderwise.com/api/v1/library/au/tariff-tabs?heading=9404&tariffType=IMP&year=2022 https://app.borderwise.com/api/v1/library/books/61a856726b37123a14272d2a/timestamp https://app.borderwise.com/api/v1/user/license/status/5b40bb5553d34c027765d6140529725b/ https://app.borderwise.com/api/v1/user/search-history https://app.borderwise.com/app/templates/common/generic-tariff-tabs-container.html?v=1702526785863 https://app.borderwise.com/assets/css/style.hsnotes.min.6801e23b.css https://app.borderwise.com/manifest.webmanifest googletagmanager.com - no idea ;( Theory was: if it's blocked using redirect to non-whitelisted domain it will trigger CSP, but it is probably wrong theory because final destination URL will be reported, not the original one Example: https://www.googletagmanager.com/gtag/js?id=G-EVQQLXSL5E cs.hae123.cn - might be extension, users likely not affected From fatkun-pro-mv in URL I found Fatkun Batch Download Image Chrome extension, seems pretty legit and couldn't reproduce the issue Only happened on login page, might be some bots... Examples: https://cs.hae123.cn/gw?name=fatkun-pro-mv3&uid=7da14c81d7284d9e84cc898b70bf214e baidu.com - likely ads from malware Params from URL: \"con wid =240&con hei =350\" suggest it's width and height of the ad? See malware verdict for the same URL as we see in logs Examples: https://pos.baidu.com/dcsm?conwid=240&conhei=350&rdid=6818871&dc=3&di=u6818871&s1=2839721755&dtm=HTML_POST alicdn.com - reliability monitoring, could affect user experience in CN Seems like it detects blank/white pages, most likely part of Alibaba Cloud performance/reliability monitoring. Potentially might be used to report/fix pages with resources unavailable in China? I guess we don't really need it Example: https://g.alicdn.com/woodpeckerx/itrace-next/??itrace-blank.iife.js login.microsoftonline.com - because Sentry is blocked I guess it's most likely related to sentry domain being blocked by security Looks like a legit login for Toll Group based on background image [[Dmitry Kislov]] said \"we don\u2019t have SSO redirects yet but will soon have for WTG users\" Once url-decoded, SAMLRequest part can be decoded using https://www.samltool.com/decode.php, looks like this: xml <?xml version=\"1.0\"?> <AuthnRequest xmlns=\"urn:oasis:names:tc:SAML:2.0:protocol\" Version=\"2.0\" ID=\"_913cf84de6779bcba0ff358c3aef9514f893a8f069862e2714b2b2bcf3d2f947\" IssueInstant=\"2023-12-14T22:00:59Z\"> <Issuer xmlns=\"urn:oasis:names:tc:SAML:2.0:assertion\"> https://saml.threatpulse.net:8443/saml/saml_realm</Issuer> </AuthnRequest> RelayState is base64 encoded ocs=https://o827986.ingest.sentry.io/api/5818829/envelope/?sentry_key=dfc9b7af16364687a712017b52545d52&sentry_version=7, in all examples Examples: https://login.microsoftonline.com/0f004b2e-fb07-45e7-a568-caf4905b0339/saml2?SAMLRequest=jZA7a8QwEIT7%2BxVG%2Fdl6%2BSFh%2BzhIc5A0SUiR5pB1K3xgy45WDvn5EQ6BlGFhi53h22Ha09c8ZZ8Q8L74jrCcklN%2FaM9bHP0zfGyAMUsOjx3ZgteLwTtqb2ZAHa1%2BOT89ap5TvYYlLnaZSPb2i0pnkl0eOnJVTFjXyBtUda0GOxjqnCgbKww4VTLpGiVM42ilmooDr5kceBrrxI07JeuEQdzg4jEaHxOZcnFk%2FMjkK%2BeaUl2qd9K3uyn8J61BhBBTStKPMa6oiwLNPOVxDGDiuk0IuYeoGynFruzrmsRpboufP31b%2FC2pP3wD&RelayState=b2NzPWh0dHBzOi8vbzgyNzk4Ni5pbmdlc3Quc2VudHJ5LmlvL2FwaS81ODE4ODI5L2VudmVsb3BlLz9zZW50cnlfa2V5PWRmYzliN2FmMTYzNjQ2ODdhNzEyMDE3YjUyNTQ1ZDUyJnNlbnRyeV92ZXJzaW9uPTcs https://login.microsoftonline.com/0f004b2e-fb07-45e7-a568-caf4905b0339/saml2?SAMLRequest=jZAxa8MwEIX3%2FAqjPbYky7YsbIdAlkC7tKVDl6BIMg7YsqM7h%2F78qi6Fjl0O7t7ju8drDp%2FTmDxcgNvsW8JSSg7drjmuOPgXd18dYBIdHlqyBq9mDTdQXk8OFBr1enx%2BUjylagkzzmYeSfL%2Bi4pnkpxPLbkYlltbWylkrjXrWW8LV0lem8rUvCzjJqkRheivdV0wKazk1ljj7LUqaSW%2BMQCrO3tA7TGSKc%2F3jO85e%2BNMFVxR9kG6ZjOF%2F6TVAC5gTEm6AXEBlWWgpzHFITiNyzqCS71DJYXIN2UblyiOU5P9%2FOma7G9J3e4L&RelayState=b2NzPWh0dHBzOi8vbzgyNzk4Ni5pbmdlc3Quc2VudHJ5LmlvL2FwaS81ODE4ODI5L2VudmVsb3BlLz9zZW50cnlfa2V5PWRmYzliN2FmMTYzNjQ2ODdhNzEyMDE3YjUyNTQ1ZDUyJnNlbnRyeV92ZXJzaW9uPTcs https://login.microsoftonline.com/0f004b2e-fb07-45e7-a568-caf4905b0339/saml2?SAMLRequest=jZAxa8MwFIT3%2FAqjPbYky0okbIdAl0C6tKVDlyDLEg7Ysqv3HPrzK1wKHbvc8O747nj16Wsas4eLcJ9DQ1hOyand1ecVh%2FDiPlcHmKVEgIasMejZwB10MJMDjVa%2Fnp%2BvmudUL3HG2c4jyd5%2FUelMsstTQ27CW6G6ikmpjKS%2Bkp0yJRPG%2BL7nvKdcdlIpJaQtbcl6dvDMV4Iy6QU9uAQBWN0lAJqAiUt5uWd8z49vnGmhNBMfpK23UPzPVgPgIqaNpB0QF9BFAWYacxyiM7isI7g8ONRHIcrN2eSWzHGqi5%2Beti7%2BvqjdfQM%3D&RelayState=b2NzPWh0dHBzOi8vbzgyNzk4Ni5pbmdlc3Quc2VudHJ5LmlvL2FwaS81ODE4ODI5L2VudmVsb3BlLz9zZW50cnlfa2V5PWRmYzliN2FmMTYzNjQ2ODdhNzEyMDE3YjUyNTQ1ZDUyJnNlbnRyeV92ZXJzaW9uPTcs trendmicro.com - Trend Micro's Password Manager - could affect user experience Examples: https://pwm-image.trendmicro.com https://pwm-image.trendmicro.com/5.8/extensionFrame/styles/engineV3.css effirst.com - mobile apps monitoring platform in CN Alibaba mobile applications monitoring platform, happened once on login, users likely not affected Examples: https://px.effirst.com/api/v1/jssdk/upload?wpk-header=app%3D9zxuc2j4-4ogpwyg4%26cp%3Dnone%26de%3D4%26seq%3D1702623996903%26tm%3D1702623996%26ud%3Dc4a9cfcc-782c-4420-92af-c774019af6ba%26ver%3D%26type%3Dflow%26sver%3D2.3.4%26sign%3D9bf8a190ef82c5049df7b199c599c45b&uc_param_str=prveosfrnwutmisv&data=%7B%22uid%22%3A%22%22%2C%22wid%22%3A%22c4a9cfcc-782c-4420-92af-c774019af6ba%22%2C%22type%22%3A%22flow%22%2C%22category%22%3A5%2C%22w_cnt%22%3A1%2C%22w_rel%22%3A%22%22%2C%22w_bid%22%3A%229zxuc2j4-4ogpwyg4%22%2C%22w_cid%22%3A%22%22%2C%22w_spa%22%3Afalse%2C%22w_frmid%22%3A%2272c6aa58-8eb2-45e2-2e68-f3244f481c3d%22%2C%22w_tm%22%3A1702623996897%2C%22log_src%22%3A%22jssdk%22%2C%22sdk_ver%22%3A%222.3.4%22%2C%22w_url%22%3A%22https%3A%2F%2Fapp.borderwise.com%2Faccount%2Flogin%22%2C%22w_query%22%3A%22%22%2C%22w_ref%22%3A%22%22%2C%22w_title%22%3A%22BorderWise%22%2C%22ua%22%3A%22Mozilla%2F5.0%20(iPhone%3B%20CPU%20iPhone%20OS%2017_0_3%20like%20Mac%20OS%20X%3B%20zh-cn)%20AppleWebKit%2F601.1.46%20(KHTML%2C%20like%20Gecko)%20Mobile%2F21A360%20Quark%2F6.7.7.1942%20Mobile%22%2C%22referrer%22%3A%22https%3A%2F%2Fapp.borderwise.com%2F%22%2C%22dsp_dpi%22%3A3%2C%22dsp_w%22%3A393%2C%22dsp_h%22%3A852%2C%22net%22%3A%22%22%2C%22w_send_mode%22%3A%22img%22%7D tql.com - most likely some Truckload corporate extension... Resources are likely from tql.com intranet Examples: https://svcs.tql.com/CrossReferenceContent/libraries/bootstrap/fonts/glyphicons-halflings-regular.ttf https://svcs.tql.com/CrossReferenceContent/libraries/bootstrap/fonts/glyphicons-halflings-regular.woff https://svcs.tql.com/CrossReferenceContent/libraries/bootstrap/fonts/glyphicons-halflings-regular.woff2 data Login and app pages, including with authtoken param Violated directives: font-src - we don't load fonts using data, none of the content does it either media-src - only for <audio> and <video> which we don't use, I checked Source files: googletagmanager.com/gtag/js - only font-src violated, kinda weird, it doesn't show anything, likely some proxy error response Empty - violates both font-src and media-src ; Empty source means inline stuff or eval Checked all content for all possibilities of data, didn't find anything, so it all must be coming from unknown 3rd party sources like extensions, etc. [x] Finish checking all fields recursively (for direct books, etc.) for known issues [x] src=\"http... [x] src=\"??? [x] audio/video/track [x] data:font","title":"Getting Started"},{"location":"Guides/CSP-Violations/#getting-started","text":"Use csp-reports-analyzer to find new issues, aggregate them, group, etc. Github Copilot helps a lot. Check out Known issues Pro Tip : for checking all the content for 3rd party images of base64 data usage, you can use mongo-html-checker","title":"Getting Started"},{"location":"Guides/CSP-Violations/#known-issues","text":"Note, that this is an exert from Maxim Mazurok's Obsidian, so internal links might not work, but should be good enough Images from wcotradetools.org - fixed by manual team [x] Double check all other content for src=\"h... , etc. Examples: https://www.wcotradetools.org/sites/default/files/nomenclature/inline-images/explanatory-notes/0629_2942_en_2017_fichiers/image001.png https://www.wcotradetools.org/sites/default/files/nomenclature/inline-images/explanatory-notes/0629_2942_en_2017_fichiers/image138.png https://www.wcotradetools.org/s3/global/wco/hsnotes/2022/images/explanatory-notes/0629_2942_en_2017_fichiers/image138.png https://www.wcotradetools.org/sites/default/files/nomenclature/inline-images/explanatory-notes/0629_2925_en_2017_fichiers/image001.gif https://www.wcotradetools.org/sites/default/files/nomenclature/inline-images/explanatory-notes/0739_3905_en_2017_fichiers/image001.gif https://www.wcotradetools.org/sites/default/files/nomenclature/inline-images/explanatory-notes/0629_2924_en_2017_fichiers/image001.gif fonts.gstatic.cn - legit Google fonts for China, allowed Examples: https://fonts.gstatic.cn/s/roboto/v30/KFOkCnqEu92Fr1Mu51xIIzIXKMny.woff2 https://fonts.gstatic.cn/s/roboto/v30/KFOlCnqEu92Fr1MmYUtfChc4AMP6lbBP.woff2 https://fonts.gstatic.cn/s/roboto/v30/KFOlCnqEu92Fr1MmSU5fABc4AMP6lbBP.woff2 etc. collinsdictionary.com - looks like some plugin used for translations/definitions? Examples: https://www.collinsdictionary.com/external/images/info.png?version=5.0.34 https://www.collinsdictionary.com/external/images/info.png?version=5.0.35 https://www.collinsdictionary.com/external/images/info.png?version=5.0.37 https://www.collinsdictionary.com/external/images/lbcefrb/a2.png?version=5.0.35 https://www.collinsdictionary.com/external/images/lbcefrb/b1plus.png?version=5.0.34 https://www.collinsdictionary.com/external/images/lbcefrb/b2.png?version=5.0.35 Already allowed, weird: `sentry.io Examples: https://o827986.ingest.sentry.io/api/5818829/envelope/?sentry_key=dfc9b7af16364687a712017b52545d52&sentry_version=7 hotjar.io Examples: https://metrics.hotjar.io/?v=6 https://static.hotjar.com/c/hotjar-3657851.js?sv=6 borderwise.com Examples: https://app.borderwise.com/api/error-report/frontend-issue https://app.borderwise.com/api/host/version https://app.borderwise.com/api/v1/library/au/tariff-tabs?heading=9404&tariffType=IMP&year=2022 https://app.borderwise.com/api/v1/library/books/61a856726b37123a14272d2a/timestamp https://app.borderwise.com/api/v1/user/license/status/5b40bb5553d34c027765d6140529725b/ https://app.borderwise.com/api/v1/user/search-history https://app.borderwise.com/app/templates/common/generic-tariff-tabs-container.html?v=1702526785863 https://app.borderwise.com/assets/css/style.hsnotes.min.6801e23b.css https://app.borderwise.com/manifest.webmanifest googletagmanager.com - no idea ;( Theory was: if it's blocked using redirect to non-whitelisted domain it will trigger CSP, but it is probably wrong theory because final destination URL will be reported, not the original one Example: https://www.googletagmanager.com/gtag/js?id=G-EVQQLXSL5E cs.hae123.cn - might be extension, users likely not affected From fatkun-pro-mv in URL I found Fatkun Batch Download Image Chrome extension, seems pretty legit and couldn't reproduce the issue Only happened on login page, might be some bots... Examples: https://cs.hae123.cn/gw?name=fatkun-pro-mv3&uid=7da14c81d7284d9e84cc898b70bf214e baidu.com - likely ads from malware Params from URL: \"con wid =240&con hei =350\" suggest it's width and height of the ad? See malware verdict for the same URL as we see in logs Examples: https://pos.baidu.com/dcsm?conwid=240&conhei=350&rdid=6818871&dc=3&di=u6818871&s1=2839721755&dtm=HTML_POST alicdn.com - reliability monitoring, could affect user experience in CN Seems like it detects blank/white pages, most likely part of Alibaba Cloud performance/reliability monitoring. Potentially might be used to report/fix pages with resources unavailable in China? I guess we don't really need it Example: https://g.alicdn.com/woodpeckerx/itrace-next/??itrace-blank.iife.js login.microsoftonline.com - because Sentry is blocked I guess it's most likely related to sentry domain being blocked by security Looks like a legit login for Toll Group based on background image [[Dmitry Kislov]] said \"we don\u2019t have SSO redirects yet but will soon have for WTG users\" Once url-decoded, SAMLRequest part can be decoded using https://www.samltool.com/decode.php, looks like this: xml <?xml version=\"1.0\"?> <AuthnRequest xmlns=\"urn:oasis:names:tc:SAML:2.0:protocol\" Version=\"2.0\" ID=\"_913cf84de6779bcba0ff358c3aef9514f893a8f069862e2714b2b2bcf3d2f947\" IssueInstant=\"2023-12-14T22:00:59Z\"> <Issuer xmlns=\"urn:oasis:names:tc:SAML:2.0:assertion\"> https://saml.threatpulse.net:8443/saml/saml_realm</Issuer> </AuthnRequest> RelayState is base64 encoded ocs=https://o827986.ingest.sentry.io/api/5818829/envelope/?sentry_key=dfc9b7af16364687a712017b52545d52&sentry_version=7, in all examples Examples: https://login.microsoftonline.com/0f004b2e-fb07-45e7-a568-caf4905b0339/saml2?SAMLRequest=jZA7a8QwEIT7%2BxVG%2Fdl6%2BSFh%2BzhIc5A0SUiR5pB1K3xgy45WDvn5EQ6BlGFhi53h22Ha09c8ZZ8Q8L74jrCcklN%2FaM9bHP0zfGyAMUsOjx3ZgteLwTtqb2ZAHa1%2BOT89ap5TvYYlLnaZSPb2i0pnkl0eOnJVTFjXyBtUda0GOxjqnCgbKww4VTLpGiVM42ilmooDr5kceBrrxI07JeuEQdzg4jEaHxOZcnFk%2FMjkK%2BeaUl2qd9K3uyn8J61BhBBTStKPMa6oiwLNPOVxDGDiuk0IuYeoGynFruzrmsRpboufP31b%2FC2pP3wD&RelayState=b2NzPWh0dHBzOi8vbzgyNzk4Ni5pbmdlc3Quc2VudHJ5LmlvL2FwaS81ODE4ODI5L2VudmVsb3BlLz9zZW50cnlfa2V5PWRmYzliN2FmMTYzNjQ2ODdhNzEyMDE3YjUyNTQ1ZDUyJnNlbnRyeV92ZXJzaW9uPTcs https://login.microsoftonline.com/0f004b2e-fb07-45e7-a568-caf4905b0339/saml2?SAMLRequest=jZAxa8MwEIX3%2FAqjPbYky7YsbIdAlkC7tKVDl6BIMg7YsqM7h%2F78qi6Fjl0O7t7ju8drDp%2FTmDxcgNvsW8JSSg7drjmuOPgXd18dYBIdHlqyBq9mDTdQXk8OFBr1enx%2BUjylagkzzmYeSfL%2Bi4pnkpxPLbkYlltbWylkrjXrWW8LV0lem8rUvCzjJqkRheivdV0wKazk1ljj7LUqaSW%2BMQCrO3tA7TGSKc%2F3jO85e%2BNMFVxR9kG6ZjOF%2F6TVAC5gTEm6AXEBlWWgpzHFITiNyzqCS71DJYXIN2UblyiOU5P9%2FOma7G9J3e4L&RelayState=b2NzPWh0dHBzOi8vbzgyNzk4Ni5pbmdlc3Quc2VudHJ5LmlvL2FwaS81ODE4ODI5L2VudmVsb3BlLz9zZW50cnlfa2V5PWRmYzliN2FmMTYzNjQ2ODdhNzEyMDE3YjUyNTQ1ZDUyJnNlbnRyeV92ZXJzaW9uPTcs https://login.microsoftonline.com/0f004b2e-fb07-45e7-a568-caf4905b0339/saml2?SAMLRequest=jZAxa8MwFIT3%2FAqjPbYky0okbIdAl0C6tKVDlyDLEg7Ysqv3HPrzK1wKHbvc8O747nj16Wsas4eLcJ9DQ1hOyand1ecVh%2FDiPlcHmKVEgIasMejZwB10MJMDjVa%2Fnp%2BvmudUL3HG2c4jyd5%2FUelMsstTQ27CW6G6ikmpjKS%2Bkp0yJRPG%2BL7nvKdcdlIpJaQtbcl6dvDMV4Iy6QU9uAQBWN0lAJqAiUt5uWd8z49vnGmhNBMfpK23UPzPVgPgIqaNpB0QF9BFAWYacxyiM7isI7g8ONRHIcrN2eSWzHGqi5%2Beti7%2BvqjdfQM%3D&RelayState=b2NzPWh0dHBzOi8vbzgyNzk4Ni5pbmdlc3Quc2VudHJ5LmlvL2FwaS81ODE4ODI5L2VudmVsb3BlLz9zZW50cnlfa2V5PWRmYzliN2FmMTYzNjQ2ODdhNzEyMDE3YjUyNTQ1ZDUyJnNlbnRyeV92ZXJzaW9uPTcs trendmicro.com - Trend Micro's Password Manager - could affect user experience Examples: https://pwm-image.trendmicro.com https://pwm-image.trendmicro.com/5.8/extensionFrame/styles/engineV3.css effirst.com - mobile apps monitoring platform in CN Alibaba mobile applications monitoring platform, happened once on login, users likely not affected Examples: https://px.effirst.com/api/v1/jssdk/upload?wpk-header=app%3D9zxuc2j4-4ogpwyg4%26cp%3Dnone%26de%3D4%26seq%3D1702623996903%26tm%3D1702623996%26ud%3Dc4a9cfcc-782c-4420-92af-c774019af6ba%26ver%3D%26type%3Dflow%26sver%3D2.3.4%26sign%3D9bf8a190ef82c5049df7b199c599c45b&uc_param_str=prveosfrnwutmisv&data=%7B%22uid%22%3A%22%22%2C%22wid%22%3A%22c4a9cfcc-782c-4420-92af-c774019af6ba%22%2C%22type%22%3A%22flow%22%2C%22category%22%3A5%2C%22w_cnt%22%3A1%2C%22w_rel%22%3A%22%22%2C%22w_bid%22%3A%229zxuc2j4-4ogpwyg4%22%2C%22w_cid%22%3A%22%22%2C%22w_spa%22%3Afalse%2C%22w_frmid%22%3A%2272c6aa58-8eb2-45e2-2e68-f3244f481c3d%22%2C%22w_tm%22%3A1702623996897%2C%22log_src%22%3A%22jssdk%22%2C%22sdk_ver%22%3A%222.3.4%22%2C%22w_url%22%3A%22https%3A%2F%2Fapp.borderwise.com%2Faccount%2Flogin%22%2C%22w_query%22%3A%22%22%2C%22w_ref%22%3A%22%22%2C%22w_title%22%3A%22BorderWise%22%2C%22ua%22%3A%22Mozilla%2F5.0%20(iPhone%3B%20CPU%20iPhone%20OS%2017_0_3%20like%20Mac%20OS%20X%3B%20zh-cn)%20AppleWebKit%2F601.1.46%20(KHTML%2C%20like%20Gecko)%20Mobile%2F21A360%20Quark%2F6.7.7.1942%20Mobile%22%2C%22referrer%22%3A%22https%3A%2F%2Fapp.borderwise.com%2F%22%2C%22dsp_dpi%22%3A3%2C%22dsp_w%22%3A393%2C%22dsp_h%22%3A852%2C%22net%22%3A%22%22%2C%22w_send_mode%22%3A%22img%22%7D tql.com - most likely some Truckload corporate extension... Resources are likely from tql.com intranet Examples: https://svcs.tql.com/CrossReferenceContent/libraries/bootstrap/fonts/glyphicons-halflings-regular.ttf https://svcs.tql.com/CrossReferenceContent/libraries/bootstrap/fonts/glyphicons-halflings-regular.woff https://svcs.tql.com/CrossReferenceContent/libraries/bootstrap/fonts/glyphicons-halflings-regular.woff2 data Login and app pages, including with authtoken param Violated directives: font-src - we don't load fonts using data, none of the content does it either media-src - only for <audio> and <video> which we don't use, I checked Source files: googletagmanager.com/gtag/js - only font-src violated, kinda weird, it doesn't show anything, likely some proxy error response Empty - violates both font-src and media-src ; Empty source means inline stuff or eval Checked all content for all possibilities of data, didn't find anything, so it all must be coming from unknown 3rd party sources like extensions, etc. [x] Finish checking all fields recursively (for direct books, etc.) for known issues [x] src=\"http... [x] src=\"??? [x] audio/video/track [x] data:font","title":"Known issues"},{"location":"Guides/CapDev-or-R%26D-%253F/","text":"Remember that we usually treat WIs as defects if the functionality has already been released to users and it's not working as it used to before. Examples of NOT defects: - functionality is in master branch but not available to customers => not defect better to classify as a refactor - functionality is not working as expected but it has never worked this way => product enhancement","title":"CapDev or R&D %3F"},{"location":"Guides/Creating-a-UMP-Test-Database-Backup/","text":"Open Microsoft SQL Server Management Studio Connect to the UMP Test database server: Server type: Database Engine Server name: sydsp-ssql-8.sand.wtg.zone Authentication: SQU Server Authentication Login: OdysseyAdmin Password: ask a team member Open the 'Databases' folder in the Object Explorer Right click 'BorderWise_UMP_Test' Select 'Tasks' > 'Back Up...' Configure the backup Database: BorderWise_UMP_Test Backup type: Full Backup component: Database Back up to: Disk Add: \\\\sydsp-ssql-8.sand.wtg.zone\\SQL_Backup\\BorderWise_UMP\\<backup name>.bak Press 'OK' Go to \\\\sydsp-ssql-8.sand.wtg.zone\\SQL_Backup\\BorderWise_UMP\\<backup name>.bak in your file explorer to find the backup","title":"Creating a UMP Test Database Backup"},{"location":"Guides/Docker-%28podman%29/","text":"Following are my posts from Containerization and orchestrators MS Teams channel. Hopefully they will help you set up Docker/Podman on local. So as I've shared recently I'm switching to WTG laptop instead of personal PC due to performance reasons. And I don't have Docker Desktop on WTG laptop, so I decided to give Podman a try. Since I'm using WSL for performance reasons, I will be using Podman in WSL as well, not in Windows. As soon as I figured out that we need to install podman from Kubic repository to get the latest version - it was smooth sailing. Just changed docker in my commands to podman and it works. A quick guide: Follow instructions (follow the instructions under \"If you would prefer newer...\") to install podman from Kubic repository (NOT from official repositories) ( important! , see why ) Run sudo apt install podman-docker - to get docker=podman alias working even outside of bash (in scripts/tests that is) Run docker login -u <username> -p <password> -v docker.io (need to sign up for account on docker website and use credentials here ) Edit /etc/containers/registries.conf and make sure that short-name-mode=\"permissive\" (will need sudo privileges to write changes) Replace host.docker.internal with host.containers.internal - if you use it somewhere to reference host machine IP (won't work with podman v3 from official repo, need v4 from Kubic repo, see ) To run docker-rebuild-and-run.sh locally you might need to update references to %%API_HTTP_HOST%% to a real URL. Note for Running Docker.spec tests - Make sure you run npm ci and npm run build-dev beforehand Hi guys, When installing podman in Ubuntu, make sure to use Kubic repository (endorsed by podman) because it has the latest version (v4). While the official Ubuntu repository has a way outdated podman v3. (Note: This may no longer be a problem by 25/04/2024 when the new Ubuntu LTS comes out) Instructions: https://podman.io/getting-started/installation#ubuntu Credits: https://github.com/containers/podman/discussions/15586#discussioncomment-3725602 Also I've updated the guide above: added podman-docker package for setting up the alias removed registry config, because v4 comes with all the registries pre-configured removed notes about network issues as they are completely resolved by using v4 Cheers! If you ever need docker-compose - use https://github.com/containers/podman-compose instead","title":"Docker (podman)"},{"location":"Guides/Entity-Framework%253A-How-to-make-a-UMP-or-WW-database-migration/","text":"The UMP project uses Entity Framework Core as its object-database mapper. In order to update the database you will need to add a new migration which provides the relevant instructions to upgrade the database from the previous version to the new one. The migration also contains instruction on how to downgrade from the new version to the previous one. I will use the example of adding a new required boolean isEnabled column to the OrgContact table. Follow these instructions to complete your first migration: Ensure you have the EF Core tools installed: EF Core tools reference (.NET CLI) - EF Core Edit the model you wish to update: Within the Database project locate the Models folder and the OrgContact model: Database/Models/OrgContact.cs Add a field for the new column: public bool OcIsEnabled { get; set; } This defines that the column should be of type bool (bit) and that it is required Add additional context: In the same project find DatabaseContext : Database/DatabaseContext.cs Update the respective modelBuilder.Entity<>() call that defines how the model is mapped to the database table In this example the following would be added: entity.Property(e => e.OcIsEnabled) .IsRequired() .HasColumnName(\"OC_IsEnabled\") .HasDefaultValue(true); Add a new migration ( official documentation ): Open the command line in Visual Studio: Tools > Command Line > Developer Command Prompt Run the following command: dotnet ef migrations add <NameOfMigration> --project Database , in this case AddOcIsEnabled This should create a new file in the Migrations folder within the Database project named something like XXXXXXXXXXXXXX_AddOcIsEnabled.cs The new file should contain two methods, Up and Down that are already populated with migration instructions. To apply the migration run the following command in the same command prompt as previously: dotnet ef database update --project Database Similarly to undo the migration run dotnet ef database update <NameOfPreviousMigration> --project Database Note : the migration will also be applied automatically when the API is run locally Verify that the database has been updated in Microsoft SQL Server Management Studio To remove the last migration from the solution run the following: dotnet ef migrations remove --project Database This should not be done after the migration has already been applied to the production database","title":"Entity Framework%3A How to make a UMP or WW database migration"},{"location":"Guides/Generating-MongoDB-Transformation-Files-in-BWW/","text":"Prereq Ensure you have the dotnet-script tool installed before proceeding, which can be installed with the following powershell command: dotnet tool install -g dotnet-script Generating Transformation Files From the DataTransformation folder in the Core project within the BWW Backend solution, run the following powershell command: dotnet script .\\GenerateTransformation.csx \"<your description>\" Where is the description of your transformation Example Command run: dotnet script .\\GenerateTransformation.csx \"Add ChapterHeadingMapping Collection for non-existent items\" Resulting file: Transform_20230912063825_Add_ChapterHeadingMapping_Collection_for_nonexistent_items.cs Resulting file contents:","title":"Generating MongoDB Transformation Files in BWW"},{"location":"Guides/Generating-MongoDB-Transformation-Files-in-BWW/#prereq","text":"Ensure you have the dotnet-script tool installed before proceeding, which can be installed with the following powershell command: dotnet tool install -g dotnet-script","title":"Prereq"},{"location":"Guides/Generating-MongoDB-Transformation-Files-in-BWW/#generating-transformation-files","text":"From the DataTransformation folder in the Core project within the BWW Backend solution, run the following powershell command: dotnet script .\\GenerateTransformation.csx \"<your description>\" Where is the description of your transformation","title":"Generating Transformation Files"},{"location":"Guides/Generating-MongoDB-Transformation-Files-in-BWW/#example","text":"Command run: dotnet script .\\GenerateTransformation.csx \"Add ChapterHeadingMapping Collection for non-existent items\" Resulting file: Transform_20230912063825_Add_ChapterHeadingMapping_Collection_for_nonexistent_items.cs Resulting file contents:","title":"Example"},{"location":"Guides/How-to-encode-passwords-for-DAT/","text":"Run the following script in Powershell to encode a password/secret. The resulting value can be stored in source control and will be decrypted by DAT during deployment. $plaintext = (Read-Host -Prompt \"Enter text to encrypt\"); $plaintextBytes = [System.Text.Encoding]::UTF8.GetBytes($plaintext); $crypto = [System.Security.Cryptography.RSACryptoServiceProvider]::new(); try { $crypto.FromXmlString(\"<RSAKeyValue><Modulus>uJFr8lEdDZ6fUXQTiwKRVMrhS7t4oR6/eCuajCQARwIjO3v46HPefEgyMFC6uxGyQg+S7n4xAIfN5a4Ax5yimn5XQNamxgNRQpBQ5mcrGx7r/vRfLqUtt54N4pgwq77arJEMJDEtU+B3wjF+5bMUE1gHmc3Cs5XzGwWbFLHMmWk=</Modulus><Exponent>AQAB</Exponent></RSAKeyValue>\"); $encrypted = $crypto.Encrypt($plaintextBytes, $true); Write-Host ([Convert]::ToBase64String($encrypted)) } finally { $crypto.Dispose() } Contact your team lead to store the original password/secret in https://safe.wisetechglobal.com/","title":"How to encode passwords for DAT"},{"location":"Guides/How-to-setup-dynamic-blue-tab/","text":"DynamicBlueTabEntry Collection There is a collection DynamicBlueTabEntry which contains all dynamic blue tab references. To make a tariff tab \"dynamic,\" you need to create a record in this collection: { \"headingLinksCode\": \"caDumpings\", \"year\": \"2022\", \"countryCode\": \"CA\", \"data\": [ { \"bookCode\": \"CA_DMRS\", \"fieldName\": \"hsCode\", \"headingLength\": 4, \"rangeSplitter\": \" \" } ] } Where caDumpings is a code from the TariffTabEntries collection. The DynamicBlueTabEntry 's data array is required to have at least one record that contains parameters to extract \"active\" headings. Options BookCode and TOC Title Used to extract a collection, with the field ref containing heading numbers used in this blue tab. json { \"bookCode\": \"CA_TARIFF_CON_2021_2022\", \"tocTitle\": \"HS 2021-HS 2022\", \"fieldName\": \"ref\", \"headingLength\": 4 } Just BookCode Used to extract a collection with the hsCode field. json { \"bookCode\": \"CA_DMRS\", \"fieldName\": \"hsCode\", \"headingLength\": 4 } Direct Collection Name Specify the collection name directly. json { \"collectionName\": \"US_EN_PGA_2022\", \"fieldName\": \"ref\", \"headingLength\": 4 } Filter Query Option to specify a filterQuery to apply to the collection for extracting headings. json { \"collectionName\": \"ZA_EN_ImportTariff_Schedule1_Parts_2022\", \"fieldName\": \"ref\", \"headingLength\": 4, \"filterQuery\": \"{ partNo: 2, part: '2a' }\" } Heading Range Field Option to Specify range fields to get 'from' and 'to' values, where 'from' and 'to' is names of fields in the collection: { \"collectionName\": \"US_EN_ImportTariff_SectionNotes_AdditionalNotes_Tab_2022\", \"headingFieldRange\": { \"from\": \"headingFrom\", \"to\": \"headingTo\" } } Headings List Instead of referencing a field in a collection, you can provide a static list of headings. json { \"headingLength\": 4, \"headingsRange\": \"2710 3826\" } If the data array contains more than one element, they are combined by an \"OR\" condition: { \"headingLinksCode\": \"hsConcordance\", \"year\": \"2022\", \"countryCode\": \"US\", \"data\": [ { \"bookCode\": \"US_CONCO_HTS_2021_2022\", \"tocTitle\": \"Concordance HTS 2022-2021\", \"fieldName\": \"ref\", \"headingLength\": 4, \"rangeSplitter\": \" \" }, { \"bookCode\": \"US_CONCO_HTS_2021_2022\", \"tocTitle\": \"Concordance HTS 2021-2022\", \"fieldName\": \"ref\", \"headingLength\": 4, \"rangeSplitter\": \" \" } ] } Dynamic Blue Tabs Update If there is a parameter bookCode , it will update when the corresponding book is updated. Use the book's timestamp . If there is no bookCode , the DynamicBlueTab updates once a week. To trigger the update, you can unset DynamicBlueTab.timestamp for the corresponding records.","title":"How to setup dynamic blue tab"},{"location":"Guides/How-to-setup-dynamic-blue-tab/#dynamicbluetabentry-collection","text":"There is a collection DynamicBlueTabEntry which contains all dynamic blue tab references. To make a tariff tab \"dynamic,\" you need to create a record in this collection: { \"headingLinksCode\": \"caDumpings\", \"year\": \"2022\", \"countryCode\": \"CA\", \"data\": [ { \"bookCode\": \"CA_DMRS\", \"fieldName\": \"hsCode\", \"headingLength\": 4, \"rangeSplitter\": \" \" } ] } Where caDumpings is a code from the TariffTabEntries collection. The DynamicBlueTabEntry 's data array is required to have at least one record that contains parameters to extract \"active\" headings.","title":"DynamicBlueTabEntry Collection"},{"location":"Guides/How-to-setup-dynamic-blue-tab/#options","text":"BookCode and TOC Title Used to extract a collection, with the field ref containing heading numbers used in this blue tab. json { \"bookCode\": \"CA_TARIFF_CON_2021_2022\", \"tocTitle\": \"HS 2021-HS 2022\", \"fieldName\": \"ref\", \"headingLength\": 4 } Just BookCode Used to extract a collection with the hsCode field. json { \"bookCode\": \"CA_DMRS\", \"fieldName\": \"hsCode\", \"headingLength\": 4 } Direct Collection Name Specify the collection name directly. json { \"collectionName\": \"US_EN_PGA_2022\", \"fieldName\": \"ref\", \"headingLength\": 4 } Filter Query Option to specify a filterQuery to apply to the collection for extracting headings. json { \"collectionName\": \"ZA_EN_ImportTariff_Schedule1_Parts_2022\", \"fieldName\": \"ref\", \"headingLength\": 4, \"filterQuery\": \"{ partNo: 2, part: '2a' }\" } Heading Range Field Option to Specify range fields to get 'from' and 'to' values, where 'from' and 'to' is names of fields in the collection: { \"collectionName\": \"US_EN_ImportTariff_SectionNotes_AdditionalNotes_Tab_2022\", \"headingFieldRange\": { \"from\": \"headingFrom\", \"to\": \"headingTo\" } } Headings List Instead of referencing a field in a collection, you can provide a static list of headings. json { \"headingLength\": 4, \"headingsRange\": \"2710 3826\" } If the data array contains more than one element, they are combined by an \"OR\" condition: { \"headingLinksCode\": \"hsConcordance\", \"year\": \"2022\", \"countryCode\": \"US\", \"data\": [ { \"bookCode\": \"US_CONCO_HTS_2021_2022\", \"tocTitle\": \"Concordance HTS 2022-2021\", \"fieldName\": \"ref\", \"headingLength\": 4, \"rangeSplitter\": \" \" }, { \"bookCode\": \"US_CONCO_HTS_2021_2022\", \"tocTitle\": \"Concordance HTS 2021-2022\", \"fieldName\": \"ref\", \"headingLength\": 4, \"rangeSplitter\": \" \" } ] }","title":"Options"},{"location":"Guides/How-to-setup-dynamic-blue-tab/#dynamic-blue-tabs-update","text":"If there is a parameter bookCode , it will update when the corresponding book is updated. Use the book's timestamp . If there is no bookCode , the DynamicBlueTab updates once a week. To trigger the update, you can unset DynamicBlueTab.timestamp for the corresponding records.","title":"Dynamic Blue Tabs Update"},{"location":"Guides/NPM/","text":"devDependencies TL;DR: always use dependencies , never use devDependencies or --save-dev Explanation: Traditionally it's considered good practice to separate your production dependencies required to run your code from your development dependencies required for testing, for example. It makes sense when you're writing plan JS files that use Express as a dependency to run API server, and then you have dev dependency jest to test it. Then to run your code on production you only need to run npm install --production and it will install express but jest. However, in most modern scenarios and especially for front-end it doesn't make sense to separate them. For example, for the front-end projects you don't actually run any code in production, all you do is just building dist folder, and then deploying it as static files. One could argue that in this case your deps is everything you need for the build, like gulp or React, and the devDeps is everything you need for testing like jest. However in most CI/CD setups you will have build and test as part of the same pipeline. So you will do npm ci && npm run build && npm test and then deploy results of the build if tests pass. It is also considered safer since sometimes there can be variance in the build processes. For example, in gulp when you create a bundle - your files can be concatenated in random order, and sometimes it results in issues. So you want to make sure that dist you're deploying is exactly the same dist as you tested. devDeps are also relevant when someone is installing your project as a dependency. For example, Vuetify might be using jest for running tests. But if we use mocha for running tests - we don't need to install jest just to use Vuetify . So jest would be devDep of Vuetify that we don't need to install. Since nobody is installing our app/project as a dependency - we don't care about devDeps. One could argue that in this case there's no difference between using devDeps or not, but I've seen some plugins and tools that help analyze your dependencies to act weirdly if you use devDeps, so it's safer to just not use them unless you're creating a library package that will be published and consumed by other node projects. Patches Sometimes we need to fix something in our dependencies. We should aim to do that by creating an issue and/or PR to the dependency's project on GitHub. Sometimes we can't wait for it to be merged. Other times we need a fix that is only applicable to our use-case. In these rare circumstances, we have to patch our dependencies. This is done using patch-package . Keeping track It's very important to keep track of what we're patching, why, how and to have a plan of when the patch will be removed. Every patch is bad for maintenance, so we always should find or create an issue on GitHub and link it in the patch file comments. For smaller patches we use first lines of the *.patch file for comments. They are ignored by the patcher. Make sure to include as much details as possible, so that we know why it is there, what it does and when should it be removed. See other patch files in BWW and WW for examples. In some cases, like with orval - they have minified code in node_modules, making it extremely hard to see what the patch actually does. In this case it is even more important to keep track of changes. In my case, I created a README with patching instructions in the patches folder and created a bww branch in my fork, into which I merged all other branches with fixes, and created PRs to the main repo as well. How to upgrade dependency for which we have a patch For example, here's how to upgrade vuetify in WW from v3.3.3 to v3.3.8: 1. do npm ci on a clean master 1. do npm install vuetify@3.3.8 1. do npx patch-package - it will show warning saying that we have patch for v3.3.3 but trying to apply it to v3.3.8; worst case scenario - it will show error, unable to apply patch. - If there was a warning - it's usually fine, follow its instructions, run npx patch-package vuetify and manually review notes from the patch, verify that they are either resolved in vuetify new version and remove that note, or verify that required code changes are in the patch. You can use git blame to find what changes were added for which feature comment. - If there was an error - you need to see old patch file, go to the node_modules files, manually do changes where they are still needed, potentially some code could change drastically and new solution will be required. After all done run npx patch-package vuetify . This command generates new patch file based on the diff between npm version of vuetify and your local one in node_modules.","title":"devDependencies"},{"location":"Guides/NPM/#devdependencies","text":"TL;DR: always use dependencies , never use devDependencies or --save-dev Explanation: Traditionally it's considered good practice to separate your production dependencies required to run your code from your development dependencies required for testing, for example. It makes sense when you're writing plan JS files that use Express as a dependency to run API server, and then you have dev dependency jest to test it. Then to run your code on production you only need to run npm install --production and it will install express but jest. However, in most modern scenarios and especially for front-end it doesn't make sense to separate them. For example, for the front-end projects you don't actually run any code in production, all you do is just building dist folder, and then deploying it as static files. One could argue that in this case your deps is everything you need for the build, like gulp or React, and the devDeps is everything you need for testing like jest. However in most CI/CD setups you will have build and test as part of the same pipeline. So you will do npm ci && npm run build && npm test and then deploy results of the build if tests pass. It is also considered safer since sometimes there can be variance in the build processes. For example, in gulp when you create a bundle - your files can be concatenated in random order, and sometimes it results in issues. So you want to make sure that dist you're deploying is exactly the same dist as you tested. devDeps are also relevant when someone is installing your project as a dependency. For example, Vuetify might be using jest for running tests. But if we use mocha for running tests - we don't need to install jest just to use Vuetify . So jest would be devDep of Vuetify that we don't need to install. Since nobody is installing our app/project as a dependency - we don't care about devDeps. One could argue that in this case there's no difference between using devDeps or not, but I've seen some plugins and tools that help analyze your dependencies to act weirdly if you use devDeps, so it's safer to just not use them unless you're creating a library package that will be published and consumed by other node projects.","title":"devDependencies"},{"location":"Guides/NPM/#patches","text":"Sometimes we need to fix something in our dependencies. We should aim to do that by creating an issue and/or PR to the dependency's project on GitHub. Sometimes we can't wait for it to be merged. Other times we need a fix that is only applicable to our use-case. In these rare circumstances, we have to patch our dependencies. This is done using patch-package .","title":"Patches"},{"location":"Guides/NPM/#keeping-track","text":"It's very important to keep track of what we're patching, why, how and to have a plan of when the patch will be removed. Every patch is bad for maintenance, so we always should find or create an issue on GitHub and link it in the patch file comments. For smaller patches we use first lines of the *.patch file for comments. They are ignored by the patcher. Make sure to include as much details as possible, so that we know why it is there, what it does and when should it be removed. See other patch files in BWW and WW for examples. In some cases, like with orval - they have minified code in node_modules, making it extremely hard to see what the patch actually does. In this case it is even more important to keep track of changes. In my case, I created a README with patching instructions in the patches folder and created a bww branch in my fork, into which I merged all other branches with fixes, and created PRs to the main repo as well.","title":"Keeping track"},{"location":"Guides/NPM/#how-to-upgrade-dependency-for-which-we-have-a-patch","text":"For example, here's how to upgrade vuetify in WW from v3.3.3 to v3.3.8: 1. do npm ci on a clean master 1. do npm install vuetify@3.3.8 1. do npx patch-package - it will show warning saying that we have patch for v3.3.3 but trying to apply it to v3.3.8; worst case scenario - it will show error, unable to apply patch. - If there was a warning - it's usually fine, follow its instructions, run npx patch-package vuetify and manually review notes from the patch, verify that they are either resolved in vuetify new version and remove that note, or verify that required code changes are in the patch. You can use git blame to find what changes were added for which feature comment. - If there was an error - you need to see old patch file, go to the node_modules files, manually do changes where they are still needed, potentially some code could change drastically and new solution will be required. After all done run npx patch-package vuetify . This command generates new patch file based on the diff between npm version of vuetify and your local one in node_modules.","title":"How to upgrade dependency for which we have a patch"},{"location":"Guides/Orval-%26-Vue-Query/","text":"Caveats (READ ME) Reactivity Orval does not seem to recognize changes to prop values. This means that if you are passing a prop as a parameter to a useQuery hook, the query won't be re-run when the prop value is updated. To work around this issue, convert the prop to a ref in order to make it reactive. Example from /Frontend/src/views/content-setup/content-definition/ConfigurationTab.tsx : \u274c Bad code: setup(props) { const apiExtractionLineQuery = useApiExtractionLineGETAllOfSelected(props.itemPK); const customExtractionLineQuery = useCustomExtractionLineGETAllOfSelected(props.itemPK); const webExtractionLineQuery = useWebExtractionLineGETAllOfSelected(props.itemPK); } \u2705 Good code: setup(props) { const itemPK = toRef(() => props.itemPK); const apiExtractionLineQuery = useApiExtractionLineGETAllOfSelected(itemPK); const customExtractionLineQuery = useCustomExtractionLineGETAllOfSelected(itemPK); const webExtractionLineQuery = useWebExtractionLineGETAllOfSelected(itemPK); } Overview Generation Process Backend build generates a swagger.json file containing the OpenAPI schema that describes all of the endpoints that the backend exposes. This file is stored at /Swagger/swagger.json . Frontend has an npm script swagger:generate which runs Orval. This is automatically done post install as well. Orval uses the defined config at /Frontend/orval.config.js to generate the queries and respective hooks that correspond to the various endpoints outlined in the swagger schema file. The generated code is located at /Frontend/src/api . ( Note: the generated code is ignored by git ) To use any of the queries or hooks simply import them from /Frontend/src/api .","title":"Caveats (READ ME)"},{"location":"Guides/Orval-%26-Vue-Query/#caveats-read-me","text":"","title":"Caveats (READ ME)"},{"location":"Guides/Orval-%26-Vue-Query/#reactivity","text":"Orval does not seem to recognize changes to prop values. This means that if you are passing a prop as a parameter to a useQuery hook, the query won't be re-run when the prop value is updated. To work around this issue, convert the prop to a ref in order to make it reactive. Example from /Frontend/src/views/content-setup/content-definition/ConfigurationTab.tsx : \u274c Bad code: setup(props) { const apiExtractionLineQuery = useApiExtractionLineGETAllOfSelected(props.itemPK); const customExtractionLineQuery = useCustomExtractionLineGETAllOfSelected(props.itemPK); const webExtractionLineQuery = useWebExtractionLineGETAllOfSelected(props.itemPK); } \u2705 Good code: setup(props) { const itemPK = toRef(() => props.itemPK); const apiExtractionLineQuery = useApiExtractionLineGETAllOfSelected(itemPK); const customExtractionLineQuery = useCustomExtractionLineGETAllOfSelected(itemPK); const webExtractionLineQuery = useWebExtractionLineGETAllOfSelected(itemPK); }","title":"Reactivity"},{"location":"Guides/Orval-%26-Vue-Query/#overview","text":"","title":"Overview"},{"location":"Guides/Orval-%26-Vue-Query/#generation-process","text":"Backend build generates a swagger.json file containing the OpenAPI schema that describes all of the endpoints that the backend exposes. This file is stored at /Swagger/swagger.json . Frontend has an npm script swagger:generate which runs Orval. This is automatically done post install as well. Orval uses the defined config at /Frontend/orval.config.js to generate the queries and respective hooks that correspond to the various endpoints outlined in the swagger schema file. The generated code is located at /Frontend/src/api . ( Note: the generated code is ignored by git ) To use any of the queries or hooks simply import them from /Frontend/src/api .","title":"Generation Process"},{"location":"Guides/PostgreSQL-Servers-%28Test-%26-Prod%29/","text":"Web Watcher Common Setup Set up Jumphost Follow these instructions to set up Jumphost SSH into a server We usually use PuTTY to ssh into the different servers through Jumphost. Check with BorderWise team members if they have some shortcuts they're willing to share. If you want to set up your own shortcuts: Download the PuTTY client ( .exe ) and place it in a folder Create a Windows shortcut with the PuTTY executable as the target in the same folder Append -ssh borderwise@<server URL> to the Target. It should look like <path to PuTTY> -ssh borderwise@<server URL> Create shortcuts for whichever servers you need to connect to Copy the folder with PuTTY and the shortcuts into Jumphost. This can be easily done by selecting the folder and pressing Ctrl + C followed by Ctrl + V in Jumphost When connecting to servers you will be prompted for passwords. Reach out to a BorderWise team member if you need the password for a specific server Test TODO: Verify that these instructions work. Original documentation Server: au2wt-sdkr-2.test.wisecloud.zone Server Setup host$ docker-compose build host$ docker-compose up -d host$ docker exec -it postgresql_db_1 bash root@container# su - postgres Change password of postgres db user: postgres@container# psql postgres=# \\password postgres , then type the password You can request the password from a team lead Passwords are stored in https://safe.wisetechglobal.com/ postgres=# exit Create the DB user: postgres@container# createuser -U postgres testing -P Creates a new user from the postgres user called 'testing' and prompts for a password. Password should match the one in stored in https://safe.wisetechglobal.com/ Create the DB: createdb -O testing webwatcher_test Creates the database 'webwatcher_test' which is owned by the user 'testing' Open pgAdmin Create a new server for Test if it isn't there already and enter the relevant host address, username, and password. Follow instructions from [https://wisetechglobal.sharepoint.com/sites/StackOverflow/questions/Forms/AllItems.aspx?id=%2Fsites%2FStackOverflow%2Fquestions%2F10763%2Emd&parent=%2Fsites%2FStackOverflow%2Fquestions) to restore data. Production Original documentation Servers: - Primary: au2wp-sbps-401.wisecloud.zone - Secondary: au2wp-sbps-402.wisecloud.zone Server Setup Primary Check that there is a 'main' cluster on the server: pg_lsclusters If there isn't create a new cluster called 'main' like so: pg_createcluster 14 main Create a new user to be used to replicate the cluster: createuser -U postgres secondary -P --replication Stop the cluster if it is running: sudo systemctl stop postgresql Update the config file to enable connections and replication: /etc/postgresql/14/main/postgresql.conf Set listen_addresses = '*' Set wal_level = replica Update the authentication config file to allow connections /etc/postgresql/14/main/pg_hba.conf . Add the following lines to the end of the file: host replication all 10.2.88.44/32 md5 (To allow replication from the secondary server) host webwatcher webwatcher 10.61.198.26/32 md5 (To allow pgAdmin to connect to the webwatcher database as the webwatcher user) host webwatcher webwatcher 10.2.67.111/32 md5 (To allow WebWatcher BE to connect to the webwatcher database as the webwatcher user) host webwatcher webwatcher 10.2.67.112/32 md5 (To allow WebWatcher BE to connect to the webwatcher database as the webwatcher user) Start the cluster: sudo systemctl start postgresql Check that it is running: pg_lsclusters Open pgAdmin Create a new server for Prod if it isn't there already and enter the relevant host address, username, and password. Follow instructions from this Stack Overflow post to restore data. Secondary Generate an SSH key: ssh-keygen -t ed25519 (Not sure if this is necessary) Copy the generated ~/.ssh/id_ed25519.pub to the primary server's postgres user's home. Save it as ~/.ssh/authorized_keys (Not sure if this is necessary) Connect from the secondary server to the primary server and confirm the server ssh fingerprint: (Not sure if this is necessary) On secondary: ssh postgres@<primary IP> On primary: ssh postgres@localhost Confirm that the ssh fingerprint for both are the same Complete the connection from the secondary server to the primary to save the fingerprint Sign in as the postgres : su postgres The password should be the same as the server password Check that there is a 'main' cluster on the server: pg_lsclusters If there isn't create a new cluster called 'main' like so: pg_createcluster 14 main Stop the cluster if it is running: Log out of the postgres user: exit Stop the cluster: sudo systemctl stop postgresql Sign in as the postgres again: su postgres Remove all of the files in the data directory: rm -rv /var/lib/postgresql/14/main/ Start replicating from the primary server: pg_basebackup -h 10.2.88.43 -U secondary -X stream -C -S <replica slot name> -v -R -W -D /var/lib/postgresql/14/main/ -h : Host, the IP address of the primary server -U : Replication user, the user secondary which we created on the primary server -X : Stream value tells pg_basebackup to stream from the primary server -C : Creates a replication slot -S : Names the replication slot, this can be called anything. replica_1 for example. -v : Prints verbose output of the process -R : Instructs server to operate as standby server for the primary server -W : Prompts to provide password for the replication user secondary -D : The directory for the backed up files Log out of the postgres user: exit Start the cluster: sudo systemctl start postgresql Verify that the primary server is streaming to the secondary server. On the primary server: Open the postgres command line interface as the postgres user: sudo -u postgres psql Enter and run the query SELECT client_addr, state FROM pg_stat_replication; There should be one row with the IP address of the secondary server and state as streaming Verify that the secondary server is mirroring the data in the primary server. On the secondary server: Open the postgres command line interface as the postgres user: sudo -u postgres psql Run some queries on the database and see if the output matches the same queries on the primary server. Alternatively: run queries through pgAdmin . Setup regular backups on the secondary server: Sign in as the postgres : su postgres Copy the backup scripts to the home directory of the postgres user /var/lib/postgresql/ dump_backup.sh ( Original source ) invoke_backup_cleaner.sh ( Original source ) backup_cleaner.py ( Original source ) Create a dumps directory /var/lib/postgresql/dumps/ Make sure that the scripts have executable rights and the dumps directory has writable rights Set up cron jobs: crontab -e ``` m h dom mon dow command 0 */3 * * * /var/lib/postgresql/dump_backup.sh 0 2 * * * /var/lib/postgresql/invoke_backup_cleaner.sh `` - Runs dump_backup.sh every 3 hours - Runs invoke_backup_cleaner.sh` every day on the second hour of the day Replication instruction inspired by this Recover From Backup Copy most recent backup zip to the data directory of the cluster you want to recover Backup zips are stored on the secondary server at /var/lib/postgresql/dumps/ Extract it using tar -xvf <backup>.tar.gz Move the extracted contents up into the data directory: mv <backup>/* <data directory> This should give an error for some sub-directories which cannot be overridden since they have existing files in them Empty the failed directories in the data directory: rm -r <failed directory>/* Run the move command again: `mv <backup>/* <data directory> Remove standby.signal if it is in the data directory Start the cluster Steps if Primary Goes Down Update the connection strings in WebWatcher BE to point to the secondary server Update both Backend/src/WebApi/publish.appsettings.Production.json and Backend/src/Scheduler/publish.appsettings.Production.json On the secondary server: Stop the cluster if it is running: sudo systemctl stop postgresql Update the config file to enable connections if it hasn't been already: /etc/postgresql/14/main/postgresql.conf Set listen_addresses = '*' Update the authentication config file to allow connections if it hasn't been already /etc/postgresql/14/main/pg_hba.conf . Add the following lines to the end of the file: host webwatcher webwatcher 10.2.67.111/32 md5 (To allow WebWatcher BE to connect to the webwatcher database as the webwatcher user) host webwatcher webwatcher 10.2.67.112/32 md5 (To allow WebWatcher BE to connect to the webwatcher database as the webwatcher user) Start the cluster: sudo systemctl start postgresql Sign in as the postgres again: su postgres Promote the cluster to no longer run in standby mode: pg_ctlcluster 14 main promote If this is not done the cluster will be read-only Deploy the updated connection strings to WebWatcher production Fix the primary server and restore its database Can use a backup from /var/lib/postgresql/dumps/ on the secondary server or from somewhere else Revert the changes to the connection string in WebWatcher BE so that they point back to the primary server Deploy the reverted connection strings Set up replication on the secondary server again following the instructions from above Password rotation Generate a new password. 1. Generate new password using the password generator on https://safe.wisetechglobal.com [Use the WTG_SVC_Account settings] 2. Update the password on https://safe.wisetechglobal.com 3. Encrypt the password for use on DAT by running the following command in Powershell - $plaintext = (Read-Host -Prompt \"Enter text to encrypt\"); $plaintextBytes = [System.Text.Encoding]::UTF8.GetBytes($plaintext); $crypto = [System.Security.Cryptography.RSACryptoServiceProvider]::new(); try { $crypto.FromXmlString(\"<RSAKeyValue><Modulus>uJFr8lEdDZ6fUXQTiwKRVMrhS7t4oR6/eCuajCQARwIjO3v46HPefEgyMFC6uxGyQg+S7n4xAIfN5a4Ax5yimn5XQNamxgNRQpBQ5mcrGx7r/vRfLqUtt54N4pgwq77arJEMJDEtU+B3wjF+5bMUE1gHmc3Cs5XzGwWbFLHMmWk=</Modulus><Exponent>AQAB</Exponent></RSAKeyValue>\"); $encrypted = $crypto.Encrypt($plaintextBytes, $true); Write-Host ([Convert]::ToBase64String($encrypted)) } finally { $crypto.Dispose() } - See this article on StackOverflow for more detail Update WebWatcher\\Deployment\\Deployment\\WebWatcherConfigFileUpdater.cs with the new plaintext encrypted password generated in step 2 After CH0 finished Update postgresql server password Restart postgresql process Deploy new build on Crikey","title":"Web Watcher"},{"location":"Guides/PostgreSQL-Servers-%28Test-%26-Prod%29/#web-watcher","text":"","title":"Web Watcher"},{"location":"Guides/PostgreSQL-Servers-%28Test-%26-Prod%29/#common-setup","text":"Set up Jumphost Follow these instructions to set up Jumphost SSH into a server We usually use PuTTY to ssh into the different servers through Jumphost. Check with BorderWise team members if they have some shortcuts they're willing to share. If you want to set up your own shortcuts: Download the PuTTY client ( .exe ) and place it in a folder Create a Windows shortcut with the PuTTY executable as the target in the same folder Append -ssh borderwise@<server URL> to the Target. It should look like <path to PuTTY> -ssh borderwise@<server URL> Create shortcuts for whichever servers you need to connect to Copy the folder with PuTTY and the shortcuts into Jumphost. This can be easily done by selecting the folder and pressing Ctrl + C followed by Ctrl + V in Jumphost When connecting to servers you will be prompted for passwords. Reach out to a BorderWise team member if you need the password for a specific server","title":"Common Setup"},{"location":"Guides/PostgreSQL-Servers-%28Test-%26-Prod%29/#test","text":"TODO: Verify that these instructions work. Original documentation Server: au2wt-sdkr-2.test.wisecloud.zone","title":"Test"},{"location":"Guides/PostgreSQL-Servers-%28Test-%26-Prod%29/#server-setup","text":"host$ docker-compose build host$ docker-compose up -d host$ docker exec -it postgresql_db_1 bash root@container# su - postgres Change password of postgres db user: postgres@container# psql postgres=# \\password postgres , then type the password You can request the password from a team lead Passwords are stored in https://safe.wisetechglobal.com/ postgres=# exit Create the DB user: postgres@container# createuser -U postgres testing -P Creates a new user from the postgres user called 'testing' and prompts for a password. Password should match the one in stored in https://safe.wisetechglobal.com/ Create the DB: createdb -O testing webwatcher_test Creates the database 'webwatcher_test' which is owned by the user 'testing' Open pgAdmin Create a new server for Test if it isn't there already and enter the relevant host address, username, and password. Follow instructions from [https://wisetechglobal.sharepoint.com/sites/StackOverflow/questions/Forms/AllItems.aspx?id=%2Fsites%2FStackOverflow%2Fquestions%2F10763%2Emd&parent=%2Fsites%2FStackOverflow%2Fquestions) to restore data.","title":"Server Setup"},{"location":"Guides/PostgreSQL-Servers-%28Test-%26-Prod%29/#production","text":"Original documentation Servers: - Primary: au2wp-sbps-401.wisecloud.zone - Secondary: au2wp-sbps-402.wisecloud.zone","title":"Production"},{"location":"Guides/PostgreSQL-Servers-%28Test-%26-Prod%29/#server-setup_1","text":"Primary Check that there is a 'main' cluster on the server: pg_lsclusters If there isn't create a new cluster called 'main' like so: pg_createcluster 14 main Create a new user to be used to replicate the cluster: createuser -U postgres secondary -P --replication Stop the cluster if it is running: sudo systemctl stop postgresql Update the config file to enable connections and replication: /etc/postgresql/14/main/postgresql.conf Set listen_addresses = '*' Set wal_level = replica Update the authentication config file to allow connections /etc/postgresql/14/main/pg_hba.conf . Add the following lines to the end of the file: host replication all 10.2.88.44/32 md5 (To allow replication from the secondary server) host webwatcher webwatcher 10.61.198.26/32 md5 (To allow pgAdmin to connect to the webwatcher database as the webwatcher user) host webwatcher webwatcher 10.2.67.111/32 md5 (To allow WebWatcher BE to connect to the webwatcher database as the webwatcher user) host webwatcher webwatcher 10.2.67.112/32 md5 (To allow WebWatcher BE to connect to the webwatcher database as the webwatcher user) Start the cluster: sudo systemctl start postgresql Check that it is running: pg_lsclusters Open pgAdmin Create a new server for Prod if it isn't there already and enter the relevant host address, username, and password. Follow instructions from this Stack Overflow post to restore data. Secondary Generate an SSH key: ssh-keygen -t ed25519 (Not sure if this is necessary) Copy the generated ~/.ssh/id_ed25519.pub to the primary server's postgres user's home. Save it as ~/.ssh/authorized_keys (Not sure if this is necessary) Connect from the secondary server to the primary server and confirm the server ssh fingerprint: (Not sure if this is necessary) On secondary: ssh postgres@<primary IP> On primary: ssh postgres@localhost Confirm that the ssh fingerprint for both are the same Complete the connection from the secondary server to the primary to save the fingerprint Sign in as the postgres : su postgres The password should be the same as the server password Check that there is a 'main' cluster on the server: pg_lsclusters If there isn't create a new cluster called 'main' like so: pg_createcluster 14 main Stop the cluster if it is running: Log out of the postgres user: exit Stop the cluster: sudo systemctl stop postgresql Sign in as the postgres again: su postgres Remove all of the files in the data directory: rm -rv /var/lib/postgresql/14/main/ Start replicating from the primary server: pg_basebackup -h 10.2.88.43 -U secondary -X stream -C -S <replica slot name> -v -R -W -D /var/lib/postgresql/14/main/ -h : Host, the IP address of the primary server -U : Replication user, the user secondary which we created on the primary server -X : Stream value tells pg_basebackup to stream from the primary server -C : Creates a replication slot -S : Names the replication slot, this can be called anything. replica_1 for example. -v : Prints verbose output of the process -R : Instructs server to operate as standby server for the primary server -W : Prompts to provide password for the replication user secondary -D : The directory for the backed up files Log out of the postgres user: exit Start the cluster: sudo systemctl start postgresql Verify that the primary server is streaming to the secondary server. On the primary server: Open the postgres command line interface as the postgres user: sudo -u postgres psql Enter and run the query SELECT client_addr, state FROM pg_stat_replication; There should be one row with the IP address of the secondary server and state as streaming Verify that the secondary server is mirroring the data in the primary server. On the secondary server: Open the postgres command line interface as the postgres user: sudo -u postgres psql Run some queries on the database and see if the output matches the same queries on the primary server. Alternatively: run queries through pgAdmin . Setup regular backups on the secondary server: Sign in as the postgres : su postgres Copy the backup scripts to the home directory of the postgres user /var/lib/postgresql/ dump_backup.sh ( Original source ) invoke_backup_cleaner.sh ( Original source ) backup_cleaner.py ( Original source ) Create a dumps directory /var/lib/postgresql/dumps/ Make sure that the scripts have executable rights and the dumps directory has writable rights Set up cron jobs: crontab -e ```","title":"Server Setup"},{"location":"Guides/PostgreSQL-Servers-%28Test-%26-Prod%29/#m-h-dom-mon-dow-command","text":"0 */3 * * * /var/lib/postgresql/dump_backup.sh 0 2 * * * /var/lib/postgresql/invoke_backup_cleaner.sh `` - Runs dump_backup.sh every 3 hours - Runs invoke_backup_cleaner.sh` every day on the second hour of the day Replication instruction inspired by this","title":"m h dom mon dow command"},{"location":"Guides/PostgreSQL-Servers-%28Test-%26-Prod%29/#recover-from-backup","text":"Copy most recent backup zip to the data directory of the cluster you want to recover Backup zips are stored on the secondary server at /var/lib/postgresql/dumps/ Extract it using tar -xvf <backup>.tar.gz Move the extracted contents up into the data directory: mv <backup>/* <data directory> This should give an error for some sub-directories which cannot be overridden since they have existing files in them Empty the failed directories in the data directory: rm -r <failed directory>/* Run the move command again: `mv <backup>/* <data directory> Remove standby.signal if it is in the data directory Start the cluster","title":"Recover From Backup"},{"location":"Guides/PostgreSQL-Servers-%28Test-%26-Prod%29/#steps-if-primary-goes-down","text":"Update the connection strings in WebWatcher BE to point to the secondary server Update both Backend/src/WebApi/publish.appsettings.Production.json and Backend/src/Scheduler/publish.appsettings.Production.json On the secondary server: Stop the cluster if it is running: sudo systemctl stop postgresql Update the config file to enable connections if it hasn't been already: /etc/postgresql/14/main/postgresql.conf Set listen_addresses = '*' Update the authentication config file to allow connections if it hasn't been already /etc/postgresql/14/main/pg_hba.conf . Add the following lines to the end of the file: host webwatcher webwatcher 10.2.67.111/32 md5 (To allow WebWatcher BE to connect to the webwatcher database as the webwatcher user) host webwatcher webwatcher 10.2.67.112/32 md5 (To allow WebWatcher BE to connect to the webwatcher database as the webwatcher user) Start the cluster: sudo systemctl start postgresql Sign in as the postgres again: su postgres Promote the cluster to no longer run in standby mode: pg_ctlcluster 14 main promote If this is not done the cluster will be read-only Deploy the updated connection strings to WebWatcher production Fix the primary server and restore its database Can use a backup from /var/lib/postgresql/dumps/ on the secondary server or from somewhere else Revert the changes to the connection string in WebWatcher BE so that they point back to the primary server Deploy the reverted connection strings Set up replication on the secondary server again following the instructions from above","title":"Steps if Primary Goes Down"},{"location":"Guides/PostgreSQL-Servers-%28Test-%26-Prod%29/#password-rotation","text":"Generate a new password. 1. Generate new password using the password generator on https://safe.wisetechglobal.com [Use the WTG_SVC_Account settings] 2. Update the password on https://safe.wisetechglobal.com 3. Encrypt the password for use on DAT by running the following command in Powershell - $plaintext = (Read-Host -Prompt \"Enter text to encrypt\"); $plaintextBytes = [System.Text.Encoding]::UTF8.GetBytes($plaintext); $crypto = [System.Security.Cryptography.RSACryptoServiceProvider]::new(); try { $crypto.FromXmlString(\"<RSAKeyValue><Modulus>uJFr8lEdDZ6fUXQTiwKRVMrhS7t4oR6/eCuajCQARwIjO3v46HPefEgyMFC6uxGyQg+S7n4xAIfN5a4Ax5yimn5XQNamxgNRQpBQ5mcrGx7r/vRfLqUtt54N4pgwq77arJEMJDEtU+B3wjF+5bMUE1gHmc3Cs5XzGwWbFLHMmWk=</Modulus><Exponent>AQAB</Exponent></RSAKeyValue>\"); $encrypted = $crypto.Encrypt($plaintextBytes, $true); Write-Host ([Convert]::ToBase64String($encrypted)) } finally { $crypto.Dispose() } - See this article on StackOverflow for more detail Update WebWatcher\\Deployment\\Deployment\\WebWatcherConfigFileUpdater.cs with the new plaintext encrypted password generated in step 2 After CH0 finished Update postgresql server password Restart postgresql process Deploy new build on Crikey","title":"Password rotation"},{"location":"Guides/Running-BWW-FE-on-HTTPS-locally/","text":"Based on https://stackoverflow.com/a/42298344/4536543 1. cd ~/BorderWiseWeb/Frontend/src 1. openssl req -x509 -newkey rsa:2048 -keyout keytmp.pem -out cert.pem -days 365 -passout pass:1234 -subj \"/\" (passphrase 1234 and all default options) 1. openssl rsa -in keytmp.pem -out key.pem -passin pass:1234 (passphrase 1234 from before) 1. In src/http-server.js : - replace const http = require('http'); with const https = require('https'); - replace const server = http.createServer((req, res) => { with const server = https.createServer({ key: fs.readFileSync('./key.pem'), cert: fs.readFileSync('./cert.pem') }, (req, res) => { 1. Run npm run serve as normal 1. In browser, open https://localhost:3000 1. When you see a warning - use \"Advanced\" -> \"Proceed anyway\" (on Chrome), see more Notes: - Sometimes it will be hanging in loading state, I think it has something to do with service worker, just do a hard reload Ctrl + Shift + R, I think it'll disable service worker and website will load - If running npm start or npm run build - might have to disable findUnusedFiles task in gulp To make it run like app.borderwise.com (WIP): add 127.0.0.1 app.borderwise.com to widnows hosts file change defaultBWWHostPort to 443 use this to make nvm work in sudo : https://stackoverflow.com/a/40078875/4536543 run sudo npm start","title":"Running BWW FE on HTTPS locally"},{"location":"Guides/TSX-and-JSX-Templates-in-Vue/","text":"In the wild most often you will find docs/examples of Vue code written using Vue.js Templates which looks more like regular HTML. What we are using in BWW and WW is TSX (JSX + TypeScript) templates, which uses similar syntax to React. You can find more on it here https://vuejs.org/guide/extras/render-function.html#jsx-tsx And various examples here: https://github.com/vuejs/babel-plugin-jsx#usage Short summary of differences: - v-for => [].map() - @click => onClick - :some-value => just someValue - v-model=\"bla\" => v-model={bla} - some-attribute=\"some string\" => someAttribute=\"some string\" or someAttribute={\"some string\"} - <a v-if=\"bla\"/> => bla && <a />","title":"TSX and JSX Templates in Vue"},{"location":"Guides/Tariff-Data-Sources/","text":"This section will describe the sources that we use to collect Tariff Data. Currently, we collect data for the following regions/countries: Pacific - Australia - New Zealand - Singapore Americas - United States - Canada - Mexico Europe - United Kingdom & Northern Ireland - European Union Africa - South Africa Asia - China & Taiwan","title":"Tariff Data Sources"},{"location":"Guides/Test-rigs-%28UATs%29-in-BorderWise/","text":"Test rigs in BorderWise In BWW we have now dynamic test rigs. This guide is intended for new BorderWise product team members AND developers wishing to have their code functionally tested. How do I create a test rig for BorderWise? The steps below outline how to include the test rig template in a SHV or UAT task. Create a new SHV task The task can either be assigned to the developer or to yourself (if you wish to be notified of the status of the build) Paste in the Pull Request URL as the first line inside task notes Right click in the Task Notes box. Select Insert Template > \u201cBorderWise UAT\u201d. Change the task type depending on the situations outlined below, then press save. SH0 \u2013 if you require the unit tests to be run as well as the test rig to be created. If no test-rig parameter is provided in task notes, DAT runs unit tests only. UA0 \u2013 if you just want to build the test rig (test-rig parameter is mandatory). The UAT link passed in task note is used to create the container name during frontend and backend deployment. The UAT link pattern as per different projects is as below: BorderWise UAT: https://{WorkItemNumber}.app-uat.borderwise.com WebWatcher UAT: https://webwatcher-{WorkItemNumber}.app-uat.borderwise.com UMP UAT: https://ump-{WorkItemNumber}.app-uat.borderwise.com If there are multiple workflows under your work item for which if a developer needs to SH0 or UA0, then the developer should make sure workitemnumber value passed in UAT link in the task note is unique. Otherwise, deployment will override the container. To make the value unique, the developer can append suffixes such as: https://{WorkItemNumber}{WorkflowNumber}.app-uat.borderwise.com When the build is complete, the UAT link in SH0/UA0 task will become active. You can also use Crikey Test Results page to access details of the test rig deployment. BorderWise application is deployed as package where frontend and backend are deployed together in test rig. Frontend ports range from 401 to 449 and Backend range from 301 to 349 a. From the Crikey test results under BorderWise/BorderWiseWeb/DynamicTestRigDeployment, copy the WebApi Url to access backend b. From the Crikey test results under BorderWise/BorderWiseWeb/DynamicTestRigDeployment, copy the front-end url to access the BorderWise application If there are changes in both BorderWise and UMP, we can set up both applications. Include the UAT URLs for both applications in the task notes in Step 5 and refer to the screenshot below for guidance. Alternatively, if there is only a change in BorderWise but you want it to connect to the Custom UAT test rig, you can provide the UMP UAT: {URL} for BW to connect to. If no UMP UAT URL is mentioned, BorderWise will connect to UMPBaseUrl from appsettings.UAT.json Click here to see the original article on this topic.","title":"Test rigs in BorderWise"},{"location":"Guides/Test-rigs-%28UATs%29-in-BorderWise/#test-rigs-in-borderwise","text":"In BWW we have now dynamic test rigs. This guide is intended for new BorderWise product team members AND developers wishing to have their code functionally tested.","title":"Test rigs in BorderWise"},{"location":"Guides/Test-rigs-%28UATs%29-in-BorderWise/#how-do-i-create-a-test-rig-for-borderwise","text":"The steps below outline how to include the test rig template in a SHV or UAT task. Create a new SHV task The task can either be assigned to the developer or to yourself (if you wish to be notified of the status of the build) Paste in the Pull Request URL as the first line inside task notes Right click in the Task Notes box. Select Insert Template > \u201cBorderWise UAT\u201d. Change the task type depending on the situations outlined below, then press save. SH0 \u2013 if you require the unit tests to be run as well as the test rig to be created. If no test-rig parameter is provided in task notes, DAT runs unit tests only. UA0 \u2013 if you just want to build the test rig (test-rig parameter is mandatory). The UAT link passed in task note is used to create the container name during frontend and backend deployment. The UAT link pattern as per different projects is as below: BorderWise UAT: https://{WorkItemNumber}.app-uat.borderwise.com WebWatcher UAT: https://webwatcher-{WorkItemNumber}.app-uat.borderwise.com UMP UAT: https://ump-{WorkItemNumber}.app-uat.borderwise.com If there are multiple workflows under your work item for which if a developer needs to SH0 or UA0, then the developer should make sure workitemnumber value passed in UAT link in the task note is unique. Otherwise, deployment will override the container. To make the value unique, the developer can append suffixes such as: https://{WorkItemNumber}{WorkflowNumber}.app-uat.borderwise.com When the build is complete, the UAT link in SH0/UA0 task will become active. You can also use Crikey Test Results page to access details of the test rig deployment. BorderWise application is deployed as package where frontend and backend are deployed together in test rig. Frontend ports range from 401 to 449 and Backend range from 301 to 349 a. From the Crikey test results under BorderWise/BorderWiseWeb/DynamicTestRigDeployment, copy the WebApi Url to access backend b. From the Crikey test results under BorderWise/BorderWiseWeb/DynamicTestRigDeployment, copy the front-end url to access the BorderWise application If there are changes in both BorderWise and UMP, we can set up both applications. Include the UAT URLs for both applications in the task notes in Step 5 and refer to the screenshot below for guidance. Alternatively, if there is only a change in BorderWise but you want it to connect to the Custom UAT test rig, you can provide the UMP UAT: {URL} for BW to connect to. If no UMP UAT URL is mentioned, BorderWise will connect to UMPBaseUrl from appsettings.UAT.json Click here to see the original article on this topic.","title":"How do I create a test rig for BorderWise?"},{"location":"Guides/Typescript-Tips-%26-Tricks/","text":"We use typescript across BWW, WW, UMP and WCT. This page is to act as a collection of handy tips and tricks to help you when working with typescript. Troubleshooting Sometimes when working with typescript, we may come across error messages such as: The expected type comes from property 'variant' which is declared here on type 'IntrinsicAttributes & Partial<{ symbol: any; replace: boolean; flat: boolean; exact: boolean; active: boolean; block: boolean; disabled: boolean; size: string | number; tag: string; ... 4 more ...; ripple: boolean; }> & ... 16 more ... & Omit<...>' which can be hard to interpret. In these situations, you can use noErrorTruncation in tsconfig (under compilerOptions ) to expand all of the ... and to help us understand what is actually going on. For more information on this, check out https://www.typescriptlang.org/tsconfig. To see full TS definitions on mouse hover in VS Code you can try this: https://stackoverflow.com/a/69223288/4536543","title":"Typescript Tips & Tricks"},{"location":"Guides/Typescript-Tips-%26-Tricks/#troubleshooting","text":"Sometimes when working with typescript, we may come across error messages such as: The expected type comes from property 'variant' which is declared here on type 'IntrinsicAttributes & Partial<{ symbol: any; replace: boolean; flat: boolean; exact: boolean; active: boolean; block: boolean; disabled: boolean; size: string | number; tag: string; ... 4 more ...; ripple: boolean; }> & ... 16 more ... & Omit<...>' which can be hard to interpret. In these situations, you can use noErrorTruncation in tsconfig (under compilerOptions ) to expand all of the ... and to help us understand what is actually going on. For more information on this, check out https://www.typescriptlang.org/tsconfig. To see full TS definitions on mouse hover in VS Code you can try this: https://stackoverflow.com/a/69223288/4536543","title":"Troubleshooting"},{"location":"Guides/User-setup-for-integration-test-with-CW1-instance/","text":"Launch CW1 instance (either test-rigs on SAND or CW1Alpha on Corp or your local CW1 instance) and login as cw1Support. Open Staff and Resources module, select any user from Recent items. In case there is no record in recent items, then you could search any staff with Active status to true; Modify user's password and email address; The login name is the one used to login CW1 instance. Email should be same as the existing email in UMP (production / test) depends on which env you want to test with. Password is used for CW1 login Set up Security Rights -> give permission to access custom files Save If you want to test features for UMP, then Open registry -> set up \"BorderWise UMP API Base Address\", overide the default value with your test rig url If you want to test features for BW, then do the same thing for \"BorderWise Web Address\". Sometime you need to change both fields Save and login as new user, then use the login name and password to login. If you override the ump API base address with your test rig URL and are wanting autologin to take you to a BW test rig, you will need to update the autologin url in appsettings in your UMP UAT You should be able to connect BW via autologin now.","title":"User setup for integration test with CW1 instance"},{"location":"Guides/Vue%2BVuetify-migration-from-AngularJS/","text":"When adding new features or migrating old components to Vue consider creating/migrating them straight to Vuetify. We're using: - Vue 3 ( docs ) - framework, replacement for AngualrJS - TSX ( why and how ) - templating language, replacement for HTML - Vuetify 3 ( docs - not the ones from Google results!) - UI library, a replacement for Bootstrap - Material Design ( docs ) - design system, a replacement for Bootstrap's design system The app design may slightly change when migrating to be in line with the Material Design system, and we need to put a bit of effort into that during the design phase, please consult with Maxim Mazurok (Material Design expert), especially at first while we build the foundation and set examples. Reach out to him with any questions or concerns. TSX TSX (TypeScript+JSX) is slightly different from the HTML used in most examples in Vuetify docs. Here is a basic example of how to translate them: <v-list lines=\"one\"> <v-list-item v-for=\"item in items\" :key=\"item.title\" :title=\"item.title\" subtitle=\"...\" ></v-list-item> </v-list> in TSX will look like this: <VList lines=\"one\"> {items.map((item) => ( <VListItem key={item.title} title={item.title} subtitle=\"...\" /> ))} </VList> Check existing code that uses Vuetify components for more examples. See https://github.com/vuejs/babel-plugin-jsx#slot when something doesn't work, especially slots, etc. Icons FontAwesome icons need to be replaced by Material Design Icons. We're using @mdi/js to get icons. Search for icons here: https://pictogrammers.com/library/mdi/ Prefer icons with \"Created by Google\" badge: Icons don't have to look the same as the old ones, aim for the best match by meaning. For example, We had this icon: Now we use this one: You can see they don't match exactly, however, they both have the same meaning: \"history\". We currently have \"Quick Links\": Let's search for \"quick\": Nothing has a matching meaning. Let's search for \"links\": no results; Let's search for \"link\": This is a pretty good candidate, because it's a link icon - good way to represent links. And it's created by Google: However, usually it's used to represent a single link, maybe on a button to copy/open that link. In our case it's a dropdown of links collection, so probably not the best match on a meaning. Current icon name is fa-flash : Let's search for \"flash\": This is a pretty good match. It is created by google, similar to our current icon, and matches by meaning because the purpose of quick links is to be quickly accessible (quick as a flash), so this seems like a very good choice.","title":"Vue+Vuetify migration from AngularJS"},{"location":"Guides/Vue%2BVuetify-migration-from-AngularJS/#tsx","text":"TSX (TypeScript+JSX) is slightly different from the HTML used in most examples in Vuetify docs. Here is a basic example of how to translate them: <v-list lines=\"one\"> <v-list-item v-for=\"item in items\" :key=\"item.title\" :title=\"item.title\" subtitle=\"...\" ></v-list-item> </v-list> in TSX will look like this: <VList lines=\"one\"> {items.map((item) => ( <VListItem key={item.title} title={item.title} subtitle=\"...\" /> ))} </VList> Check existing code that uses Vuetify components for more examples. See https://github.com/vuejs/babel-plugin-jsx#slot when something doesn't work, especially slots, etc.","title":"TSX"},{"location":"Guides/Vue%2BVuetify-migration-from-AngularJS/#icons","text":"FontAwesome icons need to be replaced by Material Design Icons. We're using @mdi/js to get icons. Search for icons here: https://pictogrammers.com/library/mdi/ Prefer icons with \"Created by Google\" badge: Icons don't have to look the same as the old ones, aim for the best match by meaning. For example, We had this icon: Now we use this one: You can see they don't match exactly, however, they both have the same meaning: \"history\". We currently have \"Quick Links\": Let's search for \"quick\": Nothing has a matching meaning. Let's search for \"links\": no results; Let's search for \"link\": This is a pretty good candidate, because it's a link icon - good way to represent links. And it's created by Google: However, usually it's used to represent a single link, maybe on a button to copy/open that link. In our case it's a dropdown of links collection, so probably not the best match on a meaning. Current icon name is fa-flash : Let's search for \"flash\": This is a pretty good match. It is created by google, similar to our current icon, and matches by meaning because the purpose of quick links is to be quickly accessible (quick as a flash), so this seems like a very good choice.","title":"Icons"},{"location":"Guides/Vue-Query-Gotchas/","text":"Invalidating Related Queries When using Vue Query it's very important to know which queries share common data with other queries. If any of these queries update data in the backend, we must make sure that all the other interrelated queries are invalidated so that they refresh their data. For example: in WebWatcher we have our ContentSetup page that uses the activeCollectionDetailsQuery which displays a list of items in a table and their associated pieces of important data. const activeCollectionDetailsQuery = useMonitoringRequestsGETDetailedMonitoringRequests(activeCollectionPK); However in other components such as ScheduleForm, we can update associated data such as nextCheck , checkInterval , and isActive . const saveScheduleQuery = useMonitoringSchedulePATCH({...}); Currently in both WebWatcher and BorderWise this hasn't been a problem as most components will re-fetch data automatically when the component is re-rendered, however relying on this assumed behaviour could be dangerous and we should instead run const queryClient = useQueryClient(); queryClient.invalidateQueries(...)","title":"Invalidating Related Queries"},{"location":"Guides/Vue-Query-Gotchas/#invalidating-related-queries","text":"When using Vue Query it's very important to know which queries share common data with other queries. If any of these queries update data in the backend, we must make sure that all the other interrelated queries are invalidated so that they refresh their data. For example: in WebWatcher we have our ContentSetup page that uses the activeCollectionDetailsQuery which displays a list of items in a table and their associated pieces of important data. const activeCollectionDetailsQuery = useMonitoringRequestsGETDetailedMonitoringRequests(activeCollectionPK); However in other components such as ScheduleForm, we can update associated data such as nextCheck , checkInterval , and isActive . const saveScheduleQuery = useMonitoringSchedulePATCH({...}); Currently in both WebWatcher and BorderWise this hasn't been a problem as most components will re-fetch data automatically when the component is re-rendered, however relying on this assumed behaviour could be dangerous and we should instead run const queryClient = useQueryClient(); queryClient.invalidateQueries(...)","title":"Invalidating Related Queries"},{"location":"Guides/ve%252D%252A-components-props-validation/","text":"TL;DR: Required Vue props are now guaranteed to be provided Big improvement for AngularJS+Vue was just checked in. Now if you defined your prop as required in Vue like so: props: { tabName: { type: String, required: true } } then we will perform prop validation automatically. We will wait for AngularJS digest cycle to finish Once it's done - we will check if required prop is not present - error will be logged and displayed instead of the component. The component will only attempt to render if required prop is present. This means that you can trust TypeScript types and just use props.tabName trusting that it will always be String and never undefined . Because if it is undefined - the error is logged and displayed instead of the actual component. The main benefit is that this helps ensure that AngularJS implementation matches our Vue expectations. Previously we might declare prop as \"required\" and TS will tell us that it's always defined, but AngularJS might set it to undefined. And we had no good way of handling these errors. If you get errors saying that required prop isn't defined - it means one of two things: Either Vue misunderstood AngularJS implementation and this prop is actually optional, and so we should always check before using/relying on it Or AngularJS has a bug, and sets required prop to undefined instead of throwing error or something, then we need to fix AngularJS logic, this is a silent bug that we always had but never knew about it And there are two reasons why props validation error can occur: 1. The prop should actually be optional. For example, if we had <ve-user-info/> component - then studentId prop would be optional, because not everyone is a student. 2. There is a bug on AngularJS side and it shouldn't be rendering ve- component without required prop. For example, if we had <ve-user-info/> component - then userEmail prop can not be optional. It is required in order for the component to function, and if we don't provide it - the component loses reason for existing. We also can't \"solve\" this by providing default email, because why would you be showing some empty/dummy email to user? The challenge is when prop needs to be dynamically loaded by calling API for example. We should use the following principles: 1. Do not render component without providing required prop - error will be thrown 2. Do not add default prop value just to make error go away 3. If it takes more than 1 second to load - ideally show loading 4. It's better to handle this in the parent of ve- component For a concrete example of what not to do see this pr . I'll update you guys with an example of what to do after we fix it. Props validation for ve-* components allows us to ensure that strict contracts between components aren't broken. And this is very important when working in a large team with large number of components, not everyone knows everything about every component, so the least we can do it to make props reliable. Check out PR here .","title":"ve%2D%2A components props validation"},{"location":"Guides/PostgreSQL-Servers-%28Test-%26-Prod%29/Backup-Scripts/","text":"invoke_backup_cleaner.sh #!/bin/bash ~/backup_cleaner.py --backup-dir ~/dumps/ --day 1 --week 1 --month 1 --year 2 --keep-older dump_backup.sh #!/bin/bash # use pg_basebackup to make a dump of the local database, should run this on the secondary node BACKUP_NAME=\"backup_\"`date -u \"+%Y-%m-%d-%H-%M-%S\"` cd ~/dumps/ pg_basebackup -D \"${BACKUP_NAME}\" tar cvzf \"${BACKUP_NAME}.tar.gz\" \"${BACKUP_NAME}\" rm -r \"${BACKUP_NAME}\" backup_cleaner.py #!/usr/bin/python3 import argparse import os import shutil import sys from datetime import datetime, timedelta from typing import List, Tuple _FileDateList = List[Tuple[str, datetime]] _UTC_NOW = datetime.utcnow class _TimeBucketing: def __init__(self, start: datetime, length: timedelta, limit: int): self._start = start self._end = start - length self._length = length self._limit = limit def _bucketing(self, file_date_list: _FileDateList) -> List[_FileDateList]: file_date_list.sort(key=lambda x: x[1], reverse=True) buckets: List[_FileDateList] = [] start = self._start bucketing_delta = self._length / self._limit for _ in range(self._limit): end = start - bucketing_delta bucket = [] while file_date_list and file_date_list[0][1] > end: bucket.append(file_date_list.pop(0)) if bucket: buckets.append(bucket) start -= bucketing_delta for bucket in buckets: bucket.sort(key=lambda x: x[1]) return buckets def get_keeping_list(self, file_list: _FileDateList) -> List[str]: file_list_in_range = [] for file, date in file_list: if self._end < date < self._start: file_list_in_range.append((file, date)) keeping_list: List[str] = [] if self._limit == 0: return [] buckets = self._bucketing(file_list_in_range) while buckets: remaining_buckets = [] for bucket in buckets: first = bucket.pop(0)[0] if len(keeping_list) < self._limit: keeping_list.append(first) if bucket: remaining_buckets.append(bucket) buckets = remaining_buckets return keeping_list class BackupCleaner: \"\"\" Backup Cleaner Clean backup files and keep backups in a logarithmically manner. Need to specify the backup directory and how many backups are needed for each time period. The backups should be name as: name_yyyy-mm-dd-hh-mm-ss.ext * \"name\" can be any string without the _ (underscore). * The time is in UTC time. Example: backup_2021-02-16-23-18-68.gz \"\"\" def __init__(self, args: List[str]): parser = self._argument_parser() arguments = parser.parse_args(args) self._backup_dir = arguments.backup_dir self._backups_this_day = arguments.day self._backups_this_week = arguments.week self._backups_this_month = arguments.month self._backups_this_year = arguments.year self._dry_run = arguments.dry_run self._keep_older = arguments.keep_older @staticmethod def _argument_parser(): parser = argparse.ArgumentParser(description=\"Backup Cleaner\") parser.add_argument( \"--backup-dir\", dest=\"backup_dir\", type=str, required=True, help=\"Backup directory, search backups in the directory.\", ) parser.add_argument( \"--day\", dest=\"day\", type=int, required=True, help=\"How many backups need for the previous day.\", ) parser.add_argument( \"--week\", dest=\"week\", type=int, required=True, help=\"How many backups need for the previous week.\", ) parser.add_argument( \"--month\", dest=\"month\", type=int, required=True, help=\"How many backups need for the previous month.\", ) parser.add_argument( \"--year\", dest=\"year\", type=int, required=True, help=\"How many backups need for the previous year.\", ) parser.add_argument( \"--dry-run\", dest=\"dry_run\", action=\"store_true\", default=False, required=False, help=\"Not actually remove files, only display the files to remove and to keep.\", ) parser.add_argument( \"--keep-older\", dest=\"keep_older\", action=\"store_true\", default=False, required=False, help=\"Keep the files which is older than a year\", ) return parser @staticmethod def _parse_file_name(filepath: str) -> datetime: filename = os.path.basename(filepath) time_string = filename.split(\"_\")[1].split(\".\")[0] # IndexError time = datetime.strptime(time_string, \"%Y-%m-%d-%H-%M-%S\") return time def _find_db_backup_files(self) -> _FileDateList: file_and_date: _FileDateList = [] for file in os.scandir(self._backup_dir): date = self._parse_file_name(file.path) file_and_date.append((file.path, date)) return file_and_date @staticmethod def _remove(path): if os.path.isdir(path): shutil.rmtree(path) else: os.remove(path) def clean(self): current_time = _UTC_NOW() file_and_date = self._find_db_backup_files() start = current_time today = _TimeBucketing(start, timedelta(days=1), self._backups_this_day) today_keeping_list = today.get_keeping_list(file_and_date) today_keeping_list.sort(reverse=True) start -= timedelta(days=1) this_week = _TimeBucketing(start, timedelta(days=7), self._backups_this_week) this_week_keeping_list = this_week.get_keeping_list(file_and_date) this_week_keeping_list.sort(reverse=True) start -= timedelta(days=7) this_month = _TimeBucketing(start, timedelta(days=30), self._backups_this_month) this_month_keeping_list = this_month.get_keeping_list(file_and_date) this_month_keeping_list.sort(reverse=True) start -= timedelta(days=30) this_year = _TimeBucketing(start, timedelta(days=365), self._backups_this_year) this_year_keeping_list = this_year.get_keeping_list(file_and_date) this_year_keeping_list.sort(reverse=True) keeping_list = ( today_keeping_list + this_week_keeping_list + this_month_keeping_list + this_year_keeping_list ) if self._keep_older: start -= timedelta(days=365) older = list(filter(lambda x: x[1] < start, file_and_date)) older_files = list(map(lambda x: x[0], older)) keeping_list += older_files # Assign remove list explicitly, so newly generated files won't be removed remove_list = list(set(map(lambda x: x[0], file_and_date)) - set(keeping_list)) remove_list.sort(reverse=True) print(\"Is Dry Run:\", self._dry_run) print(\"\") print(\"Files to keep:\") print(\"day:\", today_keeping_list) print(\"week:\", this_week_keeping_list) print(\"month:\", this_month_keeping_list) print(\"year:\", this_year_keeping_list) if self._keep_older: print(\"older:\", older_files) print(\"\") print(\"Files to remove:\") print(remove_list) if not self._dry_run: for file in remove_list: self._remove(file) print(\"Files removed\") if __name__ == \"__main__\": CLEANER = BackupCleaner(sys.argv[1:]) CLEANER.clean()","title":"invoke_backup_cleaner.sh"},{"location":"Guides/PostgreSQL-Servers-%28Test-%26-Prod%29/Backup-Scripts/#invoke_backup_cleanersh","text":"#!/bin/bash ~/backup_cleaner.py --backup-dir ~/dumps/ --day 1 --week 1 --month 1 --year 2 --keep-older","title":"invoke_backup_cleaner.sh"},{"location":"Guides/PostgreSQL-Servers-%28Test-%26-Prod%29/Backup-Scripts/#dump_backupsh","text":"#!/bin/bash # use pg_basebackup to make a dump of the local database, should run this on the secondary node BACKUP_NAME=\"backup_\"`date -u \"+%Y-%m-%d-%H-%M-%S\"` cd ~/dumps/ pg_basebackup -D \"${BACKUP_NAME}\" tar cvzf \"${BACKUP_NAME}.tar.gz\" \"${BACKUP_NAME}\" rm -r \"${BACKUP_NAME}\"","title":"dump_backup.sh"},{"location":"Guides/PostgreSQL-Servers-%28Test-%26-Prod%29/Backup-Scripts/#backup_cleanerpy","text":"#!/usr/bin/python3 import argparse import os import shutil import sys from datetime import datetime, timedelta from typing import List, Tuple _FileDateList = List[Tuple[str, datetime]] _UTC_NOW = datetime.utcnow class _TimeBucketing: def __init__(self, start: datetime, length: timedelta, limit: int): self._start = start self._end = start - length self._length = length self._limit = limit def _bucketing(self, file_date_list: _FileDateList) -> List[_FileDateList]: file_date_list.sort(key=lambda x: x[1], reverse=True) buckets: List[_FileDateList] = [] start = self._start bucketing_delta = self._length / self._limit for _ in range(self._limit): end = start - bucketing_delta bucket = [] while file_date_list and file_date_list[0][1] > end: bucket.append(file_date_list.pop(0)) if bucket: buckets.append(bucket) start -= bucketing_delta for bucket in buckets: bucket.sort(key=lambda x: x[1]) return buckets def get_keeping_list(self, file_list: _FileDateList) -> List[str]: file_list_in_range = [] for file, date in file_list: if self._end < date < self._start: file_list_in_range.append((file, date)) keeping_list: List[str] = [] if self._limit == 0: return [] buckets = self._bucketing(file_list_in_range) while buckets: remaining_buckets = [] for bucket in buckets: first = bucket.pop(0)[0] if len(keeping_list) < self._limit: keeping_list.append(first) if bucket: remaining_buckets.append(bucket) buckets = remaining_buckets return keeping_list class BackupCleaner: \"\"\" Backup Cleaner Clean backup files and keep backups in a logarithmically manner. Need to specify the backup directory and how many backups are needed for each time period. The backups should be name as: name_yyyy-mm-dd-hh-mm-ss.ext * \"name\" can be any string without the _ (underscore). * The time is in UTC time. Example: backup_2021-02-16-23-18-68.gz \"\"\" def __init__(self, args: List[str]): parser = self._argument_parser() arguments = parser.parse_args(args) self._backup_dir = arguments.backup_dir self._backups_this_day = arguments.day self._backups_this_week = arguments.week self._backups_this_month = arguments.month self._backups_this_year = arguments.year self._dry_run = arguments.dry_run self._keep_older = arguments.keep_older @staticmethod def _argument_parser(): parser = argparse.ArgumentParser(description=\"Backup Cleaner\") parser.add_argument( \"--backup-dir\", dest=\"backup_dir\", type=str, required=True, help=\"Backup directory, search backups in the directory.\", ) parser.add_argument( \"--day\", dest=\"day\", type=int, required=True, help=\"How many backups need for the previous day.\", ) parser.add_argument( \"--week\", dest=\"week\", type=int, required=True, help=\"How many backups need for the previous week.\", ) parser.add_argument( \"--month\", dest=\"month\", type=int, required=True, help=\"How many backups need for the previous month.\", ) parser.add_argument( \"--year\", dest=\"year\", type=int, required=True, help=\"How many backups need for the previous year.\", ) parser.add_argument( \"--dry-run\", dest=\"dry_run\", action=\"store_true\", default=False, required=False, help=\"Not actually remove files, only display the files to remove and to keep.\", ) parser.add_argument( \"--keep-older\", dest=\"keep_older\", action=\"store_true\", default=False, required=False, help=\"Keep the files which is older than a year\", ) return parser @staticmethod def _parse_file_name(filepath: str) -> datetime: filename = os.path.basename(filepath) time_string = filename.split(\"_\")[1].split(\".\")[0] # IndexError time = datetime.strptime(time_string, \"%Y-%m-%d-%H-%M-%S\") return time def _find_db_backup_files(self) -> _FileDateList: file_and_date: _FileDateList = [] for file in os.scandir(self._backup_dir): date = self._parse_file_name(file.path) file_and_date.append((file.path, date)) return file_and_date @staticmethod def _remove(path): if os.path.isdir(path): shutil.rmtree(path) else: os.remove(path) def clean(self): current_time = _UTC_NOW() file_and_date = self._find_db_backup_files() start = current_time today = _TimeBucketing(start, timedelta(days=1), self._backups_this_day) today_keeping_list = today.get_keeping_list(file_and_date) today_keeping_list.sort(reverse=True) start -= timedelta(days=1) this_week = _TimeBucketing(start, timedelta(days=7), self._backups_this_week) this_week_keeping_list = this_week.get_keeping_list(file_and_date) this_week_keeping_list.sort(reverse=True) start -= timedelta(days=7) this_month = _TimeBucketing(start, timedelta(days=30), self._backups_this_month) this_month_keeping_list = this_month.get_keeping_list(file_and_date) this_month_keeping_list.sort(reverse=True) start -= timedelta(days=30) this_year = _TimeBucketing(start, timedelta(days=365), self._backups_this_year) this_year_keeping_list = this_year.get_keeping_list(file_and_date) this_year_keeping_list.sort(reverse=True) keeping_list = ( today_keeping_list + this_week_keeping_list + this_month_keeping_list + this_year_keeping_list ) if self._keep_older: start -= timedelta(days=365) older = list(filter(lambda x: x[1] < start, file_and_date)) older_files = list(map(lambda x: x[0], older)) keeping_list += older_files # Assign remove list explicitly, so newly generated files won't be removed remove_list = list(set(map(lambda x: x[0], file_and_date)) - set(keeping_list)) remove_list.sort(reverse=True) print(\"Is Dry Run:\", self._dry_run) print(\"\") print(\"Files to keep:\") print(\"day:\", today_keeping_list) print(\"week:\", this_week_keeping_list) print(\"month:\", this_month_keeping_list) print(\"year:\", this_year_keeping_list) if self._keep_older: print(\"older:\", older_files) print(\"\") print(\"Files to remove:\") print(remove_list) if not self._dry_run: for file in remove_list: self._remove(file) print(\"Files removed\") if __name__ == \"__main__\": CLEANER = BackupCleaner(sys.argv[1:]) CLEANER.clean()","title":"backup_cleaner.py"},{"location":"Guides/Tariff-Data-Sources/Australia/","text":"Australia uses different Tariffs for Import & Export: Schedule-3 and AHECC. Schedule-3 There two sources that we use to build Schedule-3: - Word documents that AU Customs sends us via email, and - AU Customs website: https://www.abf.gov.au/importing-exporting-and-manufacturing/tariff-classification/current-tariff/schedule-3 We usually know in advance when tariff changes are going to occur and ask Customs to send us updated Word files - so that we can preemptively build & test BWW's Schedule-3. The website will publish the changes after their effective date, so it can only be used as a backup source. We also do a cross-comparison of the Word files vs the website to identify data discrepancies. AHECC (Australian Harmonized Export Commodity Classification) We collect AHECC from the ABS website: https://www.abs.gov.au/statistics/classifications/australian-harmonized-export-commodity-classification-ahecc/latest-release It's a collection of Excel files: 21 sections, Summary and Key of Changes. Each file Section file contains \"chapter\" tabs for Legal Notes and Nomenclature.","title":"Australia"},{"location":"Guides/Tariff-Data-Sources/Europe/","text":"Below are the questions asked by our Product Managers and the answers from our TARIC (Spanish) Team. 1. What source are you collecting the data for EU Measures? We are collection from multiple sources. But the main is the NL Customs, that have been proved to be more reliable and complete than the EU Commission or similar EU member state sources. We also collect EU data from BE, SE and PL, which is useful for comparison and as fallback in case we have any problem with NL. On the other hand, we are using the EU Commision source for translations, they are not as reliable as the other sources, but it's not a big problem to have some fields not translated in time if the data integrity is good. 2. How frequently are the EU Measures updated in the original source? Considering the original source the EU Commission, they publish daily updates. NL Customs also publish daily updates with the same data, but better. 3. What language(s) are the measure available and collected in from original source you are collecting the data from? The EU Commission publish data in every official member state language. The NL Customs, like many EU Customs, publish the data in English and their official local languages, in this case, Dutch. 4. How frequently are you collecting the EU Measures and updating your database in comparison to the how frequently the EU Measures are updated in the original source? On a daily basis, we collect the data that the sources publish during the day at night (CET) and update the internal database of our data gathering system. So each day we have the most recent information from the day before. In the data system we perform automated data integrity checks and also we allow our product team to manually review the new included data. Once data is validated we release data to our production systems. Nowadays we are releasing data once per week on Monday or Tuesday, or any other day if the nomenclature or some critical change is published from the source. We have to remember that we publish future data, so, most of the information is received in advance and is available before its effective date. But in any case, as we improve our automatic data verification processes, we expect to accelerate the release cadence 5. What is the delay from the original EU Measures updated data to availability for BorderWise to extract/download? At the same time that we prepare the data for TariffOne, we also prepare and publish the data bundle that BorderWise uses.","title":"Europe"},{"location":"Guides/Tariff-Data-Sources/New-Zealand/","text":"New Zealand uses the same tariff for Import and Export. We receive tariff updates from NZ Customs via email, usually one or two weeks prior to the updates' effective date. This gives us a head start to build and validate NZ Tariff - and send anomalies back to NZ Customs. NZ Tariff (aka NZ Working Tariff Document, NZWTD) is a set of PDF files, also available on the NZCS website: https://www.customs.govt.nz/business/tariffs/working-tariff-document/ Question: Why can't we use the files from the website? Answer: We can - but we use email updates simply because they are sent to us well in advance. The Tariff file-set is composed of 21 \"Section\" files + accompanying files (Introduction, Valid Pages, etc.) To build the data for BorderWise, we only need the \"Section\" files. One peculiarity of NZ Tariff files to be aware of: they are typeset for printing, e.g. they use ligatures, kerning, gutters, etc. All this complicates the process of converting PDF to JSON.","title":"New Zealand"},{"location":"New-Starters/1.-Intro-to-BorderWise/","text":"Introduction to BorderWise For New Starters BorderWise is border compliance software. It enables trade professionals to navigate customs and compliance requirements quickly, easily, and with confidence. \u200b What you might find yourself working on BorderWise Web (BWW) BWW is compliance software for customs brokers. It helps them to classify goods and fill in customs declarations by providing access to country specific law, WCO publications, global free trade agreements etc. all from the one place. We have a search functionality that users search these resources, making it even easier for them to get the information they need. Production: https://app.borderwise.com Test: https://app-test.borderwise.com/ Repository: https://devops.wisetechglobal.com/wtg/BorderWise/_git/BorderWiseWeb WebWatcher (WW) All this information that BWW provides its users need to be as up-to-date as possible. This is where WebWatcher (WW) comes in! WW is a tool that monitors website content changes. When website content changes, it sends off a notification that triggers a service (Content Service) to download and convert content to be displayed in BWW. Production: https://webwatcher.wtg.zone Test: https://webwatcher-test.wtg.zone/ Repository: https://devops.wisetechglobal.com/wtg/BorderWise/_git/WebWatcher User Management Portal (UMP) UMP is our authentication and authorization service for BWW. It lets us manage users and licenses for BWW. Production: https://ump.borderwise.com Test: https://ump-test1.borderwise.com Repository: https://devops.wisetechglobal.com/wtg/BorderWise/_git/UMP BorderWise Glossary BWW Term What is it? TOC Table Of Contents Tariff WW Term What is it? Monitoring Request Defines a website that we are watching for changes and its scraping options Monitoring Schedule Defines how often we watch a particular website for changes","title":"Introduction to BorderWise For New Starters"},{"location":"New-Starters/1.-Intro-to-BorderWise/#introduction-to-borderwise-for-new-starters","text":"BorderWise is border compliance software. It enables trade professionals to navigate customs and compliance requirements quickly, easily, and with confidence. \u200b","title":"Introduction to BorderWise For New Starters"},{"location":"New-Starters/1.-Intro-to-BorderWise/#what-you-might-find-yourself-working-on","text":"","title":"What you might find yourself working on"},{"location":"New-Starters/1.-Intro-to-BorderWise/#borderwise-web-bww","text":"BWW is compliance software for customs brokers. It helps them to classify goods and fill in customs declarations by providing access to country specific law, WCO publications, global free trade agreements etc. all from the one place. We have a search functionality that users search these resources, making it even easier for them to get the information they need. Production: https://app.borderwise.com Test: https://app-test.borderwise.com/ Repository: https://devops.wisetechglobal.com/wtg/BorderWise/_git/BorderWiseWeb","title":"BorderWise Web (BWW)"},{"location":"New-Starters/1.-Intro-to-BorderWise/#webwatcher-ww","text":"All this information that BWW provides its users need to be as up-to-date as possible. This is where WebWatcher (WW) comes in! WW is a tool that monitors website content changes. When website content changes, it sends off a notification that triggers a service (Content Service) to download and convert content to be displayed in BWW. Production: https://webwatcher.wtg.zone Test: https://webwatcher-test.wtg.zone/ Repository: https://devops.wisetechglobal.com/wtg/BorderWise/_git/WebWatcher","title":"WebWatcher (WW)"},{"location":"New-Starters/1.-Intro-to-BorderWise/#user-management-portal-ump","text":"UMP is our authentication and authorization service for BWW. It lets us manage users and licenses for BWW. Production: https://ump.borderwise.com Test: https://ump-test1.borderwise.com Repository: https://devops.wisetechglobal.com/wtg/BorderWise/_git/UMP","title":"User Management Portal (UMP)"},{"location":"New-Starters/1.-Intro-to-BorderWise/#borderwise-glossary","text":"","title":"BorderWise Glossary"},{"location":"New-Starters/1.-Intro-to-BorderWise/#bww","text":"Term What is it? TOC Table Of Contents Tariff","title":"BWW"},{"location":"New-Starters/1.-Intro-to-BorderWise/#ww","text":"Term What is it? Monitoring Request Defines a website that we are watching for changes and its scraping options Monitoring Schedule Defines how often we watch a particular website for changes","title":"WW"},{"location":"New-Starters/2.-Environment-Setup/","text":"Read this first: Do Common Configuration first, then do specific per-project To resolve problems/errors - check out Troubleshooting first Editing this guide Avoid duplication of instructions (prefer Common Configuration ) Problem/error solutions go in Troubleshooting (to keep main instructions concise) [[ TOC ]] Common Configuration Front-End (FE) Note: you might want to use frontend-setup script to automate the setup. This is a pre-requisite for working on all FE projects: Install VS Code Install WSL ( enable it first if needed) In cmd run wsl --set-default-version 2 - ensure WSL v2 will be used by default. Learn more Install Ubuntu on WSL; Learn mode Ensure your Ubuntu installation is actually using WSL v2 and not v1, run wsl -l -v in Windows cmd , if not v2 - migrate it to v2. sudo apt update && sudo apt upgrade -y - update everything Install \"Remote - WSL\" extension in VS Code; it should be installed automatically if you've installed all recommended plugins; Tutorial Install nvm inside WSL. This will let you run nvm install to install required NodeJS version (also see package.json 's engines section for currently supported NodeJS and NPM versions). In order for VS Code to use WSL version of Node and NPM (for example, to run jest tests in watch mode), create ~/.vscode-server/server-env-setup file in WSL, with the following contents (taken from ~/.bashrc , credit ): export NVM_DIR=\"$HOME/.nvm\" [ -s \"$NVM_DIR/nvm.sh\" ] && \\. \"$NVM_DIR/nvm.sh\" # This loads nvm (Skip for UMP) Make sure that GUI apps work (for Cypress) Try to run sudo apt install x11-apps && xcalc , if you see a calculator - skip further steps. You can try using WSLg which is enabled by default, but make sure you're using the latest WSL (run wsl --update ), see also \"WSLg doesn't work\" . If now you can see calculator when running xcalc - skip further steps Alternatively, follow this guide (ignore dbus -related stuff), also this guide might be helpful. Here's a short version: apt-get install libgtk2.0-0 libgtk-3-0 libgbm-dev libnotify-dev libgconf-2-4 libnss3 libxss1 libasound2 libxtst6 xauth xvfb Install VcXsrv ; See also: Can't download VcXsrv from the office network Run XLaunch , make sure \"Disable access control\" is checked on the \"Extra settings\" screen; Save config and add to autorun, or do it every time manually. Add this to the end of ~/.bashrc file: # set DISPLAY variable to the IP automatically assigned to WSL2 export DISPLAY=$(cat /etc/resolv.conf | grep nameserver | awk '{print $2; exit;}'):0.0 Relaunch terminal and/or VS Code Add firewall rule for VcXsrv.exe , allow all inbound connections Run xcalc again to confirm that GUI apps work now Back-end (BE) This is a draft by FE guy, please someone who does BE improve this section Quick Get Latest (QGL) Visual Studio v17.6.5, DotNet SDK v7.0.306 from Software Center Latest PostgreSQL + pgAdmin 4 might be required (for WW, for example) Studio 3T might be useful for BWW BorderWise Web (BWW) BWW FE Note : all commands in this section are supposed to be run inside of your WSL Ubuntu terminal Pre-req: Follow Common Configuration for FE ~$ git clone --config core.autocrlf=false https://devops.wisetechglobal.com/wtg/BorderWise/_git/BorderWiseWeb If you're NOT planning to use local BE (easy): ~/BorderWiseWeb/Frontend/src$ cp .env.example .env - create BorderWiseWeb/Frontend/src/.env file by copying .env.example in it In .env file uncomment # HTTP_SERVER_API_HOST=\"https://app-alpha.borderwise.com\" line (remove # at the start of the line) If you're planning to use local BE (hard): ~$ echo \"export WSL_HOST_IP=\\$(awk '/nameserver/ { print \\$2 }' /etc/resolv.conf)\" >> ~/.bashrc Add firewall rule in Windows: Allow Inbound for TCP Port 40135 : Open \"Start\" menu Search for Windows Defender Firewall with Advanced Security Select \"Inbound Rules\" Click \"New Rule...\" Rule Type: Port Protocol and Ports: TCP Specific local ports: 40135 Action: Allow the connection Profile: [x] Domain (not enough, because Windows won't recognize the VPN as a corporate domain) [x] Private [x] Public Name: BWW Build BE using QGL, open BorderWise solution in VS and run WebApi : ; see BE guide for details. After that: ~$ code ~/BorderWiseWeb/Frontend/BorderWiseWeb-Frontend.code-workspace ~$ wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb && sudo apt install ./google-chrome-stable_current_amd64.deb && rm google-chrome-stable_current_amd64.deb - download and install Chrome, required for Jasmine ~/BorderWiseWeb/Frontend/src$ nvm i - use required node version; normally this command will run automatically every time you open integrated terminal, but may not run straight away when you first open the project, either run it manually or open another integrated terminal ~/BorderWiseWeb/Frontend/src$ npm ci - installs deps for src and vue-elements in one go Make sure that API is running locally or that .env has appropriate remote API uncommented ~/BorderWiseWeb/Frontend/src$ npm start - will run local dev server, rebuild src and vue-elements on file changes BWW BE Refer to How do I set up BorderWise Web backend locally? To interact with the MongoDB database, set up Studio3T by following How do I set up Studio 3T for BorderWise Web? WebWatcher (WW) WW FE Pre-req: Follow Common Configuration for FE ~$ git clone https://devops.wisetechglobal.com/wtg/BorderWise/_git/WebWatcher If you're NOT planning to use local BE (easy): ~/WebWatcher/Frontend$ cp .env.example .env - create WebWatcher/Frontend/.env file by copying .env.example in it In .env file uncomment # HTTP_SERVER_API_HOST=\"https://webwatcher-test.wtg.zone\" line (remove # at the start of the line) If you're planning to use local BE (hard): ~$ echo \"export WSL_HOST_IP=\\$(awk '/nameserver/ { print \\$2 }' /etc/resolv.conf)\" >> ~/.bashrc Add firewall rule in Windows: Allow Inbound for TCP Port 5001 : Open \"Start\" menu Search for Windows Defender Firewall with Advanced Security Select \"Inbound Rules\" Click \"New Rule...\" Rule Type: Port Protocol and Ports: TCP Specific local ports: 5001 Action: Allow the connection Profile: [x] Domain (not enough, because Windows won't recognize the VPN as a corporate domain) [x] Private [x] Public Name: WW Build BE using QGL, open WebWatcher solution in VS and run WebApi : ; see BE guide for details. After that: ~$ code ~/WebWatcher/Frontend/WebWatcher-Frontend.code-workspace ~/WebWatcher/Frontend$ nvm i - use required node version; ~/WebWatcher/Frontend$ npm ci - installs deps Make sure that API is running locally or that .env has appropriate remote API uncommented ~/WebWatcher/Frontend$ npm start - will run local dev server WW BE Clone and build the Web Watcher repo using QGL Set up your local database by following this SO post WW Scheduler (BE) Only needed for debugging content scraping/processing, and isn't required to run API. Install Podman in WSL2 Find current selenium/standalone-chrome version here Run Selenium Grid Docker image in WSL2 (replace CURRENT_VERSION_FROM_PREVIOUS_STEP ): docker run -d -e SE_NODE_MAX_SESSIONS=24 -e SE_NODE_OVERRIDE_MAX_SESSIONS=true --restart always -p 4444:4444 --shm-size=2g --name selenium selenium/standalone-chrome:CURRENT_VERSION_FROM_PREVIOUS_STEP User Management Portal (UMP) UMP FE Pre-req: Follow Common Configuration for FE ~$ git clone https://devops.wisetechglobal.com/wtg/BorderWise/_git/UMP ~/UMP/Frontend$ cp .env.example .env - create UMP/Frontend/.env file by copying .env.example in it If you're NOT planning to use local BE (easy): In .env file uncomment # HTTP_SERVER_API_HOST=\"https://ump-test1.borderwise.com\" line (remove # at the start of the line) If you're planning to use local BE (hard): ~$ echo \"export WSL_HOST_IP=\\$(awk '/nameserver/ { print \\$2 }' /etc/resolv.conf)\" >> ~/.bashrc In ~/UMP/Frontend/.env file uncomment # HTTP_SERVER_API_HOST=\"https://localhost:5002\" line (remove # at the start of the line) Add firewall rule in Windows: Allow Inbound for TCP Port 5002 : Open \"Start\" menu Search for Windows Defender Firewall with Advanced Security Select \"Inbound Rules\" Click \"New Rule...\" Rule Type: Port Protocol and Ports: TCP Specific local ports: 5002 Action: Allow the connection Profile: [x] Domain (not enough, because Windows won't recognize the VPN as a corporate domain) [x] Private [x] Public Name: UMP Build BE using QGL, open UMP solution in VS and run Ump ; see BE guide for details. After that: ~$ code ~/UMP/Frontend ~/UMP/Frontend$ nvm i - use required node version; ~/UMP/Frontend$ npm ci - installs deps Make sure that API is running locally or that .env has appropriate remote API uncommented ~/UMP/Frontend$ npm start - will run local dev server UMP BE Refer to How do I set up BorderWise UMP API locally? Troubleshooting Garbled terminal prompt in WSL After installation, you may encounter an issue where you get a garbled terminal prompt instead of a user setup screen. If you do, follow these steps: Close + Re-Open Ubuntu. You should be logged in as root Run adduser <yourNewUsername> , you will be asked to set a password. Press enter to skip the other fields. Add your new user to the sudo users group with the following command: usermod -aG sudo <yourNewUsername> Run nano /etc/wsl.conf , add the following text and save your changes: [user] default=<yourNewUsername> Close + re-open Ubuntu. You should now be logged in as your new user by default VS Code asks \"Do you allow this workspace to modify your terminal shell?\" When prompted \"Do you allow this workspace to modify your terminal shell?\" - allow, this will set handy env vars, such as PATH=$PATH:./node_modules/.bin VS Code asks \"Do you trust the authors of the files in this workspace?\" When you see something like this - choose \"Yes, I trust the authors\", otherwise you'll have a pretty bad experience. Network-related issues Make sure you're using WSL 2 and not WSL 1, check this by running wsl -l -v in cmd . See how to migrate . Building FE in QGL takes too long FE projects aren't meant to be built using QGL on developer machines. We only support QGL as a necessity to be able to build FE on DAT. If you are doing FE development - don't use QGL to build FE. Most of the time all you need it nvm i && npm ci && npm start , see more details in guides above. Can't find QGL It's normally downloaded by Crikey, see How to install Crikey Monitor manually? or How to get QGL for Git without QGL for Git? Errors installing Java If you experience dependencies errors when installing default-jdk , try to execute ~$ sudo apt --fix-broken install . To verify Java is correctly installed use ~$ java -version in WSL Ubuntu terminal. Can't download Chrome You can find the current URL here in the \"Linux\" -> \"Stable channel\" section Running Cypress has error: \"The Test Runner unexpectedly exited via a exit event with signal SIGSEGV\" If you run into this error and still use the old VcXsrv approach to rendering GUI Apps in WSL, it's probably better to update your WSL now that GUI Apps are supported out of the box: Run powershell as admin, then run the command wsl --update Once updated, remove these lines from your .bashrc in your Ubuntu home directory: # set DISPLAY variable to the IP automatically assigned to WSL2 export DISPLAY=$(cat /etc/resolv.conf | grep nameserver | awk '{print $2; exit;}'):0.0 sudo /etc/init.d/dbus start &> /dev/null Afterwards ensure that VcXsrv AKA Windows X Server or XLaunch is no longer running in Windows. You can even uninstall it since we won't be using it anymore. Can't connect to myaccount-portal.cargowise.com from local BE Refer to Can't connect to myaccount-portal.cargowise.com from local BE . WSLg doesn't work If you're using new default X server - https://github.com/microsoft/wslg and when you do echo $DISPLAY it doesn't say :0 - you might have it disabled, check the config c:\\users\\MyUser\\.wslconfig : [wsl2] guiApplications=true see https://github.com/microsoft/wslg#wslg-system-distro for details. Can't download VcXsrv from the office network Sourceforge.net seems to be blocked by the office network. You can find VcXsrv (WSLg replacement) in this network share: \\\\datfiles.wtg.zone\\ThirdParty\\Web\\BorderWise\\VcXsrv WSL won't start after install After you install WSL and attempt to launch Ubuntu, you might get an error after some time instead of a command prompt. It can be solved by making sure these Windows Features are enabled: Hyper-V and Virtual Machine Platform (Windows Hypervisor Platform might help as well). These errors look like this: Error code: Wsl/Service/CreateInstance/CreateVm/WSL_E_VIRTUAL_MACHINE_PLATFORM_REQUIRED Error code: Wsl/Service/CreateInstance/CreateVm/ConfigureNetworking/HNS/0x80070424","title":"2. Environment Setup"},{"location":"New-Starters/2.-Environment-Setup/#common-configuration","text":"","title":"Common Configuration"},{"location":"New-Starters/2.-Environment-Setup/#front-end-fe","text":"Note: you might want to use frontend-setup script to automate the setup. This is a pre-requisite for working on all FE projects: Install VS Code Install WSL ( enable it first if needed) In cmd run wsl --set-default-version 2 - ensure WSL v2 will be used by default. Learn more Install Ubuntu on WSL; Learn mode Ensure your Ubuntu installation is actually using WSL v2 and not v1, run wsl -l -v in Windows cmd , if not v2 - migrate it to v2. sudo apt update && sudo apt upgrade -y - update everything Install \"Remote - WSL\" extension in VS Code; it should be installed automatically if you've installed all recommended plugins; Tutorial Install nvm inside WSL. This will let you run nvm install to install required NodeJS version (also see package.json 's engines section for currently supported NodeJS and NPM versions). In order for VS Code to use WSL version of Node and NPM (for example, to run jest tests in watch mode), create ~/.vscode-server/server-env-setup file in WSL, with the following contents (taken from ~/.bashrc , credit ): export NVM_DIR=\"$HOME/.nvm\" [ -s \"$NVM_DIR/nvm.sh\" ] && \\. \"$NVM_DIR/nvm.sh\" # This loads nvm (Skip for UMP) Make sure that GUI apps work (for Cypress) Try to run sudo apt install x11-apps && xcalc , if you see a calculator - skip further steps. You can try using WSLg which is enabled by default, but make sure you're using the latest WSL (run wsl --update ), see also \"WSLg doesn't work\" . If now you can see calculator when running xcalc - skip further steps Alternatively, follow this guide (ignore dbus -related stuff), also this guide might be helpful. Here's a short version: apt-get install libgtk2.0-0 libgtk-3-0 libgbm-dev libnotify-dev libgconf-2-4 libnss3 libxss1 libasound2 libxtst6 xauth xvfb Install VcXsrv ; See also: Can't download VcXsrv from the office network Run XLaunch , make sure \"Disable access control\" is checked on the \"Extra settings\" screen; Save config and add to autorun, or do it every time manually. Add this to the end of ~/.bashrc file: # set DISPLAY variable to the IP automatically assigned to WSL2 export DISPLAY=$(cat /etc/resolv.conf | grep nameserver | awk '{print $2; exit;}'):0.0 Relaunch terminal and/or VS Code Add firewall rule for VcXsrv.exe , allow all inbound connections Run xcalc again to confirm that GUI apps work now","title":"Front-End (FE)"},{"location":"New-Starters/2.-Environment-Setup/#back-end-be","text":"This is a draft by FE guy, please someone who does BE improve this section Quick Get Latest (QGL) Visual Studio v17.6.5, DotNet SDK v7.0.306 from Software Center Latest PostgreSQL + pgAdmin 4 might be required (for WW, for example) Studio 3T might be useful for BWW","title":"Back-end (BE)"},{"location":"New-Starters/2.-Environment-Setup/#borderwise-web-bww","text":"","title":"BorderWise Web (BWW)"},{"location":"New-Starters/2.-Environment-Setup/#bww-fe","text":"Note : all commands in this section are supposed to be run inside of your WSL Ubuntu terminal Pre-req: Follow Common Configuration for FE ~$ git clone --config core.autocrlf=false https://devops.wisetechglobal.com/wtg/BorderWise/_git/BorderWiseWeb If you're NOT planning to use local BE (easy): ~/BorderWiseWeb/Frontend/src$ cp .env.example .env - create BorderWiseWeb/Frontend/src/.env file by copying .env.example in it In .env file uncomment # HTTP_SERVER_API_HOST=\"https://app-alpha.borderwise.com\" line (remove # at the start of the line) If you're planning to use local BE (hard): ~$ echo \"export WSL_HOST_IP=\\$(awk '/nameserver/ { print \\$2 }' /etc/resolv.conf)\" >> ~/.bashrc Add firewall rule in Windows: Allow Inbound for TCP Port 40135 : Open \"Start\" menu Search for Windows Defender Firewall with Advanced Security Select \"Inbound Rules\" Click \"New Rule...\" Rule Type: Port Protocol and Ports: TCP Specific local ports: 40135 Action: Allow the connection Profile: [x] Domain (not enough, because Windows won't recognize the VPN as a corporate domain) [x] Private [x] Public Name: BWW Build BE using QGL, open BorderWise solution in VS and run WebApi : ; see BE guide for details. After that: ~$ code ~/BorderWiseWeb/Frontend/BorderWiseWeb-Frontend.code-workspace ~$ wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb && sudo apt install ./google-chrome-stable_current_amd64.deb && rm google-chrome-stable_current_amd64.deb - download and install Chrome, required for Jasmine ~/BorderWiseWeb/Frontend/src$ nvm i - use required node version; normally this command will run automatically every time you open integrated terminal, but may not run straight away when you first open the project, either run it manually or open another integrated terminal ~/BorderWiseWeb/Frontend/src$ npm ci - installs deps for src and vue-elements in one go Make sure that API is running locally or that .env has appropriate remote API uncommented ~/BorderWiseWeb/Frontend/src$ npm start - will run local dev server, rebuild src and vue-elements on file changes","title":"BWW FE"},{"location":"New-Starters/2.-Environment-Setup/#bww-be","text":"Refer to How do I set up BorderWise Web backend locally? To interact with the MongoDB database, set up Studio3T by following How do I set up Studio 3T for BorderWise Web?","title":"BWW BE"},{"location":"New-Starters/2.-Environment-Setup/#webwatcher-ww","text":"","title":"WebWatcher (WW)"},{"location":"New-Starters/2.-Environment-Setup/#ww-fe","text":"Pre-req: Follow Common Configuration for FE ~$ git clone https://devops.wisetechglobal.com/wtg/BorderWise/_git/WebWatcher If you're NOT planning to use local BE (easy): ~/WebWatcher/Frontend$ cp .env.example .env - create WebWatcher/Frontend/.env file by copying .env.example in it In .env file uncomment # HTTP_SERVER_API_HOST=\"https://webwatcher-test.wtg.zone\" line (remove # at the start of the line) If you're planning to use local BE (hard): ~$ echo \"export WSL_HOST_IP=\\$(awk '/nameserver/ { print \\$2 }' /etc/resolv.conf)\" >> ~/.bashrc Add firewall rule in Windows: Allow Inbound for TCP Port 5001 : Open \"Start\" menu Search for Windows Defender Firewall with Advanced Security Select \"Inbound Rules\" Click \"New Rule...\" Rule Type: Port Protocol and Ports: TCP Specific local ports: 5001 Action: Allow the connection Profile: [x] Domain (not enough, because Windows won't recognize the VPN as a corporate domain) [x] Private [x] Public Name: WW Build BE using QGL, open WebWatcher solution in VS and run WebApi : ; see BE guide for details. After that: ~$ code ~/WebWatcher/Frontend/WebWatcher-Frontend.code-workspace ~/WebWatcher/Frontend$ nvm i - use required node version; ~/WebWatcher/Frontend$ npm ci - installs deps Make sure that API is running locally or that .env has appropriate remote API uncommented ~/WebWatcher/Frontend$ npm start - will run local dev server","title":"WW FE"},{"location":"New-Starters/2.-Environment-Setup/#ww-be","text":"Clone and build the Web Watcher repo using QGL Set up your local database by following this SO post","title":"WW BE"},{"location":"New-Starters/2.-Environment-Setup/#ww-scheduler-be","text":"Only needed for debugging content scraping/processing, and isn't required to run API. Install Podman in WSL2 Find current selenium/standalone-chrome version here Run Selenium Grid Docker image in WSL2 (replace CURRENT_VERSION_FROM_PREVIOUS_STEP ): docker run -d -e SE_NODE_MAX_SESSIONS=24 -e SE_NODE_OVERRIDE_MAX_SESSIONS=true --restart always -p 4444:4444 --shm-size=2g --name selenium selenium/standalone-chrome:CURRENT_VERSION_FROM_PREVIOUS_STEP","title":"WW Scheduler (BE)"},{"location":"New-Starters/2.-Environment-Setup/#user-management-portal-ump","text":"","title":"User Management Portal (UMP)"},{"location":"New-Starters/2.-Environment-Setup/#ump-fe","text":"Pre-req: Follow Common Configuration for FE ~$ git clone https://devops.wisetechglobal.com/wtg/BorderWise/_git/UMP ~/UMP/Frontend$ cp .env.example .env - create UMP/Frontend/.env file by copying .env.example in it If you're NOT planning to use local BE (easy): In .env file uncomment # HTTP_SERVER_API_HOST=\"https://ump-test1.borderwise.com\" line (remove # at the start of the line) If you're planning to use local BE (hard): ~$ echo \"export WSL_HOST_IP=\\$(awk '/nameserver/ { print \\$2 }' /etc/resolv.conf)\" >> ~/.bashrc In ~/UMP/Frontend/.env file uncomment # HTTP_SERVER_API_HOST=\"https://localhost:5002\" line (remove # at the start of the line) Add firewall rule in Windows: Allow Inbound for TCP Port 5002 : Open \"Start\" menu Search for Windows Defender Firewall with Advanced Security Select \"Inbound Rules\" Click \"New Rule...\" Rule Type: Port Protocol and Ports: TCP Specific local ports: 5002 Action: Allow the connection Profile: [x] Domain (not enough, because Windows won't recognize the VPN as a corporate domain) [x] Private [x] Public Name: UMP Build BE using QGL, open UMP solution in VS and run Ump ; see BE guide for details. After that: ~$ code ~/UMP/Frontend ~/UMP/Frontend$ nvm i - use required node version; ~/UMP/Frontend$ npm ci - installs deps Make sure that API is running locally or that .env has appropriate remote API uncommented ~/UMP/Frontend$ npm start - will run local dev server","title":"UMP FE"},{"location":"New-Starters/2.-Environment-Setup/#ump-be","text":"Refer to How do I set up BorderWise UMP API locally?","title":"UMP BE"},{"location":"New-Starters/2.-Environment-Setup/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"New-Starters/2.-Environment-Setup/#garbled-terminal-prompt-in-wsl","text":"After installation, you may encounter an issue where you get a garbled terminal prompt instead of a user setup screen. If you do, follow these steps: Close + Re-Open Ubuntu. You should be logged in as root Run adduser <yourNewUsername> , you will be asked to set a password. Press enter to skip the other fields. Add your new user to the sudo users group with the following command: usermod -aG sudo <yourNewUsername> Run nano /etc/wsl.conf , add the following text and save your changes: [user] default=<yourNewUsername> Close + re-open Ubuntu. You should now be logged in as your new user by default","title":"Garbled terminal prompt in WSL"},{"location":"New-Starters/2.-Environment-Setup/#vs-code-asks-do-you-allow-this-workspace-to-modify-your-terminal-shell","text":"When prompted \"Do you allow this workspace to modify your terminal shell?\" - allow, this will set handy env vars, such as PATH=$PATH:./node_modules/.bin","title":"VS Code asks \"Do you allow this workspace to modify your terminal shell?\""},{"location":"New-Starters/2.-Environment-Setup/#vs-code-asks-do-you-trust-the-authors-of-the-files-in-this-workspace","text":"When you see something like this - choose \"Yes, I trust the authors\", otherwise you'll have a pretty bad experience.","title":"VS Code asks \"Do you trust the authors of the files in this workspace?\""},{"location":"New-Starters/2.-Environment-Setup/#network-related-issues","text":"Make sure you're using WSL 2 and not WSL 1, check this by running wsl -l -v in cmd . See how to migrate .","title":"Network-related issues"},{"location":"New-Starters/2.-Environment-Setup/#building-fe-in-qgl-takes-too-long","text":"FE projects aren't meant to be built using QGL on developer machines. We only support QGL as a necessity to be able to build FE on DAT. If you are doing FE development - don't use QGL to build FE. Most of the time all you need it nvm i && npm ci && npm start , see more details in guides above.","title":"Building FE in QGL takes too long"},{"location":"New-Starters/2.-Environment-Setup/#cant-find-qgl","text":"It's normally downloaded by Crikey, see How to install Crikey Monitor manually? or How to get QGL for Git without QGL for Git?","title":"Can't find QGL"},{"location":"New-Starters/2.-Environment-Setup/#errors-installing-java","text":"If you experience dependencies errors when installing default-jdk , try to execute ~$ sudo apt --fix-broken install . To verify Java is correctly installed use ~$ java -version in WSL Ubuntu terminal.","title":"Errors installing Java"},{"location":"New-Starters/2.-Environment-Setup/#cant-download-chrome","text":"You can find the current URL here in the \"Linux\" -> \"Stable channel\" section","title":"Can't download Chrome"},{"location":"New-Starters/2.-Environment-Setup/#running-cypress-has-error-the-test-runner-unexpectedly-exited-via-a-exit-event-with-signal-sigsegv","text":"If you run into this error and still use the old VcXsrv approach to rendering GUI Apps in WSL, it's probably better to update your WSL now that GUI Apps are supported out of the box: Run powershell as admin, then run the command wsl --update Once updated, remove these lines from your .bashrc in your Ubuntu home directory: # set DISPLAY variable to the IP automatically assigned to WSL2 export DISPLAY=$(cat /etc/resolv.conf | grep nameserver | awk '{print $2; exit;}'):0.0 sudo /etc/init.d/dbus start &> /dev/null Afterwards ensure that VcXsrv AKA Windows X Server or XLaunch is no longer running in Windows. You can even uninstall it since we won't be using it anymore.","title":"Running Cypress has error: \"The Test Runner unexpectedly exited via a exit event with signal SIGSEGV\""},{"location":"New-Starters/2.-Environment-Setup/#cant-connect-to-myaccount-portalcargowisecom-from-local-be","text":"Refer to Can't connect to myaccount-portal.cargowise.com from local BE .","title":"Can't connect to myaccount-portal.cargowise.com from local BE"},{"location":"New-Starters/2.-Environment-Setup/#wslg-doesnt-work","text":"If you're using new default X server - https://github.com/microsoft/wslg and when you do echo $DISPLAY it doesn't say :0 - you might have it disabled, check the config c:\\users\\MyUser\\.wslconfig : [wsl2] guiApplications=true see https://github.com/microsoft/wslg#wslg-system-distro for details.","title":"WSLg doesn't work"},{"location":"New-Starters/2.-Environment-Setup/#cant-download-vcxsrv-from-the-office-network","text":"Sourceforge.net seems to be blocked by the office network. You can find VcXsrv (WSLg replacement) in this network share: \\\\datfiles.wtg.zone\\ThirdParty\\Web\\BorderWise\\VcXsrv","title":"Can't download VcXsrv from the office network"},{"location":"New-Starters/2.-Environment-Setup/#wsl-wont-start-after-install","text":"After you install WSL and attempt to launch Ubuntu, you might get an error after some time instead of a command prompt. It can be solved by making sure these Windows Features are enabled: Hyper-V and Virtual Machine Platform (Windows Hypervisor Platform might help as well). These errors look like this: Error code: Wsl/Service/CreateInstance/CreateVm/WSL_E_VIRTUAL_MACHINE_PLATFORM_REQUIRED Error code: Wsl/Service/CreateInstance/CreateVm/ConfigureNetworking/HNS/0x80070424","title":"WSL won't start after install"},{"location":"New-Starters/3.-Next-steps/","text":"Before jumping into actual work, these are highly recommended readings by project/area: Project Area Recommended readings Conditions/Notes BW FE/BE Test rigs (UATs) Will only need after code reviews are done BW BE Generating MongoDB Transformations If you ever need to make MongoDB transformations BW FE Testing + Writing BWW Tests + Running BWW Tests TSX and JSX Templates in Vue If working on Vue part, not just legacy AngularJS part Vue+Vuetify migration from AngularJS If working on Vue/Vuetify migration AngularJS to Vue props reactivity If working with props or ng-prop ve-* components props validation If working with props on components Orval and Vue Query If using query hooks WW FE TSX and JSX Templates in Vue Orval and Vue Query If using query hooks Testing with MSW If testing functionality that involves requests","title":"3. Next steps"},{"location":"New-Starters/4.-Creating-a-PR/","text":"[[ TOC ]] Branch names Please include WI number and summary in the branch name: ${Link-from-ediProd} If there are multiple coding workflows - include the workflow name as well: ${Link-from-ediProd} - {Workflow-Description} Copy WI link from ediProd: Actions -> Copy Hyperlink to Clipboard ( Ctrl + H ) Create a new branch in VS Code: Ctrl + Shift + P -> Git: Create Branch... Enter your staff code and / , like so: MMZ/ Paste what you copied from ediProd VS Code will automatically replace spaces with - (if working on multiple coding workflows) add - and paster workflow name as well The benefit is that we can more easily switch between branches without having to remember WI numbers and using keywords instead. For example: - Staff code: MMZ - WI number: WI00525665 - WI summary: WW. Generic API support - Workflow Description: Dev - Field Child Components - Branch name entered in VS Code: MMZ/WI00525665 - WW. Generic API support - Dev - Field Child Components - Branch name after auto-conversion: MMZ/WI00525665---WW.-Generic-API-support---Dev---Field-Child-Components PR Name Same as for branches, make sure it includes the WI number and summary: ${Link from ediProd} Include Workflow Description if there are multiple coding workflows: ${Link from ediProd} - {Workflow Description} For example: - WI number: WI00525665 - WI summary: WW. Generic API support - Workflow Description: Dev - Field Child Components - PR Name: WI00525665 - WW. Generic API support - Dev - Field Child Components Draft PR To keep PRs list nice and tidy, as well as to protect from accidental merges, unless your PR is ready for reviews, create it as a Draft: If you already created PR and want to convert it to a draft: Then once it's ready: Resolving conflicts See this guide on how to resolve conflicts in VS Code for FE changes.","title":"4. Creating a PR"},{"location":"New-Starters/4.-Creating-a-PR/#branch-names","text":"Please include WI number and summary in the branch name: ${Link-from-ediProd} If there are multiple coding workflows - include the workflow name as well: ${Link-from-ediProd} - {Workflow-Description} Copy WI link from ediProd: Actions -> Copy Hyperlink to Clipboard ( Ctrl + H ) Create a new branch in VS Code: Ctrl + Shift + P -> Git: Create Branch... Enter your staff code and / , like so: MMZ/ Paste what you copied from ediProd VS Code will automatically replace spaces with - (if working on multiple coding workflows) add - and paster workflow name as well The benefit is that we can more easily switch between branches without having to remember WI numbers and using keywords instead. For example: - Staff code: MMZ - WI number: WI00525665 - WI summary: WW. Generic API support - Workflow Description: Dev - Field Child Components - Branch name entered in VS Code: MMZ/WI00525665 - WW. Generic API support - Dev - Field Child Components - Branch name after auto-conversion: MMZ/WI00525665---WW.-Generic-API-support---Dev---Field-Child-Components","title":"Branch names"},{"location":"New-Starters/4.-Creating-a-PR/#pr-name","text":"Same as for branches, make sure it includes the WI number and summary: ${Link from ediProd} Include Workflow Description if there are multiple coding workflows: ${Link from ediProd} - {Workflow Description} For example: - WI number: WI00525665 - WI summary: WW. Generic API support - Workflow Description: Dev - Field Child Components - PR Name: WI00525665 - WW. Generic API support - Dev - Field Child Components","title":"PR Name"},{"location":"New-Starters/4.-Creating-a-PR/#draft-pr","text":"To keep PRs list nice and tidy, as well as to protect from accidental merges, unless your PR is ready for reviews, create it as a Draft: If you already created PR and want to convert it to a draft: Then once it's ready:","title":"Draft PR"},{"location":"New-Starters/4.-Creating-a-PR/#resolving-conflicts","text":"See this guide on how to resolve conflicts in VS Code for FE changes.","title":"Resolving conflicts"},{"location":"Specs/Authentication-process-diagrams/","text":"Authentication process in BWW is very complicated so diagrams will help us to grab the basic stages during this process: 1. Simple authentication diagram 2. More implementation-oriented diagram","title":"Authentication process diagrams"},{"location":"Specs/Books-in-BW/","text":"Terminology and Types PlanBook Plan books are a book type that represents books within a Plan in the Plans collection. The Plans collection is made up of a bunch of 'Plans', each corresponding to a particular country (NOTE: one is global). Each 'Plan' contains a list of books. Thus, each plan book either belongs to a particular country plan (e.g Singapore (SG)) or the global plan (GB). The plan collection contains all the books found in the BorderWise library (as can be seen in the library tab). A few key properties: | Property | Description | |--|--| | Browsable | This keeps track of whether the book should be added to 'My Books' by default for new users. More on that later. | LibraryBook The LibraryBook type is used to represent books that are stored in a User in the Users collection. The Users collection keeps track of all the users that have signed up to BW. When a user decides to place some sort of preference on a book, for example, that they want it in their 'My Books' tab, the information from the corresponding PlanBook will be used to make a LibraryBook that is stored in that particular user's books list in the Users collection. To represent that the user has chosen to add it to their 'My Books', we set browsable = true for this LibraryBook. The 'My Books' list is constructed from this book list, filtering through and getting all the books where browsable = true. A few key properties: Property Description Browsable This determines whether the book should show in 'My Books' for this particular user. Searchable This determines whether the book should be searched when the 'My Books' search option is toggled on. How are the 'My Books' and 'Library' tab lists? The 'My Books' tab list is constructed as the list of LibraryBook type books in the User's book list that have `browsable = true. The 'Library' tab list is constructed as the list of PlanBook type books in the Plan of the country that the user has selected, merged with the PlanBook type books from the global Plan. How do we populate a user's 'my books' list when a user is first created? As BW devs and product specialists, we have the ability to edit and create books. When doing so, you will notice the option to Show in My Books by default for new clients . By clicking this checkbox, it means that we set the corresponding PlanBook to have property browsable = true . When we create a new user, we look in the plan repository and find any books where either browsable = true or searchable = true . For these matching books, we create a new, corresponding LibraryBook and add it to the new user's book list. Hence, any book that had the Show in My Books by default for new clients checkbox checked at that point in time, will be added to the new user's book list. How does a user update their 'My Books' list? A user can add a book to their 'My Books' list by clicking on the green plus button to the right of the book name in their library. When they click this button, we first look in the user's book list to see if the book already exists there. If it does, we set browsable = true and searchable = true for this LibraryBook. If it is not there, we create a new LibraryBook with the same information as the corresponding PlanBook but with browsable = true and searchable = true . A user may remove a book from 'My Books' by clicking on the minus arrow to the right of the book name in the 'My Books' tab. When they click this button, the book is deleted from their user book list altogether (but remains in the plan collection so will still show under the library tab). What is the searchable flag? Books in 'My Books' have the option to be made 'searchable'. This is controlled by the checkbox to the left of the book name. This means, that when searching within my books, we will only search the books that have this box checked. Clarifying some references Throughout the codebase, there are various lists of books and it is not always clear what type of book they are referring to. Here is a table to make it easier to follow. File Type referred to Name library.service.js LibraryBook service.data.books library.service.js PlanBook service.data.libraryBooks books.controller.js LibraryBook vm.myBooks books.controller.js PlanBook vm.libraryBooks","title":"Terminology and Types"},{"location":"Specs/Books-in-BW/#terminology-and-types","text":"","title":"Terminology and Types"},{"location":"Specs/Books-in-BW/#planbook","text":"Plan books are a book type that represents books within a Plan in the Plans collection. The Plans collection is made up of a bunch of 'Plans', each corresponding to a particular country (NOTE: one is global). Each 'Plan' contains a list of books. Thus, each plan book either belongs to a particular country plan (e.g Singapore (SG)) or the global plan (GB). The plan collection contains all the books found in the BorderWise library (as can be seen in the library tab). A few key properties: | Property | Description | |--|--| | Browsable | This keeps track of whether the book should be added to 'My Books' by default for new users. More on that later. |","title":"PlanBook"},{"location":"Specs/Books-in-BW/#librarybook","text":"The LibraryBook type is used to represent books that are stored in a User in the Users collection. The Users collection keeps track of all the users that have signed up to BW. When a user decides to place some sort of preference on a book, for example, that they want it in their 'My Books' tab, the information from the corresponding PlanBook will be used to make a LibraryBook that is stored in that particular user's books list in the Users collection. To represent that the user has chosen to add it to their 'My Books', we set browsable = true for this LibraryBook. The 'My Books' list is constructed from this book list, filtering through and getting all the books where browsable = true. A few key properties: Property Description Browsable This determines whether the book should show in 'My Books' for this particular user. Searchable This determines whether the book should be searched when the 'My Books' search option is toggled on.","title":"LibraryBook"},{"location":"Specs/Books-in-BW/#how-are-the-my-books-and-library-tab-lists","text":"The 'My Books' tab list is constructed as the list of LibraryBook type books in the User's book list that have `browsable = true. The 'Library' tab list is constructed as the list of PlanBook type books in the Plan of the country that the user has selected, merged with the PlanBook type books from the global Plan.","title":"How are the 'My Books' and 'Library' tab lists?"},{"location":"Specs/Books-in-BW/#how-do-we-populate-a-users-my-books-list-when-a-user-is-first-created","text":"As BW devs and product specialists, we have the ability to edit and create books. When doing so, you will notice the option to Show in My Books by default for new clients . By clicking this checkbox, it means that we set the corresponding PlanBook to have property browsable = true . When we create a new user, we look in the plan repository and find any books where either browsable = true or searchable = true . For these matching books, we create a new, corresponding LibraryBook and add it to the new user's book list. Hence, any book that had the Show in My Books by default for new clients checkbox checked at that point in time, will be added to the new user's book list.","title":"How do we populate a user's 'my books' list when a user is first created?"},{"location":"Specs/Books-in-BW/#how-does-a-user-update-their-my-books-list","text":"A user can add a book to their 'My Books' list by clicking on the green plus button to the right of the book name in their library. When they click this button, we first look in the user's book list to see if the book already exists there. If it does, we set browsable = true and searchable = true for this LibraryBook. If it is not there, we create a new LibraryBook with the same information as the corresponding PlanBook but with browsable = true and searchable = true . A user may remove a book from 'My Books' by clicking on the minus arrow to the right of the book name in the 'My Books' tab. When they click this button, the book is deleted from their user book list altogether (but remains in the plan collection so will still show under the library tab).","title":"How does a user update their 'My Books' list?"},{"location":"Specs/Books-in-BW/#what-is-the-searchable-flag","text":"Books in 'My Books' have the option to be made 'searchable'. This is controlled by the checkbox to the left of the book name. This means, that when searching within my books, we will only search the books that have this box checked.","title":"What is the searchable flag?"},{"location":"Specs/Books-in-BW/#clarifying-some-references","text":"Throughout the codebase, there are various lists of books and it is not always clear what type of book they are referring to. Here is a table to make it easier to follow. File Type referred to Name library.service.js LibraryBook service.data.books library.service.js PlanBook service.data.libraryBooks books.controller.js LibraryBook vm.myBooks books.controller.js PlanBook vm.libraryBooks","title":"Clarifying some references"},{"location":"Specs/CW1%252DBorderWise-integration-specification/","text":"Summary This document outlines the integration specification for the WebSocket API integration between CargoWise One (CW1) and BorderWise, specifically focusing on the WebSocket Hub component. What is the WebSocket API and why do we need it for BorderWise and CargoWiseOne integration? The WebSocket API allows real-time communication between two systems using WebSocket technology. It allows us to send messages to a WebSocket hub and receive event-driven responses without having to poll the WebSocket hub for a reply. In our scenario, it allows for real-time communication between BorderWise and CargowiseOne. A high level overview of BorderWise and CargoWiseOne integration using the WebSocket API When we open BorderWiseWeb from CargoWiseOne, we end up with a BorderWise Web URL with various query parameters. For example, https://app.borderwise.com/? tariff =0203.11.00 stat =07&impexp=I&c=AU&ret=edient:Command%3DShowEditForm%26LicenceCode%3DEDIEDIDAT%26ControllerID%3DBorderWiseWebReturnHook%26BusinessEntityPK%3D7edaf780-a807-4319-928a-b21cf7ad69b9%26Hash%3D%2bCTL0nUJ841AN0c6JHjyqdKbl2GezTpZz%26Args%3D&version=2& mode =Batch&WebSocketClientId=7edaf780-a807-4319-928a-b21cf7ad69b9&WebSocketTimestamp=20230531102831 This link will open up to a tariff table (as per tariff param provided in url) for the respective country ( country param), with a specific status code highlighted ( stat param). Further, when we open BorderWise with this URL, we either end up in Single-Line Tariff Classification mode or Multi-Line Tariff Classification mode based on url parameter mode . This URL also includes a WebSocketClientId , a unique client ID that BWW will use to establish a connection with the WebSocket Hub, and ensure that all messages sent to the hub by BWW can be forwarded on to the correct CW1 instance. Read more about the BorderWise URL parameters here . 1. When the BWW app is opened in the step above, we are initializing the web socket connection . 1. After successfully initializing the WebSocket connection with the BorderWise WebSocket hub, the WebSocket's onOpen event handler is triggered and BWW proceeds to send an initial message to the BorderWise WebSocket Hub, indicating the start (STA) of web socket exchange. All these messages exchanged over the WebSocket channel have a specific format . The Message flow varies according to the mode of tariff classification: For Single-line Tariff Classification After specifying the Tariff Code for the single invoice line, BorderWise will send a response message (MSG) with the tariff code to BorderWise WebSocket Hub. This gets passed on to CW1. CW1 sends message (FIN) indicating the completion of WebSocket exchange. This message is relayed to BWW and the WebSocket connection is closed as a result. For Multi-line Tariff Classification Proceeding this, BWW sends a message (MSG) to the BorderWise WebSocket Hub indicating that the WebSocket is connected, and this is then relayed to CW1. Now that the WebSocket connection has been established, CW1 can send a message to the WebSocket Hub with the invoice lines . WebSocket Hub then pushes these invoice lines to BWW. After classifying the invoice lines, BorderWise will send a response message (MSG) with the tariff classification to BorderWise WebSocket Hub. This gets passed on to CW1. CW1 sends message (FIN) indicating the completion of WebSocket exchange. This message is relayed to BWW and the WebSocket connection is closed as a result. Outside of this chain of events, we can also have error (ERR) and cancellation (CAN) messages. Read more about these here . Message Flow Diagram The below diagram depicts flow of messages between: - CargoWise One (CW1) - BorderWise Portal (BW Web) - BorderWise WebSocket Hub (BWW.API) Note : If the BorderWise web application fails to send a WebSocket message to CW1 due to WebSocket connection failure, it will fallback to using EdiCommand to send the message. BorderWise WebSocket Hub API Description This API endpoint handles WebSocket requests and establishes a WebSocket connection with the client. It listens for incoming WebSocket connections and processes messages sent by the client. Route GET https://app.borderwise.com/api/ws/hub Request Headers No specific request headers are required for this API. Request Parameters No request parameters are used in this API. Response The API establishes a WebSocket connection with the client if the request is a valid WebSocket upgrade request. If the WebSocket connection is successfully established, it proceeds to process the upcoming web socket messages. Errors If the request is not a valid WebSocket request, the API responds with a BadRequest status code (HTTP 400). If the WebSocket connection is closed with an error, the API logs the error and sets the response status code to BadRequest (HTTP 400). Authentication This API does not require any authentication. Clients are supposed to use unique identifiers (GUID) for message exchange which guarantees that fraudulent client is not able to send fake messages to real users unless they know the identifier More details Refer to the article CargowiseOne and Borderwise Integration for an overview of BorderWise WebSocket Hub Architecture. BorderWise URL Parameters When CW1 initiates the opening of BorderWise, certain URL parameters are included as part of the URL. These parameters determine the behavior of the BorderWise. Below are the details of each parameter passed in the URL: tariff : This parameter specifies the tariff code to be searched for once BorderWise is opened. After loading BorderWise, it will display the tariff table containing this specific tariff code. stat : This parameter indicates the stat code to be highlighted upon opening BorderWise. After loading BorderWise, it will highlight the specified stat code in the tariff table. impexp : This parameter determines the type of tariff classification and can take two possible values: \"I\": Indicates Import classification. \"E\": Indicates Export classification. country : This parameter identifies the country code of tariff classification. Currently supported country codes by BorderWise are AU: Australia CA: Canada EU: European Union NZ: New Zealand SG: Singapore ZA: South Africa UK: United Kingdom US: United States ret : This parameter contains the edient command that should be used to return tariff data back to CW1 in case the WebSocket connection fails. version : This parameter determines the version of the Exchange Message Model between CW1 and BorderWise. Currently supported values for this parameter are 1 : This model supports only one tariff classification at a time, and this will be outdated in some time. 2 : This model supports multi-line tariff classification in one go. mode : This parameter determines the mode of tariff classification and can have the following possible values: SINGLE : Indicates single-line tariff classification, meaning only one invoice line can be classified from the current instance of BorderWise. BATCH : Indicates multi-line tariff classification, allowing multiple invoice lines to be classified from the current instance of BorderWise. The value of this parameter is determined by BW API based on user details and country of tariff classification. WebSocketClientId : This parameter represents a unique client ID used by BorderWise to establish a connection with the WebSocket Hub. WebSocketTimestamp : This parameter specifies the current timestamp in the format yyyyMMddHHmmss . Example of BorderWise URL opened from CW1 https://app.borderwise.com/?tariff=0203.11.00&stat=07&impexp=I&c=AU&ret=edient:Command%3DShowEditForm%26LicenceCode%3DEDIEDIDAT%26ControllerID%3DBorderWiseWebReturnHook%26BusinessEntityPK%3D7edaf780-a807-4319-928a-b21cf7ad69b9%26Hash%3D%252bCTL0nUJ841AN0c6JHjyqdKbl2GezTpZz%26Args%3D&version=2&mode=Batch&WebSocketClientId=7edaf780-a807-4319-928a-b21cf7ad69b9&WebSocketTimestamp=20230531102831 Exchange Message Format The messages exchanged over WebSocket channel have to follow a specific format and the WebSocketExchangeMessage class represents the data model for it. Below are the details of fields in the WebSocketExchangeMessage Model. ClientId (Guid): A unique identifier assigned to each WebSocket client. SystemNameSender (string): A string representing the name of the system that sent the message. SystemNameRecipient (string): A string representing the name of the system intended to receive the message. Status ( WebSocketExchangeMessageStatus ): An enum representing the status of the WebSocket message. It has the following possible values. STA (Status - 1) stands for \"START\" and is used to start websocket message exchange. This should be the first message sent to WebSocket Hub after establishing the connection MSG (Message - 2) stands for \"Message\" and represents a regular message exchanged between the client and the server. FIN (Finish - 3) stands for \"Finish\" and indicates the end or completion of a WebSocket exchange or communication. ACK (Acknowledgment - 4) stands for \"Acknowledgment\" and is used to acknowledge the receipt or successful processing of a WebSocket message. ERR (Error - 5) stands for \"Error\" and is used to indicate that an error has occurred during the WebSocket communication. Usage: When an error is encountered in the communication process, the server or client may send an ERR message to inform the other party about the issue. CAN (Cancellation - 6) stands for \"Cancellation\" and is used to signal the cancellation of a WebSocket exchange or action. Usage: When a WebSocket exchange or task needs to be canceled before completion, the server or client may send a CAN message to notify the other party about the cancellation. This event is sent by CW1 once the user press the Cancel button on the modal window. Data (string): A string containing the main content or payload of the WebSocket message. The data has to follow some specific format when the type of message is not Information i.e. it is a TariffClassification/InvoiceLine msg. The format of the data attribute depends on the Mode parameter in the url passed by CW1 When the Mode param in the URL is BATCH, it implies that the user does the tariff classification for all the invoice lines in one go, then data should follow the following format { BorderWiseTariffSelectionResults: [ { invoiceLinePK : guid, invoiceLineNumber: integer, invoiceNumber : string, tariffCode : string, statCode : string, description : string, } ] } And when Mode param in the URL is SINGLE, which implies that the user is classifying only one invoice line, then data should follow the below format { TariffCode: string } Type (string): A string representing the type or category of the message. It has the following possible value. ###Information (Information): Represents that the message transferred is an information message and not an actual InvoiceLines/TariffClassification msg. MachineName (string): A string representing the name of the machine where the message originated from. Email (string): A string representing an email associated with the WebSocket message (e.g., the sender's email). Timestamp (DateTime): A DateTime property representing the timestamp of when the message was created. It is initialized with the current UTC time. Sample WebSocket Exchange Messages Message with Status MSG { ClientId: 6d674c0c-5403-4039-af8d-5c8dc6bfddfe, SystemNameSender: \"CargoWiseOne \", SystemNameRecipient: \"BorderWiseWeb\", Status: MSG, Data: { BorderWiseTariffSelectionResults : [ { Description: \".Weighing more than 1 kg but not more than 5 kg\", InvoiceLineNumber: \"1\", InvoiceNumber: \"123\", IsSelected: false, StatCode: \"\", TariffCode: \"2205.90.90 14\" } ] }, Type: null, MachineName: null, Email: \"testmail@ext\", Timestamp: 07/27/2023 09:37:29 } Cancel Message from CW1 { ClientId: 6d674c0c-5403-4039-af8d-5c8dc6bfddfe, SystemNameSender: \"CargoWiseOne \", SystemNameRecipient: \"BorderWiseWeb\", Status: CAN, Data: null, Type: null, MachineName: null, Email: \"testmail@ext\", Timestamp: 07/27/2023 09:37:29 } BorderWise Portal WebSocket event handlers Source code URL: BWW_ConnectToBWWWebSocketHub_SourceCode Initiate WebSocket Connection Source code URL: createWebSocketConnection When CW1 opens BorderWise window, the unique client ID is passed as a URL parameter with the name webSocketClientId. This client ID is used for further web socket message exchange. It ensures that all the messages sent to the Hub are redirected to the correct CW1 instance. BWW uses browser built-in WebSocket API to interact with WebSocket connections. To instantiate the WebSocket connection, the URL needed is wss://app.borderwise.com/api/ws/hub in the case of BorderWise WebSocket Hub. When WebSocket connection is successfully established Source code URL: startWebSocketMessageExchange After successfully establishing the WebSocket connection with the BorderWise WebSocket hub, the WebSocket's onOpen event handler is triggered and the client proceeds to send an STA start message. Sample STA start message { ClientId: 6d674c0c-5403-4039-af8d-5c8dc6bfddfe, SystemNameSender: \"BorderWiseWeb\", SystemNameRecipient: \"BWW.API\", Status: STA, Data: null, Type: null, MachineName: null, Email: \"testmail@ext\", Timestamp: 07/27/2023 09:37:29 } On Receiving a Message from WebSocket Hub Source code URL: processWebSocketMessageExchange WebSocket onMessage event handler is triggered which does further processing of the data received. Sample message { ClientId: 6d674c0c-5403-4039-af8d-5c8dc6bfddfe, SystemNameSender: \"CargoWiseOne \", SystemNameRecipient: \"BorderWiseWeb\", Status: MSG, Data: \"BorderWiseWeb websocket connected.\", Type: Information, MachineName: null, Email: \"testmail@ext\", Timestamp: 07/27/2023 09:37:29 } On Error in WebSocket Connection Source Code URL: onWebSocketConnectionError The onError event is fired when there is an error during the WebSocket connection or communication process. This could happen due to various reasons, such as server unavailability, connection disruptions, or protocol errors. If an error is detected during the WebSocket communication, the code generates an error message indicating the client's WebSocket ID (webSocketClientId) and the corresponding event code.","title":"Summary"},{"location":"Specs/CW1%252DBorderWise-integration-specification/#summary","text":"This document outlines the integration specification for the WebSocket API integration between CargoWise One (CW1) and BorderWise, specifically focusing on the WebSocket Hub component.","title":"Summary"},{"location":"Specs/CW1%252DBorderWise-integration-specification/#what-is-the-websocket-api-and-why-do-we-need-it-for-borderwise-and-cargowiseone-integration","text":"The WebSocket API allows real-time communication between two systems using WebSocket technology. It allows us to send messages to a WebSocket hub and receive event-driven responses without having to poll the WebSocket hub for a reply. In our scenario, it allows for real-time communication between BorderWise and CargowiseOne.","title":"What is the WebSocket API and why do we need it for BorderWise and CargoWiseOne integration?"},{"location":"Specs/CW1%252DBorderWise-integration-specification/#a-high-level-overview-of-borderwise-and-cargowiseone-integration-using-the-websocket-api","text":"When we open BorderWiseWeb from CargoWiseOne, we end up with a BorderWise Web URL with various query parameters. For example, https://app.borderwise.com/? tariff =0203.11.00 stat =07&impexp=I&c=AU&ret=edient:Command%3DShowEditForm%26LicenceCode%3DEDIEDIDAT%26ControllerID%3DBorderWiseWebReturnHook%26BusinessEntityPK%3D7edaf780-a807-4319-928a-b21cf7ad69b9%26Hash%3D%2bCTL0nUJ841AN0c6JHjyqdKbl2GezTpZz%26Args%3D&version=2& mode =Batch&WebSocketClientId=7edaf780-a807-4319-928a-b21cf7ad69b9&WebSocketTimestamp=20230531102831 This link will open up to a tariff table (as per tariff param provided in url) for the respective country ( country param), with a specific status code highlighted ( stat param). Further, when we open BorderWise with this URL, we either end up in Single-Line Tariff Classification mode or Multi-Line Tariff Classification mode based on url parameter mode . This URL also includes a WebSocketClientId , a unique client ID that BWW will use to establish a connection with the WebSocket Hub, and ensure that all messages sent to the hub by BWW can be forwarded on to the correct CW1 instance. Read more about the BorderWise URL parameters here . 1. When the BWW app is opened in the step above, we are initializing the web socket connection . 1. After successfully initializing the WebSocket connection with the BorderWise WebSocket hub, the WebSocket's onOpen event handler is triggered and BWW proceeds to send an initial message to the BorderWise WebSocket Hub, indicating the start (STA) of web socket exchange. All these messages exchanged over the WebSocket channel have a specific format . The Message flow varies according to the mode of tariff classification: For Single-line Tariff Classification After specifying the Tariff Code for the single invoice line, BorderWise will send a response message (MSG) with the tariff code to BorderWise WebSocket Hub. This gets passed on to CW1. CW1 sends message (FIN) indicating the completion of WebSocket exchange. This message is relayed to BWW and the WebSocket connection is closed as a result. For Multi-line Tariff Classification Proceeding this, BWW sends a message (MSG) to the BorderWise WebSocket Hub indicating that the WebSocket is connected, and this is then relayed to CW1. Now that the WebSocket connection has been established, CW1 can send a message to the WebSocket Hub with the invoice lines . WebSocket Hub then pushes these invoice lines to BWW. After classifying the invoice lines, BorderWise will send a response message (MSG) with the tariff classification to BorderWise WebSocket Hub. This gets passed on to CW1. CW1 sends message (FIN) indicating the completion of WebSocket exchange. This message is relayed to BWW and the WebSocket connection is closed as a result. Outside of this chain of events, we can also have error (ERR) and cancellation (CAN) messages. Read more about these here .","title":"A high level overview of BorderWise and CargoWiseOne integration using the WebSocket API"},{"location":"Specs/CW1%252DBorderWise-integration-specification/#message-flow-diagram","text":"The below diagram depicts flow of messages between: - CargoWise One (CW1) - BorderWise Portal (BW Web) - BorderWise WebSocket Hub (BWW.API) Note : If the BorderWise web application fails to send a WebSocket message to CW1 due to WebSocket connection failure, it will fallback to using EdiCommand to send the message.","title":"Message Flow Diagram"},{"location":"Specs/CW1%252DBorderWise-integration-specification/#borderwise-websocket-hub-api","text":"","title":"BorderWise WebSocket Hub API"},{"location":"Specs/CW1%252DBorderWise-integration-specification/#description","text":"This API endpoint handles WebSocket requests and establishes a WebSocket connection with the client. It listens for incoming WebSocket connections and processes messages sent by the client.","title":"Description"},{"location":"Specs/CW1%252DBorderWise-integration-specification/#route","text":"GET https://app.borderwise.com/api/ws/hub","title":"Route"},{"location":"Specs/CW1%252DBorderWise-integration-specification/#request-headers","text":"No specific request headers are required for this API.","title":"Request Headers"},{"location":"Specs/CW1%252DBorderWise-integration-specification/#request-parameters","text":"No request parameters are used in this API.","title":"Request Parameters"},{"location":"Specs/CW1%252DBorderWise-integration-specification/#response","text":"The API establishes a WebSocket connection with the client if the request is a valid WebSocket upgrade request. If the WebSocket connection is successfully established, it proceeds to process the upcoming web socket messages.","title":"Response"},{"location":"Specs/CW1%252DBorderWise-integration-specification/#errors","text":"If the request is not a valid WebSocket request, the API responds with a BadRequest status code (HTTP 400). If the WebSocket connection is closed with an error, the API logs the error and sets the response status code to BadRequest (HTTP 400).","title":"Errors"},{"location":"Specs/CW1%252DBorderWise-integration-specification/#authentication","text":"This API does not require any authentication. Clients are supposed to use unique identifiers (GUID) for message exchange which guarantees that fraudulent client is not able to send fake messages to real users unless they know the identifier","title":"Authentication"},{"location":"Specs/CW1%252DBorderWise-integration-specification/#more-details","text":"Refer to the article CargowiseOne and Borderwise Integration for an overview of BorderWise WebSocket Hub Architecture.","title":"More details"},{"location":"Specs/CW1%252DBorderWise-integration-specification/#borderwise-url-parameters","text":"When CW1 initiates the opening of BorderWise, certain URL parameters are included as part of the URL. These parameters determine the behavior of the BorderWise. Below are the details of each parameter passed in the URL: tariff : This parameter specifies the tariff code to be searched for once BorderWise is opened. After loading BorderWise, it will display the tariff table containing this specific tariff code. stat : This parameter indicates the stat code to be highlighted upon opening BorderWise. After loading BorderWise, it will highlight the specified stat code in the tariff table. impexp : This parameter determines the type of tariff classification and can take two possible values: \"I\": Indicates Import classification. \"E\": Indicates Export classification. country : This parameter identifies the country code of tariff classification. Currently supported country codes by BorderWise are AU: Australia CA: Canada EU: European Union NZ: New Zealand SG: Singapore ZA: South Africa UK: United Kingdom US: United States ret : This parameter contains the edient command that should be used to return tariff data back to CW1 in case the WebSocket connection fails. version : This parameter determines the version of the Exchange Message Model between CW1 and BorderWise. Currently supported values for this parameter are 1 : This model supports only one tariff classification at a time, and this will be outdated in some time. 2 : This model supports multi-line tariff classification in one go. mode : This parameter determines the mode of tariff classification and can have the following possible values: SINGLE : Indicates single-line tariff classification, meaning only one invoice line can be classified from the current instance of BorderWise. BATCH : Indicates multi-line tariff classification, allowing multiple invoice lines to be classified from the current instance of BorderWise. The value of this parameter is determined by BW API based on user details and country of tariff classification. WebSocketClientId : This parameter represents a unique client ID used by BorderWise to establish a connection with the WebSocket Hub. WebSocketTimestamp : This parameter specifies the current timestamp in the format yyyyMMddHHmmss . Example of BorderWise URL opened from CW1 https://app.borderwise.com/?tariff=0203.11.00&stat=07&impexp=I&c=AU&ret=edient:Command%3DShowEditForm%26LicenceCode%3DEDIEDIDAT%26ControllerID%3DBorderWiseWebReturnHook%26BusinessEntityPK%3D7edaf780-a807-4319-928a-b21cf7ad69b9%26Hash%3D%252bCTL0nUJ841AN0c6JHjyqdKbl2GezTpZz%26Args%3D&version=2&mode=Batch&WebSocketClientId=7edaf780-a807-4319-928a-b21cf7ad69b9&WebSocketTimestamp=20230531102831","title":"BorderWise URL Parameters"},{"location":"Specs/CW1%252DBorderWise-integration-specification/#exchange-message-format","text":"The messages exchanged over WebSocket channel have to follow a specific format and the WebSocketExchangeMessage class represents the data model for it. Below are the details of fields in the WebSocketExchangeMessage Model. ClientId (Guid): A unique identifier assigned to each WebSocket client. SystemNameSender (string): A string representing the name of the system that sent the message. SystemNameRecipient (string): A string representing the name of the system intended to receive the message. Status ( WebSocketExchangeMessageStatus ): An enum representing the status of the WebSocket message. It has the following possible values. STA (Status - 1) stands for \"START\" and is used to start websocket message exchange. This should be the first message sent to WebSocket Hub after establishing the connection MSG (Message - 2) stands for \"Message\" and represents a regular message exchanged between the client and the server. FIN (Finish - 3) stands for \"Finish\" and indicates the end or completion of a WebSocket exchange or communication. ACK (Acknowledgment - 4) stands for \"Acknowledgment\" and is used to acknowledge the receipt or successful processing of a WebSocket message. ERR (Error - 5) stands for \"Error\" and is used to indicate that an error has occurred during the WebSocket communication. Usage: When an error is encountered in the communication process, the server or client may send an ERR message to inform the other party about the issue. CAN (Cancellation - 6) stands for \"Cancellation\" and is used to signal the cancellation of a WebSocket exchange or action. Usage: When a WebSocket exchange or task needs to be canceled before completion, the server or client may send a CAN message to notify the other party about the cancellation. This event is sent by CW1 once the user press the Cancel button on the modal window. Data (string): A string containing the main content or payload of the WebSocket message. The data has to follow some specific format when the type of message is not Information i.e. it is a TariffClassification/InvoiceLine msg. The format of the data attribute depends on the Mode parameter in the url passed by CW1 When the Mode param in the URL is BATCH, it implies that the user does the tariff classification for all the invoice lines in one go, then data should follow the following format { BorderWiseTariffSelectionResults: [ { invoiceLinePK : guid, invoiceLineNumber: integer, invoiceNumber : string, tariffCode : string, statCode : string, description : string, } ] } And when Mode param in the URL is SINGLE, which implies that the user is classifying only one invoice line, then data should follow the below format { TariffCode: string } Type (string): A string representing the type or category of the message. It has the following possible value. ###Information (Information): Represents that the message transferred is an information message and not an actual InvoiceLines/TariffClassification msg. MachineName (string): A string representing the name of the machine where the message originated from. Email (string): A string representing an email associated with the WebSocket message (e.g., the sender's email). Timestamp (DateTime): A DateTime property representing the timestamp of when the message was created. It is initialized with the current UTC time.","title":"Exchange Message Format"},{"location":"Specs/CW1%252DBorderWise-integration-specification/#sample-websocket-exchange-messages","text":"","title":"Sample WebSocket Exchange Messages"},{"location":"Specs/CW1%252DBorderWise-integration-specification/#message-with-status-msg","text":"{ ClientId: 6d674c0c-5403-4039-af8d-5c8dc6bfddfe, SystemNameSender: \"CargoWiseOne \", SystemNameRecipient: \"BorderWiseWeb\", Status: MSG, Data: { BorderWiseTariffSelectionResults : [ { Description: \".Weighing more than 1 kg but not more than 5 kg\", InvoiceLineNumber: \"1\", InvoiceNumber: \"123\", IsSelected: false, StatCode: \"\", TariffCode: \"2205.90.90 14\" } ] }, Type: null, MachineName: null, Email: \"testmail@ext\", Timestamp: 07/27/2023 09:37:29 }","title":"Message with Status MSG"},{"location":"Specs/CW1%252DBorderWise-integration-specification/#cancel-message-from-cw1","text":"{ ClientId: 6d674c0c-5403-4039-af8d-5c8dc6bfddfe, SystemNameSender: \"CargoWiseOne \", SystemNameRecipient: \"BorderWiseWeb\", Status: CAN, Data: null, Type: null, MachineName: null, Email: \"testmail@ext\", Timestamp: 07/27/2023 09:37:29 }","title":"Cancel Message from CW1"},{"location":"Specs/CW1%252DBorderWise-integration-specification/#borderwise-portal-websocket-event-handlers","text":"Source code URL: BWW_ConnectToBWWWebSocketHub_SourceCode","title":"BorderWise Portal WebSocket event handlers"},{"location":"Specs/CW1%252DBorderWise-integration-specification/#initiate-websocket-connection","text":"Source code URL: createWebSocketConnection When CW1 opens BorderWise window, the unique client ID is passed as a URL parameter with the name webSocketClientId. This client ID is used for further web socket message exchange. It ensures that all the messages sent to the Hub are redirected to the correct CW1 instance. BWW uses browser built-in WebSocket API to interact with WebSocket connections. To instantiate the WebSocket connection, the URL needed is wss://app.borderwise.com/api/ws/hub in the case of BorderWise WebSocket Hub.","title":"Initiate WebSocket Connection"},{"location":"Specs/CW1%252DBorderWise-integration-specification/#when-websocket-connection-is-successfully-established","text":"Source code URL: startWebSocketMessageExchange After successfully establishing the WebSocket connection with the BorderWise WebSocket hub, the WebSocket's onOpen event handler is triggered and the client proceeds to send an STA start message. Sample STA start message { ClientId: 6d674c0c-5403-4039-af8d-5c8dc6bfddfe, SystemNameSender: \"BorderWiseWeb\", SystemNameRecipient: \"BWW.API\", Status: STA, Data: null, Type: null, MachineName: null, Email: \"testmail@ext\", Timestamp: 07/27/2023 09:37:29 }","title":"When WebSocket connection is successfully established"},{"location":"Specs/CW1%252DBorderWise-integration-specification/#on-receiving-a-message-from-websocket-hub","text":"Source code URL: processWebSocketMessageExchange WebSocket onMessage event handler is triggered which does further processing of the data received. Sample message { ClientId: 6d674c0c-5403-4039-af8d-5c8dc6bfddfe, SystemNameSender: \"CargoWiseOne \", SystemNameRecipient: \"BorderWiseWeb\", Status: MSG, Data: \"BorderWiseWeb websocket connected.\", Type: Information, MachineName: null, Email: \"testmail@ext\", Timestamp: 07/27/2023 09:37:29 }","title":"On Receiving a Message from WebSocket Hub"},{"location":"Specs/CW1%252DBorderWise-integration-specification/#on-error-in-websocket-connection","text":"Source Code URL: onWebSocketConnectionError The onError event is fired when there is an error during the WebSocket connection or communication process. This could happen due to various reasons, such as server unavailability, connection disruptions, or protocol errors. If an error is detected during the WebSocket communication, the code generates an error message indicating the client's WebSocket ID (webSocketClientId) and the corresponding event code.","title":"On Error in WebSocket Connection"},{"location":"Specs/DAT-Docker-deployment/","text":"How we use Docker TLDR: we have linux server that runs docker and we just SSH into it from DAT; On local we don't use it; I believe that we have Docker CE installed on the server, which is fine in terms of license, as opposed to Docker Desktop. When it comes to local development, we almost never use Docker. When we do need to test/debug something - we either install Docker CE inside of WSL, or use Podman inside of WSL as well. We started with Podman, but Docker CE seems to work for me fine as well. Here are some useful resources: DockerDeployment Docker (podman) All Matters Linux - Linux Infrastructure Initiative | Containerization and orchestrators | Microsoft Teams nginx/docker tests - we have this test for nginx on docker, but we only enable and run it on local for now","title":"How we use Docker"},{"location":"Specs/DAT-Docker-deployment/#how-we-use-docker","text":"TLDR: we have linux server that runs docker and we just SSH into it from DAT; On local we don't use it; I believe that we have Docker CE installed on the server, which is fine in terms of license, as opposed to Docker Desktop. When it comes to local development, we almost never use Docker. When we do need to test/debug something - we either install Docker CE inside of WSL, or use Podman inside of WSL as well. We started with Podman, but Docker CE seems to work for me fine as well. Here are some useful resources: DockerDeployment Docker (podman) All Matters Linux - Linux Infrastructure Initiative | Containerization and orchestrators | Microsoft Teams nginx/docker tests - we have this test for nginx on docker, but we only enable and run it on local for now","title":"How we use Docker"},{"location":"Specs/Server-Farm-Patterns-in-BW/","text":"Overview Server farm patterns are used in BorderWise to identify machine IDs of users that belong to a given organization. They are employed when users access BorderWise through server farm. Administrators of organizations that use server farms are encouraged to install the WTG Machine Identification Service ( Download link ) on their server farm machines. Managing Server Farm Patterns Server farm patterns are managed in UMP under the 'Organization' tab. This is accessible to organization administrators. Example: Citrix","title":"Overview"},{"location":"Specs/Server-Farm-Patterns-in-BW/#overview","text":"Server farm patterns are used in BorderWise to identify machine IDs of users that belong to a given organization. They are employed when users access BorderWise through server farm. Administrators of organizations that use server farms are encouraged to install the WTG Machine Identification Service ( Download link ) on their server farm machines.","title":"Overview"},{"location":"Specs/Server-Farm-Patterns-in-BW/#managing-server-farm-patterns","text":"Server farm patterns are managed in UMP under the 'Organization' tab. This is accessible to organization administrators.","title":"Managing Server Farm Patterns"},{"location":"Specs/Server-Farm-Patterns-in-BW/#example-citrix","text":"","title":"Example: Citrix"},{"location":"Specs/Authentication-process-diagrams/Editions-dialogs/","text":"Editions dialog We use editions dialog in 2 cases: 1. During authentication: If it's the first active device, or new user, then the editions dialog will appear for user to select their edition to start using BW. 2. Triggered from \"More\" menu: When user want to change their edition to different one, they can trigger \"Change Edition\" option from \"More\" menu, and select a new edition. => Although there are quite a few common sub-components, there is different logic in both types of editions dialog, so it's better to separate them to avoid confusion. We can also extract those sub-components to re-use in both types. Comparison diagrams There are diagrams comparing both types to make the difference easier to understand and spot: Diagrams for finding the initial edition Diagrams for the dialog components layout Common diagrams Apart from differences, in editions dialog, user can or cannot change edition based on whether user is student, or whether the organization allows them to change edition. This logic is the same for both types. Diagram for user permission to change edition","title":"Editions dialog"},{"location":"Specs/Authentication-process-diagrams/Editions-dialogs/#editions-dialog","text":"We use editions dialog in 2 cases: 1. During authentication: If it's the first active device, or new user, then the editions dialog will appear for user to select their edition to start using BW. 2. Triggered from \"More\" menu: When user want to change their edition to different one, they can trigger \"Change Edition\" option from \"More\" menu, and select a new edition. => Although there are quite a few common sub-components, there is different logic in both types of editions dialog, so it's better to separate them to avoid confusion. We can also extract those sub-components to re-use in both types.","title":"Editions dialog"},{"location":"Specs/Authentication-process-diagrams/Editions-dialogs/#comparison-diagrams","text":"There are diagrams comparing both types to make the difference easier to understand and spot:","title":"Comparison diagrams"},{"location":"Specs/Authentication-process-diagrams/Editions-dialogs/#diagrams-for-finding-the-initial-edition","text":"","title":"Diagrams for finding the initial edition"},{"location":"Specs/Authentication-process-diagrams/Editions-dialogs/#diagrams-for-the-dialog-components-layout","text":"","title":"Diagrams for the dialog components layout"},{"location":"Specs/Authentication-process-diagrams/Editions-dialogs/#common-diagrams","text":"Apart from differences, in editions dialog, user can or cannot change edition based on whether user is student, or whether the organization allows them to change edition. This logic is the same for both types.","title":"Common diagrams"},{"location":"Specs/Authentication-process-diagrams/Editions-dialogs/#diagram-for-user-permission-to-change-edition","text":"","title":"Diagram for user permission to change edition"},{"location":"Specs/Authentication-process-diagrams/Implementation%252Doriented-authentication-diagram/","text":"Below is more implementation-oriented diagram that will helps developers to follow the flow, divide the stages and maybe find a way to improve the process: In this diagram, I have divided/separated stages including: * Normal login * autologin * getSession() * initializeLicence() * contractCheck() * studentCheck() * licenceCheck() * Licence Dialog * Editions Dialog * initUser() => Currently, we are coupling these stages by Promises hell chain inside app.controller.js => So, one of the ideas is to separate them in a Vue component called AuthCover , which will be implemented in WI00587038 - BWW. Create AuthCover component in Vue","title":"Implementation%2Doriented authentication diagram"},{"location":"Specs/Authentication-process-diagrams/Simple-authentication-diagram/","text":"This is a simple diagram with not much implementation details in order for us to have the whole big picture of the authentication process:","title":"Simple authentication diagram"},{"location":"Specs/Authentication-process-diagrams/Vue-%252D-Improved-auth-process/","text":"","title":"Vue %2D Improved auth process"},{"location":"Testing/Cross-Browser-Testing/","text":"Cross Browser Testing is the practice of ensuring that a website works across various browsers and devices. There are multiple methods we can try to test cross-browser: 1. Use cross-browser testing sites: * There are lots of cross-browser testing sites that we can find online. * The most popular one would be browserstack * Pros: Quick test, multiple browsers/versions/devices are ready to test Cons: It's not free and the free trial only 1 minute testing (too short) * Another site that has longer testing period (3 minutes) is browserling but it's very limited on browsers/versions/devices => Big cons from testing sites are that we cannot test freely 2. Download the exact browsers/versions/devices: 2.1. Use PortableApps : * We can download PortableApps and then download PortableApps files to install the one we want For example: We can download Chrome Portable files here * Pros: Can test freely Cons: For older versions, lots of files are unavailable so cannot install For example, try downloading Chrome v80.0.3987.87 , it will show the error The installer was unable to download Google Chrome (Stable). The installation of the portable app will be incomplete without it. Please try installing again. (ERROR: File Not Found 404) while installing. 2.2. Download exact version for Chromium: 2.2.1. Follow the instructions from Chromium Wiki Or follow the instructions from this SO => Pros: Will be able to download much older version 2.2.2. Another way is following comment in the SO above, download from chromium.cypress.io/ => The links seem to all be official and you can filter by OS.","title":"Cross Browser Testing"},{"location":"Testing/Cypress/","text":"","title":"Cypress"},{"location":"Testing/Running-BWW-tests/","text":"Pre-requisites Make sure to follow Getting started from README.md to install all dependencies before running tests. TL;DR npm test in src and vue-elements folders; npm run *:watch during active development; There is VS Code plugin Active development Setup Make sure to install all recommended extensions from extensions.recommendations of BorderWiseWeb/Frontend/BorderWiseWeb-Frontend.code-workspace ; VS Code should've asked if you'd like to install them when you first opened the workspace. Working on legacy AngularJS Frontend/src project Running Karma unit tests npm run test-karma:watch npm run test-browser - runs in browser and allows to use Chrome DevTools to debug fit() / fdescribe() to run f ocused (specific) tests and skip others Running Cypress integration/e2e tests CLI npm run test-e2e -- --spec=\"cypress/e2e/auth/login.spec.js\" - run 1 spec file at a time npm run test-e2e - run all specs files (not recommended because there are too many specs files, and it would take very long) Cypress UI App npm run test-e2e:open - start Cypress server and open UI App Choose testing type: \"E2E Testing\", and select a browser: Select a spec file and start running tests: Working on new Vue Frontend/vue-elements project You can choose to run unit tests in a few different ways, see below. CLI This is the easiest way, also this is how tests run in CI so you can debug CI issues using CLI on your local. - npm run test:unit:watch - it.only() / describe.only() - to run only specific tests Vitest UI This is the nicest and most productive way, also quite reliable. You can open vitest UI on the second screen and get a nice view of live test results with easy navigation. - npm run test:unit:ui - npx vitest --ui PageContentSetup or similar to only run specific test file - see docs for more info VS Code test explorer Not recommended, known for many issues, may work for some tests, see docs Running all tests When? You might want to run all tests before submitting to SH0 to DAT, this might save you time because you won't have to wait in the queue to run tests, also running them locally is usually faster, especially if you're using WSL. Another reason to run all tests locally is when you need to debug some error that happened during DAT/QGL run. Still, I would recommend running specific tests, if prettier failed - only run prettier fix; if unit tests failed - only run unit tests; it'll speed things up and help you in the long run. How? Legacy AngularJS Frontend/src project npm test Check out package.json to see what this script will run, you can manually run parts to speed it up Beware that cypress ( test-e2e ) tests will likely randomly fail due to a known http-server bug. You can use npm run test-e2e:open to open Cypress UI and run specific tests you're working on individually. Or use QGL to run all tests - will be extremely slow tho. Might want to give cypress-run-all.js a try, not sure it'll help tho (need to modify it to restart http-server every time before each test). New Vue Frontend/vue-elements project npm test Docker We have integration tests for Docker and Nginx - src/__tests__/docker.skip.ts ; When? These are only to be run on local dev machine when making config changes. These will be run on DAT as well once we have Docker support on DAT. How? Rename this file from docker.skip.ts to docker.spec.ts in order to run it cd ../src && npm run build-dev - make sure to run build-dev from gulp before running these tests npx -y cross-env DEBUG=true npm run test:unit -- docker - one-time run with debug output (recommended when running for the first time) During active development - vitest docker Note, that when this test fails it may leave changes in nginx.conf - do not commit them and revert in order for test to run properly next time.","title":"Pre-requisites"},{"location":"Testing/Running-BWW-tests/#pre-requisites","text":"Make sure to follow Getting started from README.md to install all dependencies before running tests.","title":"Pre-requisites"},{"location":"Testing/Running-BWW-tests/#tldr","text":"npm test in src and vue-elements folders; npm run *:watch during active development; There is VS Code plugin","title":"TL;DR"},{"location":"Testing/Running-BWW-tests/#active-development","text":"","title":"Active development"},{"location":"Testing/Running-BWW-tests/#setup","text":"Make sure to install all recommended extensions from extensions.recommendations of BorderWiseWeb/Frontend/BorderWiseWeb-Frontend.code-workspace ; VS Code should've asked if you'd like to install them when you first opened the workspace.","title":"Setup"},{"location":"Testing/Running-BWW-tests/#working-on-legacy-angularjs-frontendsrc-project","text":"","title":"Working on legacy AngularJS Frontend/src project"},{"location":"Testing/Running-BWW-tests/#running-karma-unit-tests","text":"npm run test-karma:watch npm run test-browser - runs in browser and allows to use Chrome DevTools to debug fit() / fdescribe() to run f ocused (specific) tests and skip others","title":"Running Karma unit tests"},{"location":"Testing/Running-BWW-tests/#running-cypress-integratione2e-tests","text":"","title":"Running Cypress integration/e2e tests"},{"location":"Testing/Running-BWW-tests/#cli","text":"npm run test-e2e -- --spec=\"cypress/e2e/auth/login.spec.js\" - run 1 spec file at a time npm run test-e2e - run all specs files (not recommended because there are too many specs files, and it would take very long)","title":"CLI"},{"location":"Testing/Running-BWW-tests/#cypress-ui-app","text":"npm run test-e2e:open - start Cypress server and open UI App Choose testing type: \"E2E Testing\", and select a browser: Select a spec file and start running tests:","title":"Cypress UI App"},{"location":"Testing/Running-BWW-tests/#working-on-new-vue-frontendvue-elements-project","text":"You can choose to run unit tests in a few different ways, see below.","title":"Working on new Vue Frontend/vue-elements project"},{"location":"Testing/Running-BWW-tests/#cli_1","text":"This is the easiest way, also this is how tests run in CI so you can debug CI issues using CLI on your local. - npm run test:unit:watch - it.only() / describe.only() - to run only specific tests","title":"CLI"},{"location":"Testing/Running-BWW-tests/#vitest-ui","text":"This is the nicest and most productive way, also quite reliable. You can open vitest UI on the second screen and get a nice view of live test results with easy navigation. - npm run test:unit:ui - npx vitest --ui PageContentSetup or similar to only run specific test file - see docs for more info","title":"Vitest UI"},{"location":"Testing/Running-BWW-tests/#vs-code-test-explorer","text":"Not recommended, known for many issues, may work for some tests, see docs","title":"VS Code test explorer"},{"location":"Testing/Running-BWW-tests/#running-all-tests","text":"","title":"Running all tests"},{"location":"Testing/Running-BWW-tests/#when","text":"You might want to run all tests before submitting to SH0 to DAT, this might save you time because you won't have to wait in the queue to run tests, also running them locally is usually faster, especially if you're using WSL. Another reason to run all tests locally is when you need to debug some error that happened during DAT/QGL run. Still, I would recommend running specific tests, if prettier failed - only run prettier fix; if unit tests failed - only run unit tests; it'll speed things up and help you in the long run.","title":"When?"},{"location":"Testing/Running-BWW-tests/#how","text":"","title":"How?"},{"location":"Testing/Running-BWW-tests/#legacy-angularjs-frontendsrc-project","text":"npm test Check out package.json to see what this script will run, you can manually run parts to speed it up Beware that cypress ( test-e2e ) tests will likely randomly fail due to a known http-server bug. You can use npm run test-e2e:open to open Cypress UI and run specific tests you're working on individually. Or use QGL to run all tests - will be extremely slow tho. Might want to give cypress-run-all.js a try, not sure it'll help tho (need to modify it to restart http-server every time before each test).","title":"Legacy AngularJS Frontend/src project"},{"location":"Testing/Running-BWW-tests/#new-vue-frontendvue-elements-project","text":"npm test","title":"New Vue Frontend/vue-elements project"},{"location":"Testing/Running-BWW-tests/#docker","text":"We have integration tests for Docker and Nginx - src/__tests__/docker.skip.ts ;","title":"Docker"},{"location":"Testing/Running-BWW-tests/#when_1","text":"These are only to be run on local dev machine when making config changes. These will be run on DAT as well once we have Docker support on DAT.","title":"When?"},{"location":"Testing/Running-BWW-tests/#how_1","text":"Rename this file from docker.skip.ts to docker.spec.ts in order to run it cd ../src && npm run build-dev - make sure to run build-dev from gulp before running these tests npx -y cross-env DEBUG=true npm run test:unit -- docker - one-time run with debug output (recommended when running for the first time) During active development - vitest docker Note, that when this test fails it may leave changes in nginx.conf - do not commit them and revert in order for test to run properly next time.","title":"How?"},{"location":"Testing/Testing-with-MSW-in-WW/","text":"Our Mock Service Worker (MSW) handlers in WebWatcher (WW) use a state of mock data (which can be found in mockData.ts ) to simulate how our requests work in the real world. This state provides enough data to ensure the majority of components behave seamlessly in our tests without having to tinker with them too much. The benefit of this is that it removes the need to setup detailed mock data and routes for each separate test, meaning we're only required to make small changes to test data relevant for the functionality we're testing in a given test. When Should I Modify Handlers.ts? The routes defined in handlers.ts should be routes that are relevant to a variety of different tests. Generally, these routes return all the data associated with the default library item we use in a lot of our tests. These routes should return values from the state so that if/when we change values during our tests (or our handlers), these data changes are reflected across all our mock routes (e.g. our schedule now endpoint should update a schedule's last check property). As an example, these routes return monitoring requests in the default collection and schedule info for the default monitoring request used in our test files: What If I Want to Test Something a Bit More Specific? Modify the State During Your Test If you want to test something that only requires a small change to existing data, you should modify the state. An example of this could be modifying the default monitoring request's extraction type from \"Custom\" to \"Web\", which we might want to do when testing our ConfigurationForm component. Changes made to the state are all reset before each test in our setupTests.ts file, so we don't have to worry about possibly breaking other tests. Setup Separate Handler Inside of a Test In some cases there may be mock request routes that are only relevant to a specific test. Or maybe we don't care about a request response and only want to confirm that a request was sent (e.g. when a form is submitted). In these cases we can call server.use() within our test to setup our custom route, which will prepend our request handler to the current MSW server instance to ensure it matches before any other existing handler. An example of this can be seen in LoginForm.spec.ts :","title":"Testing with MSW in WW"},{"location":"Testing/Testing-with-MSW-in-WW/#when-should-i-modify-handlersts","text":"The routes defined in handlers.ts should be routes that are relevant to a variety of different tests. Generally, these routes return all the data associated with the default library item we use in a lot of our tests. These routes should return values from the state so that if/when we change values during our tests (or our handlers), these data changes are reflected across all our mock routes (e.g. our schedule now endpoint should update a schedule's last check property). As an example, these routes return monitoring requests in the default collection and schedule info for the default monitoring request used in our test files:","title":"When Should I Modify Handlers.ts?"},{"location":"Testing/Testing-with-MSW-in-WW/#what-if-i-want-to-test-something-a-bit-more-specific","text":"","title":"What If I Want to Test Something a Bit More Specific?"},{"location":"Testing/Testing-with-MSW-in-WW/#modify-the-state-during-your-test","text":"If you want to test something that only requires a small change to existing data, you should modify the state. An example of this could be modifying the default monitoring request's extraction type from \"Custom\" to \"Web\", which we might want to do when testing our ConfigurationForm component. Changes made to the state are all reset before each test in our setupTests.ts file, so we don't have to worry about possibly breaking other tests.","title":"Modify the State During Your Test"},{"location":"Testing/Testing-with-MSW-in-WW/#setup-separate-handler-inside-of-a-test","text":"In some cases there may be mock request routes that are only relevant to a specific test. Or maybe we don't care about a request response and only want to confirm that a request was sent (e.g. when a form is submitted). In these cases we can call server.use() within our test to setup our custom route, which will prepend our request handler to the current MSW server instance to ensure it matches before any other existing handler. An example of this can be seen in LoginForm.spec.ts :","title":"Setup Separate Handler Inside of a Test"},{"location":"Testing/Writing-BWW-tests/","text":"Mocking In Vue tests we provide some mocks for AngularJS services, such as userServiceMock , utilServiceMock , tariffServiceMock , etc. They are registered in src/utils/testHelpers.ts by calling vi.mock(\"../angularjs-services/services\", ... So, when writing your tests you need to make sure that you import mocks before importing the component that you're testing. So this will work: // mocks are registered first because testHelpers is executed at the time of import import { contentServiceMock, libraryServiceMock, renderWithVuetify } from \"../../utils/testHelpers\"; // mocks registered in the previous step will be used in place of the real module when importing the component import { CountrySelection } from \"../CountrySelection\"; But if you flip these lines around - you will import the component first with actual modules, and then register mocks which won't do anything since component was already imported with real modules.","title":"Mocking"},{"location":"Testing/Writing-BWW-tests/#mocking","text":"In Vue tests we provide some mocks for AngularJS services, such as userServiceMock , utilServiceMock , tariffServiceMock , etc. They are registered in src/utils/testHelpers.ts by calling vi.mock(\"../angularjs-services/services\", ... So, when writing your tests you need to make sure that you import mocks before importing the component that you're testing. So this will work: // mocks are registered first because testHelpers is executed at the time of import import { contentServiceMock, libraryServiceMock, renderWithVuetify } from \"../../utils/testHelpers\"; // mocks registered in the previous step will be used in place of the real module when importing the component import { CountrySelection } from \"../CountrySelection\"; But if you flip these lines around - you will import the component first with actual modules, and then register mocks which won't do anything since component was already imported with real modules.","title":"Mocking"},{"location":"Testing/Cypress/Common-errors%2C-amnesties-and-possible-fixes/","text":"Common Cypress errors/amnesties requestTimeout for wait of multiple requests aliases Too many tests in 1 spec file Leaked request because of test dependency on previous test 1. requestTimeout for wait of multiple requests aliases For example, the code below: cy.wait(['@validateLicence', '@getUser', '@recordActivity', '@checkLicenceStatus', '@getDailyUpdates']); might lead to error: CypressError: Timed out retrying after 5000ms: `cy.wait()` timed out waiting `5000ms` for the 1st request to the route: `recordActivity`. No request ever occurred. Possible fix: From the latest docs, timeout for wait is divided into requestTimeout and responseTimeout . We are waiting for an array of aliases , in which Cypress will wait for all requests to complete within the given requestTimeout and responseTimeout . And the given requestTimeout is 5000 ms So, if a request like @getUser takes too long to response (we can setTimeout to delay like 6000 ms), then we'll never reach the rest of array of aliases (including @recordActivity , @checkLicenceStatus , @getDailyUpdates ). => We need to increase requestTimeout for the wait for array of aliases command => The best increased timeout number to cover the whole array of aliases can be number of aliases multiplying default requestTimeout (which can be access via Cypress.config('requestTimeout') In the example above, the increased requestTimeout should be 5 * Cypress.config('requestTimeout') or 25000 ms => The code can be rewritten as follows: cy.wait(['@validateLicence', '@getUser', '@recordActivity', '@checkLicenceStatus', '@getDailyUpdates'], { requestTimeout: 5 * Cypress.config('requestTimeout') }); 2. Too many tests in 1 spec file If a spec file has too many tests and there are a lot of tests depending on previous tests, it is considered not best practice. There are lots of drawbacks for this kind of spec file. 1. You will have to run all tests in the spec file to reproduce a failed test => Takes long time to reach the failed test and not even sure that it can be reproduced. 2. You will have to follow the code logic/flow from the beginning to the failed test in order to trace the problem 3. You cannot run this big spec file in GUI mode because Chromium will crash because of \"Out of memory\" error => So to reproduce the problem, you have to run it from the beginning to the failed test, and run it in CLI mode => Sounds like quite a nightmare, isn't it? It can pass on your local, but it can still fail on DAT (because DAT run on different machines) => So if it fails on DAT and we cannot reproduce the problem locally because of huge number of tests, that's when those tests confuse and cost us the most. Therefore, there should be a limit to number of tests in a spec in order to improve testing + developer experience. IMO, number of tests in each spec file should be within 15-20 tests. If the number of tests start reaching or exceeding this limit, it means that there will be much higher chance for us to get trouble with the spec file soon (due to those reasons above). So if a dev faces this case, a refactor should be done as follows: * Split the spec file with huge number of tests into multiple smaller spec files. * Each small spec file has its own purpose of testing * Put those smaller spec files into 1 folder that serve for general purpose of previous huge spec file. For example: PR for refactoring search.spec.js In this case, the huge spec file is search.spec.js We split search.spec.js into multiple smaller spec file, each has its own purpose of testing, which is purpose of testing for search features. And all of those small spec files are included in folder search You should divide tests appropriately based on the purpose of search and also create utils/helpers to reuse in different spec files. For example: * search/alphabetical-search-results.spec.js * search/clear-query-button.spec.js ... Another reference is when I split tests for authentication here into multiple smaller spec files of folder auth 3. Leaked request because of test dependency on previous test For example, we have 2 tests called \"test 1\" and \"test 2\" . And \"test 1\" has intercept for a request \"request 1\" . But somehow, \"test 1\" is very quickly passed without \"request 1\" is made. Then, when it goes to \"test 2\" already, \"request 1\" is now called. But there is no intercept for \"request 1\" in \"test 2\" => This means \"request 1\" is leaked from \"test 1\" to \"test 2\" => This is when \"request 1\" is gonna return 404, which might lead to some troubles in rendering stuff. => Possible solutions to avoid this error: * Check the functionality to make sure it doesn't make any unnecessary requests continuously => most of the times I see leaked requests are because of AngularJS bad rendering mechanisms + some bad watches inside components => This is pretty hard to fix because AngularJS mechanism is hard to follow => The mitigation is probably trying to narrow down the watches, and migrate to Vue component asap Make the intercepts in \"test 1\" accessible to \"test 2\" => Doing this, the leaked request is still made successfully and would highly likely not to cause troubles during \"test 2\" Going with test isolation way: Either separate 2 tests or combine them (depending on the test context) See WI: WI00620285 - BWW. Fix \"Should auto open previous TOC item\" in left-toc.spec.js for this exact error happen. See PR: https://devops.wisetechglobal.com/wtg/BorderWise/_git/BorderWiseWeb/pullrequest/167469?_a=files for the solution with test isolation way","title":"Common Cypress errors/amnesties"},{"location":"Testing/Cypress/Common-errors%2C-amnesties-and-possible-fixes/#common-cypress-errorsamnesties","text":"requestTimeout for wait of multiple requests aliases Too many tests in 1 spec file Leaked request because of test dependency on previous test","title":"Common Cypress errors/amnesties"},{"location":"Testing/Cypress/Common-errors%2C-amnesties-and-possible-fixes/#1-requesttimeout-for-wait-of-multiple-requests-aliases","text":"For example, the code below: cy.wait(['@validateLicence', '@getUser', '@recordActivity', '@checkLicenceStatus', '@getDailyUpdates']); might lead to error: CypressError: Timed out retrying after 5000ms: `cy.wait()` timed out waiting `5000ms` for the 1st request to the route: `recordActivity`. No request ever occurred. Possible fix: From the latest docs, timeout for wait is divided into requestTimeout and responseTimeout . We are waiting for an array of aliases , in which Cypress will wait for all requests to complete within the given requestTimeout and responseTimeout . And the given requestTimeout is 5000 ms So, if a request like @getUser takes too long to response (we can setTimeout to delay like 6000 ms), then we'll never reach the rest of array of aliases (including @recordActivity , @checkLicenceStatus , @getDailyUpdates ). => We need to increase requestTimeout for the wait for array of aliases command => The best increased timeout number to cover the whole array of aliases can be number of aliases multiplying default requestTimeout (which can be access via Cypress.config('requestTimeout') In the example above, the increased requestTimeout should be 5 * Cypress.config('requestTimeout') or 25000 ms => The code can be rewritten as follows: cy.wait(['@validateLicence', '@getUser', '@recordActivity', '@checkLicenceStatus', '@getDailyUpdates'], { requestTimeout: 5 * Cypress.config('requestTimeout') });","title":"1. requestTimeout for wait of multiple requests aliases"},{"location":"Testing/Cypress/Common-errors%2C-amnesties-and-possible-fixes/#2-too-many-tests-in-1-spec-file","text":"If a spec file has too many tests and there are a lot of tests depending on previous tests, it is considered not best practice. There are lots of drawbacks for this kind of spec file. 1. You will have to run all tests in the spec file to reproduce a failed test => Takes long time to reach the failed test and not even sure that it can be reproduced. 2. You will have to follow the code logic/flow from the beginning to the failed test in order to trace the problem 3. You cannot run this big spec file in GUI mode because Chromium will crash because of \"Out of memory\" error => So to reproduce the problem, you have to run it from the beginning to the failed test, and run it in CLI mode => Sounds like quite a nightmare, isn't it? It can pass on your local, but it can still fail on DAT (because DAT run on different machines) => So if it fails on DAT and we cannot reproduce the problem locally because of huge number of tests, that's when those tests confuse and cost us the most. Therefore, there should be a limit to number of tests in a spec in order to improve testing + developer experience. IMO, number of tests in each spec file should be within 15-20 tests. If the number of tests start reaching or exceeding this limit, it means that there will be much higher chance for us to get trouble with the spec file soon (due to those reasons above). So if a dev faces this case, a refactor should be done as follows: * Split the spec file with huge number of tests into multiple smaller spec files. * Each small spec file has its own purpose of testing * Put those smaller spec files into 1 folder that serve for general purpose of previous huge spec file. For example: PR for refactoring search.spec.js In this case, the huge spec file is search.spec.js We split search.spec.js into multiple smaller spec file, each has its own purpose of testing, which is purpose of testing for search features. And all of those small spec files are included in folder search You should divide tests appropriately based on the purpose of search and also create utils/helpers to reuse in different spec files. For example: * search/alphabetical-search-results.spec.js * search/clear-query-button.spec.js ... Another reference is when I split tests for authentication here into multiple smaller spec files of folder auth","title":"2. Too many tests in 1 spec file"},{"location":"Testing/Cypress/Common-errors%2C-amnesties-and-possible-fixes/#3-leaked-request-because-of-test-dependency-on-previous-test","text":"For example, we have 2 tests called \"test 1\" and \"test 2\" . And \"test 1\" has intercept for a request \"request 1\" . But somehow, \"test 1\" is very quickly passed without \"request 1\" is made. Then, when it goes to \"test 2\" already, \"request 1\" is now called. But there is no intercept for \"request 1\" in \"test 2\" => This means \"request 1\" is leaked from \"test 1\" to \"test 2\" => This is when \"request 1\" is gonna return 404, which might lead to some troubles in rendering stuff. => Possible solutions to avoid this error: * Check the functionality to make sure it doesn't make any unnecessary requests continuously => most of the times I see leaked requests are because of AngularJS bad rendering mechanisms + some bad watches inside components => This is pretty hard to fix because AngularJS mechanism is hard to follow => The mitigation is probably trying to narrow down the watches, and migrate to Vue component asap Make the intercepts in \"test 1\" accessible to \"test 2\" => Doing this, the leaked request is still made successfully and would highly likely not to cause troubles during \"test 2\" Going with test isolation way: Either separate 2 tests or combine them (depending on the test context) See WI: WI00620285 - BWW. Fix \"Should auto open previous TOC item\" in left-toc.spec.js for this exact error happen. See PR: https://devops.wisetechglobal.com/wtg/BorderWise/_git/BorderWiseWeb/pullrequest/167469?_a=files for the solution with test isolation way","title":"3. Leaked request because of test dependency on previous test"},{"location":"Testing/Cypress/Cypress-Amnesty-Failure-%252D-Approaches/","text":"Pre-req: Make sure to checkout and understand the definition of Intermittent Failures and Amnesty tests: See StackOverflow references below: - What are Intermittent Failures? - Why are test failures shown in grey vs black? - What is the difference between an incident, a defect, an issue, and an amnesty? TL;DR Intermittent Failure is a failure of a test on one DAT test agent, when a subsequent execution of the same test within the same DAT run has passed A \"confirmed\" failure is the test was run on three separate DAT test agents and the test failed all three times. An Amnesty is a unit test that has started failing, but no change in the DAT run could be automatically identified as the cause of the failure. This could be due to intermittent failures, time-based failures (e.g. tests that depend upon Daylight Savings), systemic issues (e.g. updates to DAT virtual machines) or any other reason that would cause a test to work for a certain amount of time before it started to fail. \"Confirmed\" failures on DAT that is caused by your PR/changes: If your DAT run always fail when you submit your changes, and the failed test is related to your changes, then you will have to fix it (or ask for assisting to fix it) within your PR/changes. If your changes make Cypress tests failed on DAT, but it still pass on master branch, it can have 2 meanings: Your functionality code changes has problems that broke a functionality that is supposed to work The code for Cypress test needs to be updated to reflect your changes \"Confirmed\" failures on DAT that is NOT caused by your PR/changes: Please double check carefully that the \"confirmed\" failure is NOT caused by your changes. If your DAT run fail, but it's really NOT related to your changes, then it is highly likely to be an Amnesty test failure. => You can try submitting for another DAT run (because sometimes DAT env can cause some unexpected issues). Any failed DAT run will be recorded, and if a Cypress test fail too many times, DAT will start seeing it as an Amnesty test, and we will have an Amnesty WI to fix it. Approaches for Amnesty test failure Fix it :) Please read more about Cypress best practices in their docs to find possible fix See also: Wiki for Common errors, amnesties and possible fixes Common Cypress errors in README.md After finding the fix: When we finish fixing any Cypress Amnesty Failure, there are few cautious approaches that we can take in order to make sure amnesty test actually fixed: 1. Before starting to fix the test - make sure you can reproduce the failure. Edit script run-tests-until-failure.sh to run the test you're trying to fix and run it until it fails. If it's been running for a while (30 minutes, etc) and still doesn't fail - maybe try to think what could go wrong and try to fail it manually. Like switching in devtools to slower network speed, or adding timeouts/delays for requests, trying to create a race condition, etc. 1. Try script run-tests-until-failure.sh to re-run a test locally as many times as you want. If the script running your test successfully for an acceptable number of times (at least 5-10 times, but best to leave it running for as long as you can while working on something else, like an hour or so), then most likely the test is fixed. 1. Try running your fix on DAT for multiple times (like shelf test for 3 times). Be aware that every time we run shelf test on DAT, the test can be run in different machine, so even multiple run on DAT can pass but amnesty test can still come back on another DAT machine. 1. Try running tests under system load. For example, download y-cruncher (password \"shadow\" because it's blocked by IS for no good reason). In there you can run stress-test. You can also change performance mode to something other than \"Ultra Performance\" in Dell Optimized, and might want to change Windows power mode to \"Best power efficiency\". The goal is to slow down you PC to be like DAT, as it can be very slow under load. And keep on running run-tests-until-failure.sh . Skip Amnesty test and fix later: Skipping a test is NOT a recommended way whenever we have a failure. Even when amnesty test is NOT caused by your changes, you should try to fix it before thinking of skipping. There are some cases that we can have tolerance for skipping Cypress tests: Some breaking changes like upgrading Cypress, huge PR, work in progress, etc... When a priority WIs require faster merge like defects fixes, etc... => But then, the skipped Cypress tests should be pushed to be fixed immediately. See similar SO reference also: What to do when Cypress e2e test fails intermittently on DAT for BorderWise Web? Important notes for skipping a Cypress failure test: If you decide to skip a Cypress test, then when you create WI to fix that test, please include the link of failed DAT run, and also attach the screenshot + log file of that failed Cypress test. If you don't include the link, the screenshot and the log file, we can't trace back whatever the issue you have with that test. This ends up we enable the test again just to see why it failed. If the re-enabled test pass, then we'll never know the real issue. If the re-enabled test fail, we are never sure that it's the same issue as when you skipped it. Screenshots + logs files stored on DAT artifacts repository will be removed after a period of time. => So please attach them in eDocs when you create the WI to fix the Cypress test. => This will help preserve them permanently for us to trace back. See WI00632147 - BWW. Fix skipped cypress tests for example: no information at all to find possible fix :cry:","title":"Pre-req:"},{"location":"Testing/Cypress/Cypress-Amnesty-Failure-%252D-Approaches/#pre-req","text":"Make sure to checkout and understand the definition of Intermittent Failures and Amnesty tests: See StackOverflow references below: - What are Intermittent Failures? - Why are test failures shown in grey vs black? - What is the difference between an incident, a defect, an issue, and an amnesty?","title":"Pre-req:"},{"location":"Testing/Cypress/Cypress-Amnesty-Failure-%252D-Approaches/#tldr","text":"Intermittent Failure is a failure of a test on one DAT test agent, when a subsequent execution of the same test within the same DAT run has passed A \"confirmed\" failure is the test was run on three separate DAT test agents and the test failed all three times. An Amnesty is a unit test that has started failing, but no change in the DAT run could be automatically identified as the cause of the failure. This could be due to intermittent failures, time-based failures (e.g. tests that depend upon Daylight Savings), systemic issues (e.g. updates to DAT virtual machines) or any other reason that would cause a test to work for a certain amount of time before it started to fail.","title":"TL;DR"},{"location":"Testing/Cypress/Cypress-Amnesty-Failure-%252D-Approaches/#confirmed-failures-on-dat-that-is-caused-by-your-prchanges","text":"If your DAT run always fail when you submit your changes, and the failed test is related to your changes, then you will have to fix it (or ask for assisting to fix it) within your PR/changes. If your changes make Cypress tests failed on DAT, but it still pass on master branch, it can have 2 meanings: Your functionality code changes has problems that broke a functionality that is supposed to work The code for Cypress test needs to be updated to reflect your changes","title":"\"Confirmed\" failures on DAT that is caused by your PR/changes:"},{"location":"Testing/Cypress/Cypress-Amnesty-Failure-%252D-Approaches/#confirmed-failures-on-dat-that-is-not-caused-by-your-prchanges","text":"Please double check carefully that the \"confirmed\" failure is NOT caused by your changes. If your DAT run fail, but it's really NOT related to your changes, then it is highly likely to be an Amnesty test failure. => You can try submitting for another DAT run (because sometimes DAT env can cause some unexpected issues). Any failed DAT run will be recorded, and if a Cypress test fail too many times, DAT will start seeing it as an Amnesty test, and we will have an Amnesty WI to fix it.","title":"\"Confirmed\" failures on DAT that is NOT caused by your PR/changes:"},{"location":"Testing/Cypress/Cypress-Amnesty-Failure-%252D-Approaches/#approaches-for-amnesty-test-failure","text":"","title":"Approaches for Amnesty test failure"},{"location":"Testing/Cypress/Cypress-Amnesty-Failure-%252D-Approaches/#fix-it","text":"Please read more about Cypress best practices in their docs to find possible fix See also: Wiki for Common errors, amnesties and possible fixes Common Cypress errors in README.md","title":"Fix it :)"},{"location":"Testing/Cypress/Cypress-Amnesty-Failure-%252D-Approaches/#after-finding-the-fix","text":"When we finish fixing any Cypress Amnesty Failure, there are few cautious approaches that we can take in order to make sure amnesty test actually fixed: 1. Before starting to fix the test - make sure you can reproduce the failure. Edit script run-tests-until-failure.sh to run the test you're trying to fix and run it until it fails. If it's been running for a while (30 minutes, etc) and still doesn't fail - maybe try to think what could go wrong and try to fail it manually. Like switching in devtools to slower network speed, or adding timeouts/delays for requests, trying to create a race condition, etc. 1. Try script run-tests-until-failure.sh to re-run a test locally as many times as you want. If the script running your test successfully for an acceptable number of times (at least 5-10 times, but best to leave it running for as long as you can while working on something else, like an hour or so), then most likely the test is fixed. 1. Try running your fix on DAT for multiple times (like shelf test for 3 times). Be aware that every time we run shelf test on DAT, the test can be run in different machine, so even multiple run on DAT can pass but amnesty test can still come back on another DAT machine. 1. Try running tests under system load. For example, download y-cruncher (password \"shadow\" because it's blocked by IS for no good reason). In there you can run stress-test. You can also change performance mode to something other than \"Ultra Performance\" in Dell Optimized, and might want to change Windows power mode to \"Best power efficiency\". The goal is to slow down you PC to be like DAT, as it can be very slow under load. And keep on running run-tests-until-failure.sh .","title":"After finding the fix:"},{"location":"Testing/Cypress/Cypress-Amnesty-Failure-%252D-Approaches/#skip-amnesty-test-and-fix-later","text":"Skipping a test is NOT a recommended way whenever we have a failure. Even when amnesty test is NOT caused by your changes, you should try to fix it before thinking of skipping. There are some cases that we can have tolerance for skipping Cypress tests: Some breaking changes like upgrading Cypress, huge PR, work in progress, etc... When a priority WIs require faster merge like defects fixes, etc... => But then, the skipped Cypress tests should be pushed to be fixed immediately. See similar SO reference also: What to do when Cypress e2e test fails intermittently on DAT for BorderWise Web?","title":"Skip Amnesty test and fix later:"},{"location":"Testing/Cypress/Cypress-Amnesty-Failure-%252D-Approaches/#important-notes-for-skipping-a-cypress-failure-test","text":"If you decide to skip a Cypress test, then when you create WI to fix that test, please include the link of failed DAT run, and also attach the screenshot + log file of that failed Cypress test. If you don't include the link, the screenshot and the log file, we can't trace back whatever the issue you have with that test. This ends up we enable the test again just to see why it failed. If the re-enabled test pass, then we'll never know the real issue. If the re-enabled test fail, we are never sure that it's the same issue as when you skipped it. Screenshots + logs files stored on DAT artifacts repository will be removed after a period of time. => So please attach them in eDocs when you create the WI to fix the Cypress test. => This will help preserve them permanently for us to trace back. See WI00632147 - BWW. Fix skipped cypress tests for example: no information at all to find possible fix :cry:","title":"Important notes for skipping a Cypress failure test:"},{"location":"Testing/Cypress/Cypress-tests-with-Vue-Query/","text":"When migrating from AngularJS to Vue Query we need to change the mental model a bit for writing Cypress tests. In Cypress often times I see this: 1. Setup test 1. Visit page, login, all that 1. Setup interceptor for requests made by some function 1. Click some buttons that call those functions This works fine with the old AngularJS data-fetching model, where every time we need data - we make a request. However, with Vue Query we might choose to cache data for some time. And with Vue Query we don't want to think about manually managing requests, it's abstracted from us, we just ask for data where we need it and it magically appears, maybe from a request, maybe from cache, maybe we put it there manually, etc. So, this also requires us to change the way we set up Cypress tests. TLDR: In 99% of Cypress tests, you need to setup all intercepts before cy.visit()-ing the app page. Here's an example of the issue I found in cypress/e2e/auth/contract/contract-before-editions-dialog.spec.js","title":"Cypress tests with Vue Query"}]}